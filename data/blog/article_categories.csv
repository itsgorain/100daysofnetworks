file,text,categories
/work/blog_data/137281311.welcome-to-day-1-of-100daysofnetworks.html,"Hello everyone! Welcome to day 1 of the 2023 edition of #100daysofnetworks!As some of you are probably aware, this is the SECOND iteration of this adventure, with the first taking place in 2020. I learned so much from that first adventure, and I'm excited to do this again.This first post is going to be a bit long, as I'd like to tell a bit about who I am and what I have in mind.Who am I?My name is David Knickerbocker. I am a software engineer. Since childhood, I have been obsessed with getting computers to do interesting things. As a child, I was excited when I could programmatically get them to make beep boop noises, and then became interested in creating ASCII art after that. As a teenager, I spent a lot of time building (and breaking) computers. As an adult, my earliest obsession was web development, but that has shifted to data engineering and then to data science. My entire life has revolved around working with computers and getting them to do what I want them to do.My career has entirely been in Information Technology (IT), but this has led me to working as a web developer, SQL developer, database administrator, data operations (dataops) engineer, data engineer, platform engineer, and now I am chief engineer in a company I am helping build.My entire career's emphasis has been to help people. This has led me to working in cybersecurity, in a hospital, in companies that help the U.S. Military community, and eventually to building a company to solve certain specific problems. For me, everything I do and build is about using technology to help humans. I don't care about cybersecurity for the sake of cybersecurity or because it pays well. Helping people is my central mission.Now, my work is focuses around Natural Language Processing (NLP) and Network Analysis. The things I will discuss as part of this adventure are the types of things I work on at work. This isn't a hobby, it has real-world applicability for solving problems. These days, I am most obsessed with network and NLP insights. I enjoy using NLP and networks to map out relationship and to find hidden insights. The marriage of NLP and networks makes this possible.Outside of work, I enjoy hanging out with my cats (Eddie and Echo), playing guitar, collecting fancy rocks and minerals, and playing video games.This is Eddie. :)Why am I doing this?I launched the first iteration of this adventure in 2020, after completing more than 100 days of #100daysofnlp. During the NLP adventure, I noticed just how much network data was available in the wild, and how incapable I was at doing more than surface level network analysis. I decided that I wanted to go much deeper. After 100 days of Natural Language Processing, I had built up a lot of skill and confidence. So, I wanted to see how far I could go with networks.The answer is that it took me very far. It took me so far that I got a book deal and a company out of it, and am now so comfortable with network analysis that it has become muscle memory.However, I haven't been able to focus on learning new things about networks as I once was, and there is more that I have been wanting to explore, but haven't found time for, and haven't had a reason to commit. This adventure is my commitment to spending another hundred days (at least), learning new things about network analysis and network science. I am excited to see what I will learn, this next iteration, and I'm excited to get others excited about this stuff as well. This is a creative outlet for me, but it is also research. Things I have learned in the first adventure, I use at work, and they went into my book. Stuff I learn in this new adventure, I will use at work, and they will appear in the second edition of my book.This adventure will lead to new insights and techniques, and it will build skill, confidence, and intuition, as well.Finally, the last reason is selfish: I enjoy sharing knowledge. I enjoy talking about cool stuff I'm learning, and useful things I have learned.What did I learn in 2020?I can still remember day 1 of the earlier #100daysofnetworks. I had just completed #100daysofnlp and felt so inadequate in my ability to use large networks. The first days were very awkward, for me. I had been working with network analysis since about 2018, when I was using network science for understanding dataflows as a data operations engineer, but I had never analyzed of visualized networks with thousands of nodes. Back in 2018 or so, I had also built my first social network, using text from the book of Genesis as data. I wanted to do more stuff like that, so I did. I found that practically any text could be used for constructing networks. I wrote about that in my book. I will be showing more of that in this project. In 2020, I was still using networkx for network visualization, and I suspected that there were better libraries available. There's still no great way to visualize networks in python, but it's getting better. I've shown how I do it, in my book, and that'll be part of this project as well.I learned so much from reading various books on Network Science, Social Network Analysis, and Natural Language Processing. I will do another post about my favorite books, in upcoming weeks.I learned some cool stuff about network fragility, but didn't explore the topic much. I plan to go into that further during this next iteration.And everything I learned, from my own experience before 2020 to everything I read about in 2020, I practiced those techniques, build skill and confidence, and described them in my book, and I use these skills in my company as well.What am I doing differently in 2023?Most importantly, I am setting this up properly.I have created a github repo: https://github.com/itsgorain/100daysofnetworksI have created a blog (you are reading it)I will post on LinkedIn, directing to this blogI wrote a book last year, and you should read it to learn more!As I have written a book on this, I've build up an understanding of how this can be taught from basics to advanced, and I will follow that path this time, rather than being so disorganized as I was before.What is the plan?I plan on teaching the following, in the following order, but I may jump around a bit, to keep this organized but still a bit flexible. Introductions - this postBasics w/ networkx pre-made graphsOpen datasetsBuilding networks, manuallyBuilding networks, automatically, using textCreating datasetsCleaning networksGraph metrics (centralities, etc)Connected componentsSubgraphsEgocentric Network AnalysisCommunity DetectionNetwork weakness and destructionTons and tons of playing with interesting datasetsGraphs and machine learningI am open to requests, but won't always say yes. If there's something that you'd like explored, participate on LinkedIn and let me know what you are curious to see.What else will be covered?If I'm teaching anything related to networks, Natural Language Processing will be included. NLP and Networks go together like <two things you like together>. Peanut butter and jelly, peanut butter and honey, steak and pepper, classical music and heavy metal.I will also be discussing Network Science and Social Network Analysis, and differences between the two, and how they are used together.I would also like to dabble with causal discovery, but am still exploring and learning. I will do my best to include it in this adventure. We have time.My first use of Network Science was in mapping out dataflows and identifying critical points. I haven't discussed that much, and I'd like to show how I do this. I wrote very briefly in my book about this, but can spend a day or few on it in this adventure.Finally, Machine Learning and Data Science is a given. Final ThoughtsI think this is going to be so much fun, and it gives me a creative outlet. It's useful and therapeutic to have a creative outlet, and the structured approach will allow me to expand my knowledge on what I am interested in, and I'm excited to be able to share this adventure with you as well, so that others can learn from this and get excited.Network Analysis is a useful skill. If you have any data at all, building skill in Network Analysis will very likely give you new opportunities and perspectives in using that data.I am going to disable comments on the blog, so that I do not have to spend my time moderating this. Please interact with me and the data science community on LinkedIn! Please join https://www.linkedin.com/feed/hashtag/?keywords=100daysofnetworks and learn with us! Feel free to use the #100daysofnetworks hashtag for your own adventure, and follow along!Finally, if you like what I'm doing, please buy a copy of my book. You will learn from me during this adventure, but I also put a lot of time and energy writing a book about it. And after reading, please leave a review on Amazon so that others will find my book! Thank you!","['Artificial Intelligence and Machine Learning', 'Community Detection', 'Egocentric Network Analysis', 'Graph Algorithms', 'Graph Visualization', 'Subgraphs and Connected Components']"
/work/blog_data/137283125.day-2-of-100daysofnetworks.html,"Welcome to day 2 of #100days of networks. If you would like to learn more about networks and network analysis, please buy a copy of my book!Today, we are going to start with the WHAT and WHY behind understanding networks. I'm going to explain what networks are, where we can find them, and why they are useful to explore, analyze, and understand.Let's do that, real fast, and I'll discuss deeper in the remainder of this post.WHAT: a network is just a manifestation of things and their relationships. WHERE: networks are all around us, and network data is easy to get.WHY: being able to analyze networks gives us new ways to understand the world and universe.You should study network analysis, because networks are everywhere, and learning how to do this will give you skill to be able to analyze and explore data in new ways.Ok, back to the beginning. What even is all of this? What are we going to discuss during #100daysofnetworks? What am I going to discuss in this specific post? My plan is to start at the beginning, describing networks and parts of networks, and describing why you should care.In this post, I will discuss the following:What is a network?What is a node?What is an edge?What is a community?What is a subgraph?If you read my book or followed along with the first iteration of #100daysofnetworks, then you probably know the answer to all of these questions, but I am hoping to introduce this topic to more people, so it is important to start at the beginning. Today's discussion will be about networks, their parts, and how they manifest in seemingly everything around us.The point of this is to show how pervasive networks are and to explain that being able to explore and interrogate them gives us new abilities in understanding life and the world around us. Life is not flattened, structured data. Life is complex.What is a network?As mentioned above, a network is just a manifestation of things and their relationships. I'm sure that a more academic definition can be found, but at the end of the day, a network is things and relationships.A graph, on the other hand, is a representation of a network, for use in analysis, prediction, etc.Often, I will use the terms graph and network as synonyms. That's common. People will often use the following phrases:Graph TheoryNetwork ScienceSocial Network AnalysisGraph Data ScienceThere is so much overlap. I personally think of all of these as related. I apply Network Science when I am doing Social Network Analysis. I do not call what I do as Graph Data Science, but other people do. I like to keep things simple and just consider all of these as parts of Network Science, similar to how there are different domains in Data Science, or different domains in Software Engineering. That's how I think about all of this. A network is a manifestation of things and their relationships, and a graph is a representation of a network. That's how I see it. A network exists in the real world, and we often cannot know all of its parts. We can be aware that the network exists, and we can be aware of parts of the network, but we cannot see or understand everything. A graph is our representation of what we know of the network. Perhaps we have--on paper--created an edgelist after watching people's social interactions and hand-drawn the network of how people have interacted. We've created a graph. If we add arrows showing the directionality of the relationship, we've created a Directed Graph, etc, etc. But I slip up all the time. I'll have a whole day where I will call everything a network. Sometimes this is intentional. If in one sentence I call something a graph, and then in the next sentence I call something a network, a person may think I am talking about two different things. Or if they are from cybersecurity, they'll by default think I am talking about a computer network.So, graphs, networks, they're synonymous to me, and I work with them every day. You can be more strict with yourself, if you like.Regardless, networks exist in the real world and they are represented as graphs, and we can use graphs for analysis, prediction, and so on. Graphs are simultaneously a tool, a map, and a usable data structure. When they are visualized, they are often beautiful, like art.Networks are all around us. Here are some examples, and we'll cover several of these during this adventure.People NetworksAdversaries and Allies (Social Network)Collaboration Network (Authors, Teams, etc)Communications Network (Email, Tweets, Telegram, etc)Computer NetworksMusic NetworksSongs and GenresArtist CollaborationsSong Evolution (Song -> Remix)User Songs (for recommendations)Data NetworksEntity Relationship Diagram (RELATIONSHIP is a giveaway)Dataflow Diagram (Code -> Data)Amplification (websites, social media accounts, etc)Knowledge GraphThis is a very short list, just off the top of my head. What other kinds of networks can you imagine? Think about what we are doing when we try to 'network' with other people. We are attempting to start some kind of relationship with other people so that we can find opportunities. Analyzing networks is another way to identify opportunities or understand reality.Here is a visualization of the social network from Les Miserables. Here is the same network with labels.What is a node?As mentioned before, a node is a thing in a network. A node can be a person, a song, a food ingredient, an outcome, a computer program, a data file, a database table, etc. Use your imagination. A node is just a thing.To keep things simple, hold the idea of a node as being a person or a computer. We all understand that human relationships are a thing, and we are all probably aware that computer networks exist. Computers are nodes on a computer network. People are nodes on a social network.A node is shown on a network visualization as a dot or circle.What is an edge?An edge is a relationship between two nodes. Put another way, an edge is a relationship between two things. Put another way, things have relationships with things, and they are portrayed as an edge.An edge is shown on a network visualization as a line. The edge is the line that exists between two nodes, the line that exists between two dots or circles.I am one person. You are another person. If you are reading this, you are interacting with my words. We now have an author/reader relationship. We now have an author/reader relationship. I am one dot, you are another, and there is a line between us. In the real world, nobody can see that line, but it exists.What is a community?A community is a group of connected things. Typically, when we talk about communities, we are talking about living things. However, there is such a thing as communities of websites, communities of social media accounts, etc. We could say that that's because there are people behind those websites and people behind those accounts, and I'd agree with that. But community detection algorithms are useful beyond studying living things. I will show how to identify and visualize communities in the near future. We will use community detection very often.To keep things simple, going back to our idea of people networks, a community would be a network of connected individuals. For instance, families are densely connected. We tend to interact with people we live with. Work networks are less densely connected, and there are clear cliques/communities that work together. If you were to construct a network of every single person on the planet, it would be sparsly connected, and it would also include communities. If you are reading this blog post, you are probably part of the IT community on LinkedIn, and the IT community has smaller communities for Data Science, Cybersecurity, Data Engineering, etc. Try to think about communities that you belong to, online and offline.In a network, a community is a group of connected things. In life, we are connected to people we interact with.What is a subgraph?In a network, a subgraph is similar to a community. A community IS a subgraph of the whole graph/network. Let's keep this simple:Graph: representation of the entire networkCommunity: connected things that are part of that graph; a smaller sectionSubgraph: a smaller section of the entire graphA subgraph is a smaller section of a larger graph. You can extract a part of the whole network for analysis, rather than working with the whole network.And a community is also a smaller section of a larger graph.But a subgraph does not need to be a community. For instance, if I wanted to see the subgraph of the whole graph that contained three nodes from community A and three nodes from community B, we'd likely end up with two separate networks. If you visualized it, you'd see two clusters.A community is a subgraph, but a subgraph is not necessarily a community. For instance, here is a subgraph taken from the larger Les Miserables network.We will explore subgraphs more throughout this adventure, as they are extremely useful.That's Enough for TodayI hope you found this to be an enjoyable read, and I hope my explanations made sense. This blog post was written quickly. If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Community Detection', 'Graph Algorithms', 'Graph Visualization', 'Subgraphs and Connected Components', 'Whole Network Analysis']"
/work/blog_data/137309533.day-3-of-100daysofnetworks.html,"Welcome to day 3 of #100days of networks. If you would like to learn more about networks and network analysis, please buy a copy of my book!Today, we are going to talk about CENTRALITIES. Network Centralities are a useful tool to quickly identify interesting nodes (people, things, etc) from any network. Once you have built a graph, you should use centralities to get a lay of the land, to ""learn the main characters"", so to say.In today's exercise, we will use the Les Miserables graph from NetworkX, to keep things simple.You can use my Github code to follow along.Here is a bit about centralities:Degree Centrality: Importance based on the number of degrees (edges)Betweenness Centrality: Importance based on whether a node sits between other nodes; Information flows through them. Can also be gatekeepers. They have power.Closeness Centrality: Importance based on a nodes closeness to other nodes. Has to do with number of steps away.PageRank: Importance based on number of inbound and outbound edges. Inbound is more important than outbound.There are many, many, others.In today's exercise, I will show how to use the above, as well as another algorithm called HITS, which is used to identify hubs (many outbound edges) and authorities (many inbound edges).These are what I consider ""starting centralities"", in that I always use them, for any graph, to get a lay of the land, so to speak. However, use will depend on the scale of the network. If you are below million scale, then all of these should work, but betweenness and closeness will gradually slow down to the point of being impractical. If you are above million scale, PageRank will be your go-to algorithm. PageRank was created by the founders of Google and it scales well. Betweenness and Closeness Centrality do not scale well, but they are very useful on smaller networks, or on subsets of networks.Network Spot CheckBefore doing anything with any network, it is useful to do a few spot checks. For instance, it is useful to know the size of a network before choosing algorithms for working with the network. However, I have used this particular network several times and know that it is small and that none of today's algorithms will have any issues with a network of this size, so let's keep going. I'll show you how to get useful network metrics for Network EDA (exploratory data analysis) on another day. For today, as this is a small network, let's just visualize it.This will do nicely. Click on the image to look closer. What insights can we gather simply by LOOKING at the visualization?Valjean is clearly an important character. He sits in a very central position in the network, and there are complex relationships that exist around him, shown by the network structures (clusters) nearby. To get from one side of the network to the other side, paths go through Valjean.Gavroche is another important character. He is well-connected to very dense parts of the network, but more connected than others.Myriel is another important character, as indicated by the different node color.There are several clusters of densely connected nodes. These clusters form interesting communities and should be explored both as a single entity, but also at the single node level. How do individuals in these communities behave? How do the communities as a whole behave compared to the rest of the network? What sets them apart?There are many nodes with a single edge. These are support characters. Napoleon is one. Napoleon is an interesting character in history, but is only linked to a single character in the book.There are no isolates in this network. Isolates are nodes without any edges. They will show in a network visualization as just a dot, orbiting on the outside of a visualization. Network visualization software typically keeps them away from the connected parts of a network.Nice, so we can see a few things:There are important nodesThere are communities (community detection will be useful; we will cover it later)There is only one giant cluster in this network. Often, in real-world networks, there will be several complex and large ecosystems that exist. Because there is only one big structure in this network, our work will be easier.But today's exercise is about centralities, and this was just the spot check. Let's keep moving. This spot check looks good. We can see the network, and this network is small enough that we can use our own eyes for insights. But let's use algorithms to make our work easier.I'm going to keep my descriptions as simple as possible. If you are interested in the math or research behind these algorithms, please check the links below, and read the research papers mentioned on NetworkX.Degree CentralityHere is more information on Degree Centrality.Degree Centrality has to do with the number of degrees (edges) that nodes have. For instance, if most nodes in a network have 1-3 edges (lines), and a few have (20-30), then those few nodes are probably very important. Because for some reason, they sure have a lot of connections. Here are the top ten characters by Degree Centrality. We were able to use our eyes to see that Valjean and Gavroche were two of the most important characters, based on their position in the network, and this shows the same. It also tells us other important characters that we should look into.Degree Centrality does not care about the direction of edges (lines) from nodes to other nodes. It just has to do with the total count. It is direction agnostic.Degree Centrality should work well on graphs of any size. I have used them at million scale. It should work at billion scale.Betweenness CentralityHere is more information on betweenness centrality.Betweenness Centrality is actually my favorite centrality, because it has to do with information flow. Let's pretend there are three people (A, B, C).A - B - CIn order for Person A to share his idea with Person C, he needs to go through Person B.In the same way, if Person C wants to share her idea with Person A, she will need to go through Person B.Person B is very important in this situation, as all information must go through that person.Betweenness Centrality has to do with paths that flow through nodes. For instance, For Person A to talk to Person C, the path goes A -> B -> C. For Person C to talk to person A, the path goes C -> B -> A. In a network with any complexity, there are many paths, and these are used in calculating Betweenness Centrality scores.These are the top ten characters by Betweenness Centrality. Look at this visualization, and then compare it to degree centrality. Notice how different Valjean's betweenness centrality is from everyone else's. Also notice that Gavroche is not in the second position for Betweenness Centrality. Myriel holds that position. Find Myriel in the full network visualization and try to see why. Betweenness Centrality slows down as networks grow in size, as there are more nodes, more edges, and thus, more paths. It is a useful algorithm at thousand scale, but once at million scale, you might want to use PageRank as your primary algorithm for determining node importance. At thousand scale, I like to use Betweenness Centrality AND PageRank as my primary algorithms for importance. I use PageRank at every scale.Closeness CentralityHere is more information on closeness centrality.Closeness Centrality is another algorithm that uses paths in its calculations, like Betweenness Centrality. That means that it suffers from the same problem. It is useful on small networks, at thousand-sale, but it is impractical for million or billion scale.However, this is a very useful measure. It indicates nodes' closeness to other nodes. Put another way, is a node in the city, or out in the woods? A persons opportunities are partially dependent on environment.Personally, I always include this measure in my analysis, but it is more for added context, and less useful to me. Other measures give me more valuable insights (such as Betweenness Centrality). But I always include this, for context.In terms of closeness, Valjean and Marius are in the first two positions, and Valjean stands out compared to the others. The others are pretty similar.PageRankHere is more information on PageRank.PageRank is extremely useful. It was created by the founders of Google and part of how Google search worked. PageRank has to do with inbound and outbound links, so directionality is implied, but it also works well with undirected networks. Our Les Miserables network is undirected. PageRank clearly identified Valjean as the most important character, with Myriel in distant second place, and Gavroche in third.PageRank is a great algorithm for importance. In my experience, it is always useful. Get used to including it in your analysis.PageRank was created for the internet, which is a BILLION SCALE network. PageRank works well at any scale.HITSHere is more information on the HITS algorithm. HITS is a very cool algorithm that identifies hubs and authorities. Hubs are nodes that have many outbound links, and authorities are nodes that have many inbound links.For instance, if a website is linked to by ten thousand websites, then that website is possibly an authority on whatever content they publish. Or, on social media, if an account is retweeted by a million other accounts that account is possibly an authority on whatever they talk about. However, that is in an ideal world. We live in a world of artificial amplification, where bots and blogs provide artificial amplification, but that is for another day.A hub, on the other hand, has many OUTBOUND links. For instance, there are some cool websites that link to the weirdest news on the internet. One website might link to one thousand websites. One website is sending internet traffic to one thousand parts of the internet. That one website is a HUB.Authorities: many inbound edges (lines, links).Hubs: many outbound edges.Today's network is undirected, and this algorithm needs a directed network to be most useful. However, it will still work with an undirected network, just the two result sets will be identical. Here are the two visualizations:Notice that the two visualizations are identical. If we did this with a directed graph as input, the two visualizations would be distinct and more useful.This algorithm isn't particularly useful today, but it will be useful with any directed network. And even though it was unable to discern hubs vs authorities, it still identified Gavroche and Valjean as the two most important characters, based on network position.So, which is better?Ok, cool. So, we looked at a few choice measures for centrality, but which one is the best? NONE OF THEM ARE THE BEST. They have different uses. Personally, if I have to pick only one, I'll use PageRank, but PageRank doesn't say much about betweenness or closeness. If I'm in a situation where I will only pick one, it's typically a scalability issue. PageRank does well at any scale, and Closeness and Betweenness Centrality do not. Degree Centrality easily scales. Certain algorithms work at even billion scale, and some become unpractical beyond thousand scale.None of them are better, but you should know several, what they do, and where they do not work well.My networks are billion scale, so I am usually looking for algorithms that scale well. However, it would be a total rookie mistake to write off any algorithms simply because the network is too large. For instance, you can extract a subset of a massive network and then all algorithms will be useful. And, as I mentioned before, most networks have several large clusters, so networks are commonly analyzed in pieces, anyway.Final TakeawayThere's something you should keep in mind with regards to using centralities, PageRank, HITS, and other algorithms for determining node importance. Importance is calculated based on a nodes placement in a network. It has to do with position and surroundings, but this is network context.It is important to think from a network perspective, but do not forget that the network is just the map. Just because two people are connected does not mean they are equally influential. One of them might be stupid and ignored, just tolerated. One of them might be less connected bully, but influential by might. One might be more influential in terms of messaging. And there are always other layers to the network that have not been built into the network. For instance, if a person from one end of the network etched something into a tree and a person from the opposite end of the network read it, then information flowed in another way.These are tools that are useful from a network perspective, but there is more to the story.What are you waiting for?If you find this content interesting, please jump in and give this a try! Install Jupyter or use Google Colab and start exploring. You don't need to know everything on day one. Just get started. Learning to work with networks and explore relationships is powerful, and this skill becomes tremendously useful the deeper you go.That's Enough for TodayI hope you found this to be an enjoyable read, and I hope my explanations made sense. This blog post was written quickly. If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Community Detection', 'Graph Algorithms', 'Graph Visualization', 'Subgraphs and Connected Components', 'Whole Network Analysis']"
/work/blog_data/137309643.day-4-of-100daysofnetworks.html,"Welcome to day 4 of #100days of networks. If you would like to learn more about networks and network analysis, please buy a copy of my book!You can find the code for today's exercise here.Today, I am going to show you how to ZOOM IN on any part of a network. We've made good progress on this adventure, so far, and we're following a logical path.Day 1: We discussed expectations for this journeyDay 2: We covered network basics and did whole network visualizationDay 3: We talked about centralities and other ways to identify important nodes and edgesDay 4: We are going to learn how to zoom in on those important nodesWhy would we want to ZOOM IN on important nodes? Well, there are different ways to look at any network:Whole Network Analysis (WNA): you can learn about the overall shape and size of a network. All networks are unique. Even the same network will be unique, if looked at temporally, as networks evolve over time. Whole Network Analysis is just a snapshot in time.Egocentric Network Analysis: this is what we are doing today. Zooming in on individual nodes will tell you about an ego node's connections (alters), and a bit about alters' connections too.Community Analysis: If Whole Network Analysis is at the WHOLE NETWORK scale, and Egocentric Network Analysis is at the NODE neighborhood scale, then community analysis is zoomed out a bit from Egocentric Network Analysis. In community analysis, I'm looking at groups of nodes. I am less interested in single nodes. I am more interested in how nodes behave together, or collaborate.But today's discussion is on Egocentric Network Analysis. We are going to ZOOM IN on nodes of interest. That is the simplest way to think about Egocentric Network Analysis. It is less complicated than it sounds.Whole Network - Spot CheckALWAYS, it is a good idea to start any network analysis by doing Whole Network Analysis. However, we've looked at this network many times and know that it is small and simple enough to visualize, so let's do that, and use our eyes for insights.This should look familiar. We've looked at this a few times by now. You should be able to see a few key nodes and a few key groups, even without looking closer.Next, I am going to show you how to ""zoom in"" on any node in the network. Scroll up and try to identify all of Claquesous's connections. It's very difficult to do, because he is part of a denser area in the network. For this, we need to be able to look closer.The best first option for looking closer is to look at a node's Ego Graph. In an Ego Graph, the node of interest (Claquesous) is in the center, known as the ego node. All of the node's connections are shown as connections around the ego node, and they are known as alters. The two things to keep in mind: ego and alters. The ego is in the center, the alters are around it.A very cool thing that can happen in an Ego Graph is that you will also be able to see alters' connections to other alters. Rather than an Ego Graph simply being a star, sometimes there are other connections that can be interesting. In those cases, you can look closer with your eyes, or you can take another approach, which I do often: drop the center, and the alters will show as isolates and small groups.I will attempt to show all of this in this notebook. First, let's use PageRank to identify the most important nodes in this network.In the code, I show an easy way to extract a list of the top N nodes and use them for Egocentric Network Analysis in our next steps. I also show how to do each of the individual visualizations shown below. Please get to know the code, and try it for yourself!Now, let's look through the top five characters shown in the above visualization.ValjeanHere is Valjean's Ego Graph. Even without clicking the image for a closer look, I can see that there is one CENTER node (ego: Valjean) and lots of peripheral nodes (alter nodes). I can see that this is not a simple star network, but that there is some clustering on the center left, bottom right, and top center right. These are groups that exist in this Ego Graph. When an Ego Graph has plenty of complexity and is interesting to look at, one of my favorite tricks is to DROP the center. By doing this, it drops the ego node (Valjean) out of the graph, causing the graph to shatter into pieces, exposing the groups that exist in the graph. Let's do that.Even without clicking the image to look closer, I can see that the center node is gone and that the network has shattered into pieces. When a network shatters into pieces, it often exposes some of the things I've talked about previously:Connected Components: there are often several clusters of nodes still linked together. Above, I can see one large cluster on the left, and one smaller cluster on the top right. Look for a few dots situated closely together on the top right. That's the second group.Isolates: there are also often several isolate nodes, which are nodes with no connections whatsoever. Above, I can see five isolate nodes. They were only connected to Valjean. With Valjean removed from his own Ego Graph, these nodes became isolates.But most importantly, we've identified that Valjean is connected to two separate groups. The differences between these groups could make for interesting analysis. Why are they not connected? What do they do differently? And why are none of the isolates connected to anything else? What makes the isolates so utterly unspecial or special that nothing is connected to them?Let's keep moving. I am going to do the same for the next four important characters. We could do this for every single node in the network, and it would take a very long time to analyze, but a tremendous amount of learning could be done about the story of Les Miserables if network analysis was used along with content analysis to dig deeper. Thus, the marriage of Network Science, Social Network Analysis, and Natural Language Processing is special and important to me. Moving on.For these next characters, put your thinking caps on. Look at the images and try to answer the questions I ask.MyrielMyriel's Ego Graph is almost a star network, but there are three characters on the right who are connected with each other. Myriel has a high PageRank score because of the number of edges, but Myrie's Ego Graph is very simple. If we drop the center node, what do you think will happen? How many isolate nodes do you think we will see? How many groups will we see?As expected, dropping the center node shattered the network and left one small group and several isolate nodes. I can see seven isolate nodes and one small group. What is this small group that Myriel was a part of? What do they believe and do? Who are their members? How do they know each other?GavrocheLike Valjean, Gavroche has a very interesting Ego Graph. I can see the one center node. How many groups do you see? A group can be two people. If we drop the center node, how many groups do you think we'll be able to see? How many isolates?This graph actually tricked me. I expected that there'd be three groups, but that is because I simply was not looking closely enough. In the earlier image, It looked like there were three groups: top left, bottom left, and bottom right. There are three groups. However, a few people in the bottom group had connections with the top group, so dropping Gavroche was not enough to split these two groups. They have some cohesion. Did you guess the number of groups correctly? How about the number of isolates?One of the groups was Child 1 and Child 2. What is their relationship with Gavroche?What is the isolate's relationship with Gavroche?Finally, what is this larger cluster of characters? Why are two groups linked together, with or without Gavroche? Who are these people? How would the absence of Gavroche in the story affect these characters?MariusMarius' Ego Graph has some interesting complexity as well. I can see at least one densely connected group of nodes on the top left, and I get the feeling that this is actually two separate groups of people but that there is some cohesion with the top left group. I expect that this network will not shatter if we drop the center node and that there will be no isolates. What is your bet? Try to draw a mental picture of what will happen after Marius is dropped.As I expected, the group remained intact, even with Marius removed. Valjean is an important node in this network, and he has helped keep it together, along with others. Who are these people? How do they know each other? Why is this network so resilient? If these characters are important, what would it take to eliminate their ability to work together? On the other hand, what would bolster the network? JavertJavert's Ego Graph is the last we will do today, but we could go much further. Feel free to learn from my code and investigate every node in the network. It's a great way to explore and learn!What do you see? I see to central nodes: Javert and Valjean. I see one group of nodes on the left, and they are connected with both Javert and Valjean. I see some characters on the right who have connections to characters on the left. Because of all of this, if the center node is dropped, I suspect that this network will be resilient and not shatter. Because none of the nodes have fewer than two edges, I expect we will have zero isolates, because 2 - 1 = 1. Every remaining node will have at least one edge to another node. The network will remain intact. Essentially, this is a large group of connected individuals.As I suspected, the network did not shatter. After dropping the center node, all remaining nodes are still connected with other nodes. The dense group is a little more discernable.What is this group? Why are two very important characters so central in this network? What are the Takeaways?It is fun to explore any kind of social network, no matter the topic. You can learn a lot about any topic by exploring the social networks that exist inside that topic. In today's exercise, the topic is Les Miserables. We could have taken the raw text of this story, used the techniques from my book, and literally converted raw text into an explorable network. We can use the text of the book alongside the network to learn more about individual communities, and I will show how to do this at a later date. This is new material that is not included in the first edition of my book.This exercise also showed that different shapes of networks are more resilient to attack. For instance, if you take a star network (the second character) and drop the center node, the network shatters into pieces. If you take a more densely connected network and drop even the most important node, the network can still remain intact. What are the implications of that for cybersecurity, for leadership, for national security, for teamwork, or for your own life? What fragile networks exist in your own life? What resilient networks exist in your own life?For instance, in my own life, I am part of the LinkedIn Data Science community, and regularly post content and participate in conversations. That is a densely connected network, and that network would not be affected by my absence, or any one person's absence. It would just continue to grow and evolve. That's a resilient network that exists in my life. How about a fragile network? I have very few friends I hang out with in person. In a small group, if one person is removed, the effect is devastating on the group.Let's Zoom Out a LittleI hope you have learned a bit from these discussions. We've already covered enough material for you to jump into network analysis. We haven't talked at all about network construction, but we've found a network to use and learn from. I promise, very soon, we are going to construct our own Graphs, not use something pre-made. I enjoy using networks to explore reality, not just use someone else's networks for learning. What are you waiting for?If you find this content interesting, please jump in and give this a try! Install Jupyter or use Google Colab and start exploring. You don't need to know everything on day one. Just get started. Learning to work with networks and explore relationships is powerful, and this skill becomes tremendously useful the deeper you go.That's Enough for TodayI hope you found this to be an enjoyable read, and I hope my explanations made sense. This blog post was written quickly. If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Attack Simulation', 'Community Detection', 'Egocentric Network Analysis', 'Graph Algorithms', 'Graph Visualization', 'Subgraphs and Connected Components', 'Temporal Network Analysis', 'Whole Network Analysis']"
/work/blog_data/137309753.day-5-of-100daysofnetworks.html,"Welcome to day 5 of 100 Days of Networks!Wikipedia Edgelist GeneratorIf you would like to learn more about networks and network analysis, please buy a copy of my book!Today, I have a special treat for you! I created a Wikipedia Edgelist Generator that you can use for knowledge discovery on any topic that interests you. You can find the crawler / edgelist generator here!I wanted to build a tool for knowledge discovery, and something that could be used no matter the topic to create complex networks that are more interesting than the small networks that come with NetworkX, and more interesting than using somebody else's dataset. To me, there is nothing more interesting than my own research, and I don't like using other people's datasets for learning. I prefer to analyze my own constructed networks, and do two things at the same time:Improve my network analysis skillsLearn something about a topic of interest at the same time.Today, I created the edgelist generator, and you may use it. I have added some guidance regarding iterations and sleeps at the top of the notebook. Please be responsible or Wikipedia will block your IP and you will get nothing.Knowledge DiscoveryThe point of the edgelist generator is knowledge discovery, on any topic. For instance, to build today's dataset, I searched for four things:Network ScienceSocial Network AnalysisGraph TheoryCausal InferenceIf you look at the code on github, you will be able to see where and how I did that. After four iterations/loops of the crawling and edgelist generation, those four searches built a network of OVER 9000!!! nodes. That is why I call this knowledge discovery. Each of those 9000+ nodes is a Wikipedia page on a related topic. You will understand this more if you continue reading below.If I had done five iterations instead of four, I might have ended up with 50,000 nodes or more, which is more than I wanted for this dataset, and would query Wikipedia's API harder than I wanted to do. You should start with two iterations, check your results, and then increase RESPONSIBLY. Network AnalysisI created a second notebook (which is nearly a duplicate of Day 4 but with today's data) for analyzing this network data. You can see the code/notebook here!Previously, we used the Les Miserables network to learn a few fundamentals. From now on, we will use Wikipedia networks, as they are complex and more meaningful than character names. We can literally use the node names to continue our research into any topic of interest. Today's created network is far more complex than Les Miserables.Complex real-world networks often look like this, when rendered. This looks useless, like a big spiderweb that we cannot hope to pull insights out of, except maybe a few of the peripheral nodes sticking out, but that is completely wrong. A lot of people get stuck at Whole Network Analysis (WNA), and this series will absolutely show you how to pull insights from any network, simple or complex.The network is made up of 9,204 nodes and 14,140 edges. That's not bad for about 20 minutes of querying Wikipedia!For today's update, the edgelist generator is the most important thing, as it is useful and we will use it to create interesting datasets during the course of this adventure. I have other topics in mind that I would like to understand.Today, I chose the four 'seed searches' because they are all related:Network Science is a broader domain, like Data ScienceSocial Network Analysis falls under Network ScienceGraph Theory was the origin that led to Network ScienceCausal Inference uses directed graphs to infer causalityI was especially interested to see the overlap between Causal Inference and the rest, and I will explore that in later days.The generator itself requires some understanding, so I will keep the analysis light today. We will just look at a few ego networks, and discuss what we can see and do with the information.It is always useful to look at Page Rank and centralities, to identify important nodes. That is always a good place to start after a graph has been constructed.Very cool. We can see that the page ""Glossary of graph theory"" has a drastically higher Page Rank value than anything else. Let's take a look at the ego network for that node.This is a complex ego network! There is a lot of connectivity between the alter nodes, and this is not at all a star network! This is a complex ecosystem of information having to do with Graph Theory. But this is hard to read. Check out the Jupyter notebook and you will see how to get a list of nodes.What nodes have we uncovered? What interesting Wikipedia pages and topics have we found? Let's take a look. Here are just the first twenty nodes out of seventy-seven:Acyclic graphArborescence (graph theory)Biconnected graphBipartite graphBlock graphBridge (graph theory)Cheeger constant (graph theory)Chordal bipartite graphChordal completionChordal graphCircle graphClaw-free graphClique (graph theory)Clique graphComplete bipartite graphComplete graphComponent (graph theory)Cubic graphCut (graph theory)Cycle (graph theory)Very cool. I can already see several things I have never heard of. This could lead me down some interesting rabbit holes of discovery and education. Each of these is a separate Wikipedia page, and you can also search other sources on the internet, such as Arxiv. Let's look at more interesting ego networks.Here's the ego network for ""Graph (Discrete Mathematics)"". You could do the same thing with this one: extract interesting topics, and then go learn about them. Let's look at another.I thought this one was interesting as well. There's a lot I've never heard of that make me curious to learn more. Let's look at another.This (above) is the ego network for ""Graph Theory"", one of the original searches. There's lots of interesting topics, and even a page relating to Graph Databases.Here's a cool ego network relating to Artificial Intelligence. I can see cool topics such as Causal AI, Social Intelligence, Philosophy of Artificial Intelligence, Ethics of Artificial Intelligence, and more.Apparently, one of the picked up Wikipedia pages had to do with fallacies and that ego network is interesting as well. This would be a very interesting rabbit hole to continue down. Perhaps it might be interesting to use ""List of fallacies"" as a seed search for the edgelist generator and see where it leads us. Maybe I will do that on another day.What's the Takeaway?I've long said that the internet is a goldmine for discovery and analysis, if you learn how to use it as such. That is the reason for my obsessions in Natural Language Processing and Network Science. Natural Language Processing gives me answers regarding content and context. Network Science helps me understand relationships and flow.What I've demonstrated today can be a very useful tool for anyone to use for learning. You don't have to use my seed search terms. You can use your own. You could research any topic at all that interests you. For instance, I will use this to build a network relating to some of my favorite scientists and science fiction authors.I want to encourage you to JUMP IN and try this stuff out. It feels good to create your own networks and do your own network analysis. You can share edgelists with the community, and you can discover insights that you would likely not discover, otherwise.And now, we have a tool that we can use to make this #100daysofnetworks adventure a lot more interesting, beyond using stale NetworkX network generators (Les Miserables, etc) or other people's datasets. Research what interests you, and then use that data to build skill. Then the skill sticks, and you learn neat things in the process.If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Artificial Intelligence and Machine Learning', 'Egocentric Network Analysis', 'Graph Algorithms', 'Graph Visualization', 'Subgraphs and Connected Components', 'Whole Network Analysis']"
/work/blog_data/137309849.day-6-of-100daysofnetworks.html,"If you would like to learn more about networks and network analysis, please buy a copy of my book!Order From Chaos!In Day 5 of 100daysofnetworks, I created a Wikipedia crawler that could be used to investigate any topic that is of interest to you. You should really use it. It is a lot of fun to use and creates useful network data.What do I mean by useful?Well, today, I'm going to literally convert a graph into a learning curriculum for myself. I am going to take this:And turn it into this:In other words, I am going to turn a complex network into a clean learning curriculum that I can use to learn more about Network Science. I will use this to learn new things, and I will explore them during this series.Network Spot ChecksTrue story. When I started working with networks, I got a lot of push back for using them as people could not understand how they could be useful. There is too much information in one visualization, and the more complex the network, the more difficult it is to visualize. Look at this mess (scroll up). How are we supposed to pull anything useful out of that ball of yarn?It is not difficult if you know how. My book shows how, and this series has shown how as well. In the previous days, I've already shown a bit of how we can zoom in on parts of networks. For today's work, we are going to be zooming in on communities, but our final approach involves essentially ""peeling the onion"" layer by layer. I will show how. This is a cool technique, and not shown often. I will get to that after the Community Detection Work.Community Detection ApproachOne thing that I have found to be very useful is to look at content from a community perspective. Like attracts like. Pages that are related will link to each other, forming clusters, or communities.You can see today's code here.Check out the code to learn how to do the Community Detection work. I'm only going to write about what I see in blog posts, not describe code.This is the CORE of the largest community in the network. Pay attention to the k_core piece of code under the largest community and you will see that I am showing only nodes that have five or more edges. I only do that for this largest community. The other communities, I show completely as they are much smaller. Looking closer at this, I see many mentions of the word ""graph"". We are in the right place.What other pages are part of this community? Let's see a list of only pages that are in this community.Super easy. just like that, we can see the nodes that are part of any community, and in this case, this community is Wikipedia pages related to Network Science and Social Network Analysis. This is a clean list of relevant pages.Here is another community:Look closely! That is a computer science and computer graphics community. That community could actually be split into two communities, and there are only two bridge nodes holding the two communities together. See if you can identify them. Let's look at another:Look closely! This is a community that is related to Artificial Neural Networks. This would actually be a cool one to study, if interested in Machine Learning. What are some of the relevant nodes/pages?This would be a pretty cool starting point for learning about Neural Networks. Let's look at another network.That's really cool. It's a network of different sciences and how they relate to each other. This network would be interesting to use as seed nodes, and then redo crawling, to identify how various sciences are related. It is a small community, and incomplete, but it wasn't intentional for this to end up in the data, and that is very cool that it happened! One more!This is a communication network network! This is related to network science, in that these can be analyzed with network science.There are many more communities in the data, with a network this large. I encourage you to play with the data and to explore the networks. You will not learn this without practice. With practice, it becomes easy and intuitive. I have shown that we can look at Wikipedia data not only from a whole network perspective but also from a community perspective, and the community level is a good way to hook onto the signal that you want. That's how I think of it, at least. Networks and data can be used together. They do not need to be separate things. They are powerful when used together.K_Core and K_CoronaI use two techniques for exploring the layers of a network:K_core allows me to see the core of any network, which helps understand what is most important or influential to a network. If you give K_core the highest degree value that it can handle, it will show the most connected nodes in a network.K_corona allows me to PEEL THE ONION. Think of a network as an onion. K_corona allows me to peel the onion, one layer at a time, based on the number of degrees a node has.Using K_core, I can see that the maximum depth I can go is six edges.If I go past six, the code will fail. So, six is the number to keep in mind. Now that we have identified this using K_core, we will use the values [6, 5, 4, 3, 2, 1, 0] in K_corona to see all nodes that have exactly six edges, then exactly five edges, then exactly four edges, then exactly three edges, and so on.Why?Because I can use this to build a study guide. The most connected nodes are more important than the least connected nodes, in terms of learning. The most connected nodes have to do with other nodes, and understanding how things relate allows us to build a strong foundation. Studying fringe material is less useful, unless you are solving some fringe problem that it relates to. There is a time to focus, and a time to zoom out.In the Jupyter notebook, I show simple code to convert the main Network Science community into a study guide. Please see that code.As a result, I now have a clean list of stuff to explore and learn more about.LISTEN CLOSELY. This can be done for any topic. I made available the Wikipedia crawler that can create the datasets to do this for any topic that you are curious about. For instance, I want to pull datasets about:ALF - remember that show?Science Fiction - A science fiction network would be awesome to exploreWork TopicsCompanies I want to learn more about, and their relationshipsPeople I want to learn more about, and their relationshipsAnd so on. Network Science is all about RELATIONSHIPS between THINGS. Networks are essentially things and their relationships. This madness:Is really just a picture of THINGS and their RELATIONSHIPS with other THINGS. In this case, this is a network of TOPICS and their RELATIONSHIPS with other TOPICS. So, don't be afraid of networks or complexity. You just can't analyze these the same way as you would a spreadsheet or language.Finally, if you are interested, I threw the study guide into a document and made it available for everyone. You can access it here. Goal: Show Something UsefulMy goal for today's post was to make and show something useful. Unless you have a reason to study networks, there never feels like a reason to study networks. Same goes for any topic. But here is the thing: networks are all around us, and network data is easily accessible, even if you have to generate it yourself. If you are creative, you can use this to your advantage. You don't have to use everything for work. I have used networks to create vocabulary lists, building a network of Jane Austin's word use, and then extracting nodes with only a single edge (words only used once). I have used networks to understand the flow of ideas across space and time.I have used networks to troubleshoot server problems in minutes that used to take days.I have used networks to study how malware evolves, and to use that to detect undetected malware.And on and on and on and on and on.I think in networks, because life is networks. Life is not spreadsheets. Life is not lists. Life is people and things and relationships.So, I encourage you to learn to study networks, and this is a good place to start, as is my book, and I will recommend many other books during the course of this adventure.Wikipedia ExtrasIn the Jupyter Notebook, I also included some code for working with the Wikipedia Python library. I show how to use it to pull summaries and text for any of the nodes in the Graph. I thought that might be useful to spark creativity.Thank You!Thank you for following along on this adventure! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Artificial Intelligence and Machine Learning', 'Community Detection', 'Graph Algorithms', 'Graph Visualization', 'Subgraphs and Connected Components', 'Whole Network Analysis']"
/work/blog_data/137311107.day-7-of-100daysofnetworks.html,"If you would like to learn more about networks and network analysis, please buy a copy of my book!Graph Types (Batch 1)In Day 6 of 100 days of 100daysofnetworks, we converted our Network Analysis graph into an actual study guide we could use to learn more about networks and graphs!Today, I chose five types of graphs to explore from the study guide. You can see the study guide here. More specifically, today we are going to look at these five types of graphs:Cycle GraphDense GraphSparse GraphRegular GraphWheel GraphI recommend that you follow along with the code, today, as it explains how I did this and what I see. This blog post will be kept high level. Follow the code!We're not just going to LOOK at these kinds of graphs. I will describe their characteristics, and if you look at the Jupyter notebook, you can see how they differ from each other in terms of centralities and shortest paths.Network Analysis is exciting, and it's good to learn about different types of graphs, so that we have a name for them when we see them in the wild. Let's begin.I will use summaries from ChatGPT. As I mention in the notebook, if you use an LLM, you should validate the output. Let me know if you see any errors in the summaries. They look good to me.Finally, we have not discussed density much in regards to graphs. Density has to do with the interconnectedness of a network/graph. If every node is connected to every other node, the network will have a density value of 1.0. If zero nodes are connected with any other nodes, the network will have a density value of 0.0. Density tells a lot about how connected the network is.Cycle GraphChatGPT: ""A cycle graph is a type of graph in network science that forms a simple cycle, which is a closed path where each node is connected to exactly two neighbors except for the first and last nodes, which are connected to each other. Cycle graphs are fundamental structures in graph theory and are often used to model cyclic processes, circuits, and other scenarios with repeating patterns.In other words, in a cycle graph, nodes are connected to a single node to form a cycle.For example: A -> B -> C -> D -> AHere is the graph I built.Notice a few things:Nodes are connected to other nodes, forming a path in a single direction. A cycle would not exist if any of the arrows pointed the opposite direction. The flow would end on the first iteration.It's a long way from nodes on one side of the network to nodes on the other side of the network.It's not a dense network. Nodes are only linked to a single other node.The structure of the graph also affects its centrality scores, as well. Every single node in this network has a betweenness centrality value of 0.5. No node stands out as being more important than any other node.Shortest paths will be affected by the structure of this graph, as well:To get from node A to node C, the shortest path is A -> B -> CTo get from node E to node C, the shortest path is E -> F -> G -> A -> B -> CShortest paths between nodes are not equal. This is not ideal for information flow.If you wanted to make this more optimal for the flow of information between nodes, what do you think would help?Dense GraphChatGPT: ""A dense graph is a type of graph in network science where most of the possible edges are present, resulting in a high density of connections between nodes. In a dense graph, the number of edges is close to the maximum possible for the given number of nodes. Dense graphs are often used to model scenarios where interactions or relationships between entities are widespread and frequent.""In other words, in a dense graph, most nodes are connected with most other nodes. Here is the graph I built:Notice a few things:In this example, every node is connected to every other node. This will result in this graph having a density value of 1.0.We could have dropped an edge and the graph would still have a high density value. It is a dense graph, even with one less edge, and would still fit the definition.Even though I am showing arrows, they are bi-directional. This is not actually a directed graph. I am leaving arrow in for all of today's visualizations, to reduce code. It still makes sense and is correct. An undirected graph has bi-directional flow.Even the visualization FEELS dense. There's a lot more going on in this graph than in the cycle graph.The structure will also affect centrality scores. Check the notebook/code. Every node has a betweenness centrality of 0.0 and a degree centrality of 1.0. Essentially, all nodes are equally important, structurally.However, compared to the cycle graph, this graph is optimal for information flow. The shortest path between each node will always be equal, because every node is connected to all other nodes. The shortest path from A to C is simply A->C, and the shortest path from E to C is simply E->C.Sparse GraphIf the above is a dense graph, what do you think a sparse graph is? That's right. It is a graph where most nodes are not connected with all other nodes. It's the opposite of a dense graph. Here is an example:Notice a few things:Most nodes are not connected with all other nodes in the graph. In fact, zero nodes are connected with all other nodes in the graph.There is SOME connectivity in this graph. There is some stuff happening. That is called a connected component.There are lots of isolate nodes. Those are the dots. Dots are called isolates. They are nodes that have no edges with any other nodes. They can be very important, or not important at all, depending on the network you are dealing with.This is  a sparsely connected graph, or a sparse graph. It will have a very low density score. This graph has a density value of 0.027. The dense graph had a density value of 1.0. In the wild, you will mainly run into sparse networks, and that is not a problem. That is the nature of reality. Networks follow Power Law, or the idea that Rich Get Richer. A few nodes will have many edges, and most nodes will have few edges. A few humans will be billionaires, and most people will not. Etc.Betweenness centrality can be very useful in a sparse network, as can Page Rank and other centralities. If you look at the notebook, notice the following:Betweenness centrality found that one node was most important: node B. This is because it is the only node that sits between other nodes, or on the shortest path between other nodes.Degree centrality found that four nodes were important (A, B, C, D), but that one node (B) was most important.Shortest paths between nodes are also impacted by the structure of this graph. Scroll up and think about this: how would node E talk to node K? The answer is that it cannot. No information flow can take place between nodes E and K, or E and any other node, because node E is an isolate. It has no edges.How would node A communicate with node D? It would go A->B->D.The structure of a network has real world impact in how freely information will move in that ecosystem. Ideas are shared in a dense ecosystem. Information is essentially firewalled in isolates. Nothing is shared from or shared to isolates.Regular GraphChatGPT: ""A regular graph is a type of graph in network science where each node has the same number of connections or edges. In other words, all nodes in a regular graph have the same degree. Regular graphs are often studied for their uniform connectivity patterns and symmetry.""In other words, a regular graph is a graph where all nodes have the same number of edges. Here is my example:Our dense graph is also a regular graph, as all nodes have the same number of edges.So, there will be a few things to keep in mind:Density will be 1.0, as all nodes are connected to all other nodes.Shortest path between all nodes will be the same. There is a path from any node to any other node.Betweenness and degree centrality will have equal scores for all nodes, because none of them are different from each other, structurally.This is simple enough. Let's keep moving.Wheel GraphI had never heard of or played with a Wheel Graph before today, so this was fun for me. A wheel graph is identical to a cycle graph, but each of the outer nodes links to an inner node, creating what looks like a wheel. Here is my example:Look closely and notice a few things:Although this looks like a cycle graph, it is very different. All edges are bidirectional, not one-way, and there is one node in the center that connects to every other node.This is an information flow optimization over a cycle graph, as all nodes are equal distance from each other. The optimization has to do with the central node being added and with the fact that all edges are bidirectional. The density of this graph is 0.5. It sits between a dense graph and a sparse graph, right in the middle. There is room for many more edges.In terms of centralities:Betweenness centrality gives all nodes a value greater than 0, because all nodes sit between other nodes. However, one single node has a betweenness centrality that is 25x higher than every other node. Can you guess which one?Degree centrality gives all nodes a value greater than 0, because all nodes have edges with other nodes. Degree centrality has to do with the number of degrees (edges) that a node has, and all nodes in this graph have edges. However, one node has more edges, and has been given a value about 2x higher than every other node. Can you guess which node?So, both centrality scores were equally able to identify the most ""central"" node, but in my subjective opinion, betweenness centrality captures the relative importance better. Because if that node were dropped, information flow would be disrupted and shortest paths would be similar to the cycle graph where the distance from certain nodes would be far.In terms of shortest paths, check out this image:The shortest path between any node in this network is never more than three steps, because node H exists as a shortcut. To get from node A to node E, we can jump through H to get to our destination. Without H, we should have to go A->B->C->D->E.Graph Structure MattersI think of this as the structure of a graph, or the structure of a network. Different networks behave differently, and I hope I showed that today. There are real-world implications to this:If you manage a network of things, you can bolster that network by adding more nodes and edges. This is defense.If you want to disrupt a sparse network, you can often shatter it (think Star Wars Death Star) by disrupting central nodes, nodes with high centrality values. This is offense. Removing a central node has a massive impact in sparse networks, and can completely isolate parts of the network. Think about what that means beyond graphs, to people and things.Thank you!Thank you for following along! This was fun! We've only done five of the graph types identified on day 6, and it was quite educational already! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Artificial Intelligence and Machine Learning', 'Graph Algorithms', 'Graph Visualization', 'Subgraphs and Connected Components', 'Whole Network Analysis']"
/work/blog_data/137311139.day-8-of-100daysofnetworks.html,"Welcome to day 8 of #100daysofnetworks!If you would like to learn more about networks and network analysis, please buy a copy of my book!So far, through the course of this adventure, we've already learned some useful things such as:How to use internet resources (such as Wikipedia) to create network data that can be useful for learning network analysis.How to create network graphs using this data.How to identify important nodes in the network.How to identify communities in the network.How to explore ego networks in the overall network.This is already more than enough to be very effective in network analysis. This is because it is useful to be an explorer of networks and graphs, not just a user of graph data. I consider these as things to explore, not just things to use. By that, I mean that graphs are more than just input data for Machine Learning models, and that whole network metrics (overall density, number of nodes, number of edges, number of triangles) is just really not all that useful, in reality. When I do network analysis, I am looking for things, and it is never just a count of edges, or count of triangles, or a count of anything.With the above skills, we already know enough to get started exploring networks, so let's get away from the dry fact learning, and do something interesting. Let's use what we know and use it for MUSIC DISCOVERY! This is an excellent weekend activity.I have one goal: I want to learn three to five new things about my favorite band. I specifically set that to three to five, because it is easily possible to spend hours and hours and hours digging through a network. You should set goals so that you have something to aim for, and so that enough is enough. You can always return to an interesting network.Let's get to work!Wilco! My Favorite Band!My favorite band is Wilco. I have been listening to them since 1997. I love their folk/rock sound and their experimental nature. They aren't just folk music. They get almost jam band'ish at times. They play in chaos. But they are also really mellow, and I love their folk and rock roots. I play guitar, and I find their music approachable. I am insecure about my ability to play lead guitar, and their lead guitarists has a style and skill that is approachable, where other artists are much more difficult.Here's a picture of Wilco I found on a Google search.In the front is Jeff Tweedy, the singer and songwriter. He is easily my favorite songwriter and frontman. Here's one of my favorite Wilco songs. They're just a great band, and I love to listen to them when I need to relax. I love playing guitar along with them as well.You can learn more about them here.Yesterday, I used my Wikipedia crawler to create an edgelist for exploring the Wikipedia network that exists around Wilco. Let's get to it!Network Analysis TimeAs always, the code is available on Github.Datasets are also available. Use these to explore if you don't want to use the crawler!This time, the dataset was a little noisy/messy, so I used Wilco's ""ego network"" with a radius of 2 to create a purer Wilco graph. You can see how I did that in the code. I do not explain to do that in my book. It is a useful option for quick cleanup.After creating the network graph, the first thing I usually do is inspect the core of the overall network.This is one of the most interesting network cores I have seen in a long time. I saw a similar ""split nucleus"" (what I call it) when I inspected an NRA (National Rifle Association) graph a few years ago and could clearly see that there were two parts (politics + gun enthusiasts) in the network. I can see that there are two parts to this core: there's the actual Wilco stuff on the top left, and there's a bunch of stuff related to Roger Wilco on the bottom right. Because of the name, Roger Wilco was pulled into the dataset while crawling, using ""snowball sampling"". When using snowball sampling, you're always going to get some stuff you didn't expect, and it can be seen as junk or as something interesting. To me, this is interesting, not junk. Notice that it looks like this core would fly apart if someone took a pair of scissors and made a few snips in the middle. If we were to cut four edges, the network would cleanly split in half. We're going to actually explore network resilience soon and will play with that idea.I look at the core of every network to understand what is influencing the network. The core are nodes that are most connected in the network. The core is the foundation, and it can say a lot about the overall makeup of the network.But the core is not everything. Isolates (nodes with no edges) can be very important, too. However, the core gives me an overall picture of what is driving this network.Nodes of InterestAfter looking at the core of a network, I want to understand which are the most important nodes in the network. This can easily be done using Page Rank or one of many of the different kinds of centrality measures that we discussed previously, such as Betweenness Centrality. This is a small network of only a few hundred nodes, so I will use Betweenness Centrality (which is slow on massive networks) and Page Rank to identify important nodes based on their network positioning.First, here are the nodes that Page Rank found to be most important.And here are the nodes that Betweeness Centrality found to be most important.Here is an opportunity to build intuition: Look at the images without clicking on them to look closer. Notice that Betweenness Centrality looks very different than Page Rank. Why do you think that is? For both of them, Wilco and Jeff Tweedy are the most important nodes, but in Page Rank, the values are very different, and Jeff Tweedy has a much lower Betweenness Centrality than Wilco does. This is a good research topic, for learning. Get to know the centralities. They will quickly become intuitive, with use.With my goal of finding 3-5 new things about Wilco, I'm going to look closer and see if anything stands out, and turn it into a question:Who is Glenn Kotche?Who is Billy Bragg?Who or What is Uncle Tupelo?Who is Jay Farrar?Who or What is Tweedy?Who is John Stirratt?Now we are entering the realm of open source intelligence (OSINT)! Let's find out who each of them is and how they relate to Wilco!Glenn Kotche is the drummer in Wilco. Cool!Billy Bragg made two albums with Wilco. I love these albums!Uncle Tupelo was Jeff Tweedy's band before Wilco, from 1987 to 1994. Jay Farrar was part of Uncle Tupelo, Jeff Tweedy's band before Wilco.Tweedy is a band that Jeff Tweedy made with his son. John Stirratt plays bass in Wilco and other bands.Other than Billy Brag, I did not know about the rest of them! I struggle to memorize names, and I haven't kept up with the individual musicians in Wilco. So, already, I've identified two bands that might be cool to listen to (Uncle Tupelo and Tweedy) and found musicians that might be worth further exploration (Jay Farrar and John Stirratt). The goal of finding 3-5 new things is already met, and we are just getting started. That's real value, and amazingly quick insights.Community DetectionNext, I use Community Detection, to see how Wikipedia pages link together to form network communities. I prefer the Louvain Method, but Karate Club has a Machine Learning model called Scalable Community Detection (SCD) that is also excellent. I show how to use that in my book, and we will use it later, in this #100daysofnetworks adventure.Community Detection essentially separates a graph into a bunch of smaller graphs that can be explored separately. It is useful on massive graphs, to break them apart for separate analysis, and it is useful on small graphs as well, to detect the cliques and communities that exist.In the code, you can see more communities, but I will show and talk through just a few, here.This is the largest community in the network. The node for ""Billy Bragg"" stands out. The node is colored red because it has the highest Page Rank value, of all the nodes in this community graph. This community is related to the album Mermaid Avenue and Mermaid Avenue volume II. I highly recommend listening to those albums. They are great, especially California Skies and One by One.It makes sense that this is the largest community in the network, as this is one of their older albums. The album itself is a beautiful tribute to Woodie Guthrie. Let's look at another community.This community is very useful to me, as it is related to the band Uncle Tupelo, who I have not listened to. It was Jeff Tweedy's band before Wilco, so I should check them out! My goal was to learn 3-5 things, and I have already learned five, but it looks like there are a few more things to learn!Will I like the album Anodyne?Will I like the album No Depression?Are there other albums on Jay Farrar's discography page that I might like?Will I enjoy the rapper IDK?What is Day of the Doug? Is that a band or an album? Will I like it?I'm not going to chase those answers down right now. Let's keep going! That gives me something to explore when I need new music!Let's look at another community!This community relates to the video documentary ""I am Trying to Break Your Heart"". If you enjoy Wilco after reading this post, you will love that documentary. It's got great footage, and I have probably watched it over a dozen times by now. This introduces me to some new names and brings up questions:Who is Jay Bennett?Who or what is Kamera?What is the Conet Project? What's it about?What other documentaries has Fred Armisen made? Will I like them? What is his most recent?What is Sam Jones' Wilco photography like? Are any of his shots for sale? Are they affordable?What is ""What I Mean to Say is Goodbye""? Is this a movie, album, or what?Who is Bill Fay?Notice something. Everything that we explore in a network brings up more questions. Graph exploration can unveil a MOUNTAIN of insights. Keep a notebook next to you, or a way to keep notes on your computer. Networks are complex and gradually an overwhelming feeling builds up if you explore too long without taking breaks. Look around, write notes, look around more, write more notes, then walk away for a while and take a long break. Good network analysis isn't done in one shot. It is iterative. Find insights, ask questions, do more digging, repeat. Do this until you have as many answers as you need. There is no DONE. You are done when you have enough.Let's jump to Ego Networks, as I had the most success working with them for this analysis, more than communities. Check out the code to see more communities!Egocentric Network AnalysisWith community detection, we were interested in how pages linked together to form a community of nodes.With Egocentric Network Analysis, we are interested in seeing what nodes exist around a node of interest, and how they link to each other. The main node is called the 'ego node', and all other nodes are called 'alters' or 'alter nodes'. In an ""ego network"", the node of interest will typically be in the center, and alter nodes will surround it. However, one of my personal techniques that I find useful is to DROP the center node, the ego node. By doing this, the ego network can split apart, revealing the distinct groups that exist in an ego network. For instance, if a person is both an athlete and a video gamer, their social circle might contain both groups, and they may not interconnect. That's one example.Opportunity for intuition: what other combinations could cause a person's ego network to have different groups? Think hobbies, religions, dating life, education, etc. Humans are complicated.Let's look some ego networks!This is the ego network for Wilco. When I said that today I had better luck with ego networks than I had with community detection, this is why. With the ego network of Wilco, I get nodes that are linked to or linking to the Wilco page, so they are all relevant in some way. And this is a very interesting ego network, because there is so much cross linking between alters. That is not always the case in networks. We are lucky! I could easily find several things that catch my interest, but let's keep going.This is the ego network for Jeff Tweedy. As he is the lead singer for Wilco, it is going to be very similar to Wilco's ego network. In fact, Wilco and Jeff Tweedy's nodes show different node colors indicating high Page Rank values. Jeff Tweedy is in Wilco's ego network, and Wilco is in Jeff Tweedy's ego network. Let's continue.This is the ego network of the band Tweedy, Jeff Tweedy and his son's band. I had never heard of them before this analysis, and I started listening to them immediately. So far, I like them! I will definitely explore this, more. I enjoy playing music with my kids, so I find this network relatable and sweet. I need to know more. Also, what is that band The Bronx like? New music! Let's continue.This is the ego network of the Wilco discography--Wilco's albums! I accidentally stopped keeping up with them around 2012, so which ones have I not yet listened to?Cate Le Bon is producing a new Wilco album, out in two weeks!!!!!Star Wars: this is an album I have not listened to!Schmilco: this is an album I have not listened to!Ode to Joy: this is an album I have not listened to!That's Enough!And that's enough for one day! It is important to pace yourself when you do this kind of analysis. My goal was to identify three to five new things I did not know about Wilco and I found many more than that. What are the takeaways?Networks and graphs are amazing exploratory devices for knowledge DISCOVERYNetworks and graphs are much more useful than in just Whole Network Analysis metrics (triangles, density, degrees, etc)You can use OSINT to learn about GOOD things that interest you! OSINT isn't just for investigating bad or scary things! OSINT is literally knowledge discovery.Everyone can use network data to learn about things. It is a choice to NOT use them. It is self-limiting to NOT learn to explore and analyze networks.This was a fun post! Next post, I'll have some cool music to listen to while I am doing the work! I hope you enjoyed this!Book Giveaway!And for those who took the time to read this post, I am now going to announce a book giveaway! I am going to give away five copies of my book in digital and physical format. Winners who live in the United States can receive either a signed physical copy (or unsigned) if they don't mind waiting a few days, or they can immediately receive a digital copy. I already have the digital copies ready for winners.Winners who live outside the United States can receive a digital copy.Hew is how to participate:Find a topic of interest. For today's post, the topic of interest was ""wilco"".Use the crawler from day 5 to create a network edgelist. Limit iteration to 3-4. Beyond that, if you are a beginner, it will be overwhelming. But I do encourage people to be bold in their learning.Do network analysis like I did today. You can use today's Jupyter notebook as a template.Create an ""Insights Document"" that has your findings. Use my blog posts for inspiration. You want to show some Whole Network Analysis findings, identify important nodes using Page Rank or Centralities, and look at a few ego networks. If community detection is difficult, skip it for now!Message me the insights document on LinkedIn. I need to see your work and the insights you discovered. A Google document is easiest!You don't have to be too thorough. Just do what you can. If you know nothing, then building a graph and finding important nodes is enough! If you know your way around networks, then push a bit further and show me what you can do!I will accept submissions until September 30, 2023!I will take all acceptable submissions and add them to a raffle. I will call out the winners on October 1!And if you make your own LinkedIn post and tag #100daysofnetworks, I will add your name TWICE!If a submission is not complete enough, I will ask for a bit more. I will try not to do that. Do the work, and you'll have a shot to win. Do the work.Thank You!Thank you for reading this post! I hope you enjoyed it! I wanted to do something a bit more interesting than usual, as it is important to do things that keep us creatively engaged. I look forward to seeing what you can do! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Artificial Intelligence and Machine Learning', 'Community Detection', 'Egocentric Network Analysis', 'Graph Algorithms', 'Graph Visualization', 'Subgraphs and Connected Components', 'Whole Network Analysis']"
/work/blog_data/137334639.day-9-of-100daysofnetworks.html,"If you would like to learn more about networks and network analysis, please buy a copy of my book!Hello everyone. Welcome back to another post in this adventure. On day 8, I used our Wikipedia crawler to create a network edge list related to the band Wilco, my favorite music group.Data Scientists often complain that most of their work is in cleaning data, or data wrangling. Network Science is no different. If you do not clean your networks, then your network metrics will be based on a mixed network (on-topic and off-topic). Today, we’re going to take our Wilco network, identify the nodes that are not related to the band Wilco, and then we’re going to remove them. At the end, we will have a pure network, and this will be useful for downstream tasks.Today’s code is available on github.Spot ChecksThe first thing I always do is a spot check. If I’m working with a small enough network, I’ll visualize it. If it’s a network of thousands of nodes, I’ll look at centralities and Page Rank. Today’s network is tiny, so I’ll visualize it.This quick visualization actually already shows what we need to do. I know the band Wilco, so I have some domain knowledge on this topic. I can see that there are three sections in this network.I can see that there is some junk on the left, mostly related to Space Quest, but not entirely. And I can see that there is some junk at the bottom related to the node “Procedure word”. Think about some nodes as bridges. Which nodes do you see that lump the pure section with the junk sections? Try to guess. “Procedure word” is one of them. Look for more.If you identify these junk bridge nodes and remove them, the network begins to untangle itself. Before we do that, let’s look at the overall network’s Edge Betweenness Centralities.Edge Betweenness CentralityEdge Betweenness Centrality has to do with which edges exist on the greatest number of shortest paths. Any two nodes in a network will have a shortest path, from A to B. If an edge sits between most nodes, then that edge is a bridge between many nodes.Here are the top ten edges with the highest Edge Betweenness Centrality scores:Notice that the top one is junk. It’s linking Wilco to Space Quest. Our Wikipedia crawler made a link between the two based on the name. Roger Wilco has to do with Space Quest. The top edge should definitely be removed.The second link is between Procedure word and Wilco. This also looks like junk that should be removed. Further down, there is also a link between Allied Communication Procedures and Procedure word that should be removed.But before we do anything with these edges, let’s first manually identify junk nodes, remove them, and then see how the network looks. We’ll revisit Edge Betweenness Centrality again a bit further down.Manually Removing NodesAfter looking at the network with my eyeballs, I noticed a few junk nodes. Here is how I set them aside, and how I remove them from the network. The first link specifies the nodes to remove, and the second line removes them. How does the network look now, after removing just a few nodes? First, here is how it originally looked:And here is how it looks now:OUTSTANDING. Already, one of the junk clusters has shot off to the left, and the Space Quest stuff is held together with the Wilco network by a single edge. NOW is the right time to look at Edge Betweenness Centralities again. Dropping just a few key junk nodes was enough to do most of the cleanup, and snipping one edge will be enough to finalize the separation. Here’s the new top ten Edge Betweenness Centralities:Notice that top edge. Notice how it looks visually on the network. It has a much higher centrality score because it sits between every Wilco node and every junk node. We can easily remove this one edge.The first line is just getting the top edge, by name. The second line is removing the edge from the graph. How does the network look, now?Beautiful. The three parts of the networks have split apart. The Wilco network is in the center. We want to keep that, and disregard the rest. Let’s look at each connected component, keep the Wilco one, disregard the rest, and persist the data so that we have a clean edge list for later days.Connected ComponentsIn my book, I describe networks as having connected components that resemble islands and continents. Networks will often have one super cluster, then a bunch of large but smaller clusters, and many isolate nodes. Let’s take a look at the connected components that now exist in this graph.This shows us that there are three connected components in our graph. The first component has 41 nodes, the second component has ten nodes, and the third component has four nodes. Let’s look at each of them. This is the first connected component. I can see Jeff Tweedy and several album names. This is definitely the Wilco network. This is the good stuff. Here’s the next component:This component is clearly related to Space Quest, not Wilco. Finally, here is the third component:Keep the Wilco GraphCheck the code and you’ll see that the next step is to overwrite G with the Wilco connected component, and then I persist the network edge list as a file for tomorrow.Now we have a clean Wilco network! With this, I can crawl each of the Wilco-related Wikipedia pages and get clean content. We will use that content for Natural Language Processing, and for creating Entity graphs! More soon!That’s all for today! Thanks for reading! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Graph Algorithms', 'Graph Visualization', 'Subgraphs and Connected Components', 'Whole Network Analysis']"
/work/blog_data/137363842.day-10-of-100daysofnetworks.html,"If you would like to learn more about networks and network analysis, please buy a copy of my book!Hi everyone. I’m very excited about today’s post, for two reasons:I’ve created a Wikipedia content crawler, and you can use it.I’m going to show you how to convert raw text into entity graphs.Wikipedia Content CrawlerThe github for the Wikipedia Content Crawler has been linked to above. It currently takes a .csv edgelist as input, converts the edgelist to a graph, and then crawls content for each node in the graph.However, you can tweak the code to pass in a list of pages, if you wanted to. The more pages you pass in, the longer the crawl will take. For today’s post, I passed in a very simple edgelist for a graph with 41 nodes and 167 edges. The edges don’t matter. The nodes get crawled. As the nodes are Wikipedia pages, this means that the crawler crawls wikipedia, builds a dataset, and then saves it to file.This is a useful tool for knowledge discovery on Wikipedia. It is also a useful tool for learning Natural Language Processing and Data Science, as the crawler returns text content, all web links, and more.The above image is a preview of the crawled Wilco data. There’s a few things to notice:Page is the page title.Categories is a list. Categories could be useful in building other types of networks.Content is all of the text on a Wikipedia page.Images are images found on the Wikipedia page.Links is a list, pointing to other Wikipedia pages. We use this in our networks.Web Links is a list of website links. We can use this to make other networks.So, very quickly, in #100daysofnetworks, we have started with a theme (Wilco), created a network of Wikipedia pages that are about Wilco, removed junk nodes, and then crawled text from Wikipedia for the relevant pages. We have essentially created a clean dataset of richer content for analysis. That’s powerful.Converting Text to Entity GraphsJust FYI, all datasets created during in #100daysofnetworks are accessible here. You can dive into analyzing any of these. Using the freshly crawled Wilco content, I now have rich and useful data for Natural Language Processing. Now, I can show you the magic of Natural Language Processing and Network Science combined. I can take ANY TEXT and convert it into an Entity Network. We can explore these Entity Networks using Social Network Analysis and Network Science.I literally mean it when I say I can do this with any text. After my book was published, I found a 600 page PDF file on the internet. After converting the PDF to text, the conversion went perfectly, and it only took a few minutes.Here is what I did (see code here):Load our content datasetLoad spaCy modelCombine all ‘content’ text into one ‘text’ variable.Using spaCy, do Named Entity Recognition (NER) and extract entitiesUse the extracted entities to create an Entity NetworkThis approach is written about in my book, so you should buy a copy to learn more. In fact, today’s code was taken from my book’s Github, to save time.What is Named Entity Recognition (NER)?Other than approaches for similarity, Named Entity Recognition (NER) is probably my favorite NLP technique. NER uses Machine Learning to identity people, places, organizations, and much more from text. That’s it. You pass in text, and you get entities back. With these entities, you can tell who a piece of content is about, automatically. This is powerful for data mining and information extraction. Please read my book, ask ChatGPT, or spend some time researching NER. It is too useful to ignore, if you work in Natural Language Processing or Data Science.Let’s See the ResultsToday’s post is a show-and-tell. I wanted to announce the crawler that I built, as that will be useful to others, and I want to give a preview of what you can do with the data. You can do a lot more. This is the TOP of the rabbit hole. We haven’t even jumped in. These are just options.First, I load the data:Next, I combine the content into a text field and extract all entities:Then I use those entities to create an Entity Graph:And finally, we have a graph that we can explore. Notice that we went from 41 nodes to 914 nodes? We went from a small graph to a larger and denser graph, all related to Wilco. Our larger graph should serve us wonderfully for knowledge discovery.Let’s Explore a Bit!It’d be really boring to end today’s post without at least exploring a bit. What can I see, in this new network?I can see that it’s pretty complex, as expected. I can see that there are no isolates, due to the nature of how the graph is constructed. I can see that the network gets denser and denser as we look closer to the core.What does the core itself look like? I’ve mentioned that I usually take a look at the core to get an understanding of which nodes are most influential to a network.The core is beautiful. Which nodes are most important in the network?STOP FOR A SECOND. Truly appreciate what has happened. For this bar chart to exist, we have fused NLP with Network Science. NLP took care of the entity extraction for the graph construction, and Network Science gives us the ability to explore node importance. This is the marriage of NLP and Network Science. This looks great. There are some names I don’t recognize, and that’s great.Here are the node Betweenness Centralities. This is the BeginningThe content crawler unlocked new analysis for us, allowing us to use NLP techniques and fuse them with Network Science. Use the crawler, get interesting data, explore it and learn to analyze it.Thank you for reading today’s post! We made it to day 10!If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Artificial Intelligence and Machine Learning', 'Graph Algorithms', 'Graph Visualization', 'Subgraphs and Connected Components', 'Whole Network Analysis']"
/work/blog_data/137554246.day-11-of-100daysofnetworks.html,"Today, I want to share how I fell into Network Science, as it was not expected, but it was one of the most important accidental opportunities of my life. I fell into this.I’ll start with a timeline, so that you have this in mind, and so that this doesn’t become a post of me rambling.1999-Now: Tons and tons of Entity-Relationship Diagrams (Database Diagrams)2016: Led my team’s GDPR initiative; software inputs and outputs. Coworker inspired me to look into Data Science and use Coursera; Mentioned another better way of “mapping out” things.2016-2017: Took Coursera 5 Course Data Science specialization. Took my time, bought books, learned it well. Final course was Social Network Analysis.2017-Now: Got hooked on the idea of networks and how pervasive they are. Read “Linked” and had my mind blown. Started buying and reading every Network book I could find and afford. Became an obsession. Couldn’t stop thinking about it or talking about it.2018: Read Dmitry Zinoviev’s book Complex Analysis with Python. Connected the remaining dots in my mind. Thank you very, very much.2018-2020: Created lots of “data flow” networks. Mapped out data flow across data centers. Huge improvement to data operations, especially useful in data migrations and uplifts. Data operations people, this is very useful.2020: Used for malware analysis. Malware evolves, too, and can be analyzed using these tools! 2020: Did the original #100daysofnetworks. Went in with low confidence and very simple skills. Could only do whole network analysis in the beginning, and gradually went deep.2021: Wrote a book, and started a company. Did so much network analysis for both, and continue to use Network Science daily in my work. It is more useful to me than other aspects of Data Science such as Machine Learning.2021-Now: Continue to learn, experiment, build, and improve. This is never-ending. There is always more to learn. Network Science has a rich ecosystem, like Data Science does. Looking CloserLet’s dig into a few of these, but still keep this post short enough.First, I have been working with databases since I was sixteen years old, and learned about Entity-Relationship diagrams in my early twenties, about 23 years ago. Really fell in love with ER diagrams and database design. Found ER diagrams to be fascinating. Guess what, they are networks!An Entity Relationship diagram is a visualization that shows how database tables relate to other database tables. If one table has a foreign key for another table, they are related. ER diagrams are network visualizations, even if people don’t realize it.Knowing about ER diagrams, I began to think about code as having inputs and outputs. Data goes in, code does something, data goes out. In that way, code follows a chain, and if you map out an entire server or suite of related software, it forms a complex network.Realizing that malware is code, I wondered if it also follows a network. IT DOES. There are several ways to use network analysis to analyze malware, in my experience. No matter where I looked, I kept finding more and more networks! They’re everywhere. I could see them in my data, but I didn’t know how to effectively use the network data, or analyze it!Help People! Inspire Others!If it were not for my coworker giving a talk at a data club on his path into Data Science, I probably wouldn’t have gotten interested in Data Science. It felt unapproachable before his talk. I am forever grateful, and remind him often that I remember this.If it were not for the University of Michigan and Coursera, I wouldn’t have found Data Science useful in my work. This is the specialization that helped me. I spent three times longer than expected, bought many books for supplemental learning, and really dug in. This was incredibly helpful.If it were not for the book Linked, I wouldn’t have become obsessed with Networks.If it were not for Dmitry Zinoviev writing a book, I wouldn’t have learned to apply Network Science and Social Network Analysis to my work. He had a huge impact in my career. You should check out his book!If it were not for #100daysofnetworks, I wouldn’t have realized how deep this rabbit hole actually goes, how useful these techniques are, and how this would have changed the way that I even think about things.And there are many other books that helped and inspired me, along the way. I will mention them, throughout #100daysofnetworks.The moral to the story is this: do your thing, learn what interests you, and tell people what you learn. You never know how you will inspire others, and you may ignite the spark of an idea that has a huge positive impact in someone’s life. Thank You!Thank you, everyone who has followed along with my adventures into Data Science and Network Science. I love hearing from my readers, how these techniques are proving useful to them and showing them things they didn’t expect to see. Thank you for making this fun for me, so that I am energized to learn and share.And thank you to everyone who has participated in the first book giveaway! I’ve received some nice submissions and will be announcing the winners tomorrow! There will be more giveaways, so stay tuned!If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Artificial Intelligence and Machine Learning', 'Graph Visualization', 'Whole Network Analysis']"
/work/blog_data/137575064.day-12-of-100daysofnetworks.html,"On Day 8 of #100daysofnetworks, I announced a book giveaway. In order to participate, someone just needed to come up with a topic of interest, use the wikipedia crawler to create a knowledge graph, and then do whatever network analysis they felt comfortable doing, based on their current level of experience.When I decided to write my book “Network Science with Python”, my goal was to inspire even just a few people to dive into the topic, knowing that it would not receive anywhere near as much attention as Machine Learning. That’s really unfortunate, because I build a lot with ML, and I find this even more useful. Even knowing that this would be a quieter topic, I needed to write this book, to show how these techniques can be useful.So, I am very excited to announce today’s winners of the current book giveaway, because they did something very special. They took a leap into the unknown, to learn more, to expand their understanding, to expand the boundaries of the possible. I respect that!For the competition, I stated that I would give away five copies of my book. If there were more than five submissions, I’d do a raffle. There were three submissions, so the raffle is not needed! Every single entry is a winner. Let’ get to it and congratulate the winners!Winner #1 - Josh Leigh - Causal Knowledge GraphJosh Leigh used the Wikipedia Crawler to create a knowledge graph related to Causal Inference and Discovery. With Aleksander Molak’s new book on Causal Inference and Discovery having been recently released, there is qiute a lot of excitement on the topic. There is also overlap with Graphs and Causal Inference. We will be exploring Causal Discovery throughout #100daysofnetworks, for sure.Josh Leigh even made a cool LinkedIn post, showing some of his work! Check it out!There’s a couple quotes I especially liked:“A quick look into Endogeneity and my mind is blown - we see this in mineral system geoscience and it's rarely addressed. It's one reason why I’m interested in learning more about causality.”I like this, because he made a discovery and a connection that related to his actual work. Networks are great learning tools, and he will be able to use this to dig deeper.“Straight away I see reference to System Dynamics which is linking through to Causal loop diagrams! I have an understanding of these so this is super helpful to build from those concepts. I can also see a whole bunch of things that sound super interesting but I have no clue about! What the heck is Confirmatory factor analysis!? 🤯”That’s the good stuff. Every time I look at a network, I’ll come out with some new insights. One network visualization will be jam packed with insights, and it’s impossible to find them all in one run. Now he can explore confirmatory factor analysis, and then that may lead him to new places. I loved also loved Josh’s visualization of the core of the network.Excellent work, Josh Leigh! Congratulations! Winner #2 - Iftikhar Ud Din - Game of Thrones Social Network AnalysisIftikhar Ud Din made a great submission to the book giveaway in the form of Social Network Analysis of the Game of Thrones Social Network. I have been curious about this network for a while, but never found time to find the dataset. Iftikhar pointed me to it, so I will add it to the “Datasets” page on substack. We’ll use that dataset in #100daysofnetworks, as it looks really, really fun. Learning should be really, really fun.For now, the dataset can be found here!Iftikhar did a really cool analysis, using things in networkx I had never seen before. He has inspired me to learn more!I really like that he showed the degree distribution. I haven’t been doing that, but have wanted to bring it up. I will do a post on that at some point.The reason that’s important is because networks tend to follow a Power Law, where few nodes will have many edges and many nodes will have few edges. Take a look. Many nodes have less than ten edges (degrees, connections). It looks like only one node has more than sixty edges. This is common in real-world networks, and it is interesting to see in fictitious social networks, as well. Perhaps it has to do with an author’s ability to manage characters. You can learn more about this phenomenon here.Iftikhar did an excellent job and went above and beyond, even doing Egocentric Network Analysis and some community detection. I look forward to seeing where these capabilities take him! Great job! Congratulations!Winner #3 - Yuval Feinstein - Georgia Knowledge GraphYuval Feinstein made a Knowledge Graph related to the country of Georgia. He built this Knowledge Graph using the Wikipedia Crawler and used the country name of Georgia as well as several cities. He used Page Rank to identify important nodes (Wikipedia Pages), and these four were found to be most important:GeorgiaPolitics of GeorgiaEconomy of GeorgiaReligions of GeorgiaThat makes a lot of sense, intuitively. Religion, politics, and the economy have importance to culture, identity, and prosperity.He found one issue: due to the ambiguity of the name Georgia, Wikipedia pages relating to the state Georgia were included in the network. This is definitely something to keep in mind, and it’s similar to a google search. Sometimes, you need to include additional words to filter things OUT, or if this graph were to be used for something, then preprocessing could be done to remove the Georgia (State) related nodes.Everyone, Great Job, and Thank You!I had five books ready for the giveaway, and I’ll give away three! I’ll keep the rest around for future giveaways. There will be more! We’re only on day eleven! Thank you, Josh Leigh, Iftikhar Ud Din, and Yuval Feinstein for participating! You are the winners! Congratulations. I will be in touch with how to get access to your free copy of my book!If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Artificial Intelligence and Machine Learning', 'Community Detection', 'Egocentric Network Analysis', 'Graph Algorithms', 'Graph Visualization', 'Whole Network Analysis']"
/work/blog_data/137763516.day-13-of-100daysofnetworks.html,"I’ve been wanting to write this blog post for a while now. I’ve been brainstorming what I wanted to do for weeks, but this is a bit more complicated than whole network analysis, or egocentric network analysis. Yesterday, I figured out how I want to do today’s implementation.It is important to understand that this is just one implementation. Python dictionaries are very useful for holding temporal networks an context about them, and additional context can be added on in steps. I’m going to show how to do that today.What is Temporal Network Analysis?This is not covered in the first edition of my book, other than maybe in passing. I will include a chapter in the second edition of my book to discuss Temporal Network Analysis, possibly using parts of today’s post. Networks evolve over time. Think about your social media accounts. Every day, your followers increase, but your followers can also decrease. Every day you also follow more people, but you probably unfollow people, too. Also, these days, what I call “artificial influence” has run rampant. Bots are out of control and social media has never really taken steps to mitigate, just hand wave. Artificial influence is also done on the internet itself, not just social media.With temporal network analysis, we can analyze how networks change over time. I think of this as network evolution. Today, I’m going to show you ONE implementation of how this can be done, but there are others, and I’ll be thinking about this topic more and more during this iteration of #100daysofnetworks.Please follow along with the code. This is a bit complicated.Build the Origin GraphIf you look at the code, you can see that this time I am using a dataset called ‘wilco_content_edgelist.csv’. This network was build using Named Entity Recognition (NER) across several pages related to the band Wilco. This network was built using TEXT as input, not direct links. It is a cool approach for building networks, mentioned in my book, and written about by others as well.You can learn how I built it by reading day 10, or by looking at the code from day 10.I’m using the whole network as the origin graph, the starting point.Dropping Connected ComponentsFor this experiment, I am not dropping single nodes or random edges. I am dropping whole connected components, to simulate what would happen if a whole community walked away at once, or if a whole community joined at once.Why would that ever happen?Brands are sometimes boycotted. This year, a brand of beer was boycotted, and many people stopped drinking that brand of beer. If we had their social media data and could construct a graph, we could see entire groups walking away. We could potentially identify who was influential in the boycott.Celebrities are also boycotted or ‘cancelled’ as they say. People gradually lose interest in politicians, too.So, my idea for today is bigger than just dropping random nodes and edges. I want to see if I can identify whole communities leaving a network, or being added to a network.The first thing I need to do for this simulation is identify the largest connected components, because they will be useful in removing and adding nodes in batches.The code for that is pretty simple. Connected component index 0 contains 755 nodes, index 14 contains 14, index 55 contains 7, and index 57 contains 6. Perfect. We’ll do three rounds or removing whole components. The goal is to be able to easily detect the changes, and with more context than the node level.The largest component (index 0) is where most of the node connectivity is taking place.And if I look at another component, it’ll be much smaller and more readable.This is going to be one component that we remove and try to detect its removal.Creating “Time Slices” of a NetworkMy Eureka moment for temporal network analysis happened when I figured out how to chop networks up into what I call time slices. I watched a documentary once that described space and time like a loaf of bread and that stuck with me over the years, and it makes sense programmatically, helping me wrap my mind around this to do this. I hope it helps you, too.The first thing I need to do is create some artificial time slices, removing entire connected components while I do.Usually, a NetworkX graph is called G, but I am making a move away from that and using a Python dictionary for G, so that I can have time slices in G. Note that as I increment G, I am also dropping an entire connected component. You can see that in ‘drop_nodes’. I remove the nodes on the bottom lines. As I am using connected components, these nodes are not attached to any other part of the network.Proof of ConceptNext, in the code, I build a simple proof of concept, comparing G[1] to G[0], to see if I can find the differences.Using that, I can see which edges were dropped or added, which nodes were dropped or added, I can create a graph of what was dropped so that it can be visually inspected, I can create a graph of what was added for the same reason, and I can add on as many metrics as I want to understand what impact these changes have made on the overall network.  Notice that I am looking for changes in density, total degrees, and total edges. Please understand that you can add as many useful metrics as you want or need. Get creative.For reference, this is an image of what was dropped. It is an image of what is no longer in the network, and it has retained the relationships. Imaging a case where a whole community of people left a brand, that central node might be of interest. In this network, these nodes are not people, but imagine people or accounts. The point is that we can see what is no longer in the network. We can see what is gone.To me, that is very, very, very cool and probably important. Think about use-cases.Let’s Automate ThisI want this to be something reusable that I can use in #100daysofnetworks and any network analysis that I do, so I created a useful function. Please see the code. Running it is simple:I pass in the temporal graph, and the function adds enrichment. For the first step in the temporal graph, the output looks like this:It contains the time slice of the graph itself in ‘graph’, and has recorded the original density, degrees, and edges as values for comparison. What’s the next one look like?I can see that since G[0], there have been some changes. Under ‘dropped_edges’, I can see specifically what edges have been removed. Under ‘dropped_nodes’, I can see specifically what nodes have been removed. I can also see that no edges or nodes have been added. I can see that overall density of the graph actually increased by a little, even though there has been a drop in degrees and edges. Many other metrics could have been added and compared, easily.And if I visualize ‘graph_dropped’, I can actually see what was removed from the network.This is better than just a list of nodes. We can see the shape those nodes used to have in the network. We can see that it was a community that dropped. By looking at other slices, I can see the next round of changes.Finally, since G is an enriched dictionary, we can investigate the bottom three fields over time.I can see that there was a bigger initial change to density, and that subsequent changes were smaller. Removing connected components actually increased overall network density. Why is that?I can also see that there was a bigger initial change to degrees, which shows in our work. The values are negative, but it would appear that the drop is slowing. However, this is just an example, and it doesn’t really say anything. This is a HOW post.What Does it Mean?What do you think this can be useful in? I mentioned influence, but there are other uses. How would you use this? What metrics would you include to better understand network changes over time? At this point, this is where Network Analysis meets Time Series Analysis. Enjoy!That’s All, Folks!That’s all for today! Thanks for reading! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Artificial Intelligence and Machine Learning', 'Attack Simulation', 'Egocentric Network Analysis', 'Graph Algorithms', 'Graph Visualization', 'Subgraphs and Connected Components', 'Temporal Network Analysis', 'Whole Network Analysis']"
/work/blog_data/137960086.day-14-of-100daysofnetworks.html,"Today, I’m writing a different kind of post for #100daysofnetworks. I want to recognize someone who has been inspiring me in my learning for several years, now: my daughter.If you have read my book, you might have noticed this in the ‘Acknowledgements’ section.First, I would like to thank my family for constantly supporting me throughout my life and career. With regard to this book, I would especially like to thank my daughter, who helped with the editing process.That sentence doesn’t do her justice, so I want to talk about how she has helped me with #100daysofnetworks, with writing my book, and even with a Data Engineering presentation I gave in 2016. Today’s blog post is dedicated to her, and to all the people who help inspire us to learn, and to all support us in our learning and other adventures.I’m not going to name her, as she is still a minor. The fact that she has already contributed to data science and network science at such a young age is a huge accomplishment, I think.About the BookWhen I was writing my book, I was also building a company. It was my first time doing both, so I was absolutely overwhelmed. I gave up nearly every weekend for over a year to write my book. I was constantly tired and burnt out, but I pushed forward a little bit every week. Baby steps get the job done. Life is a marathon, not a sprint.As I was so overloaded, I asked my daughter if she’d like to help edit chapters before sending them to the publisher’s editors. The goal was to catch and remove as many of the annoying little things as possible before sending to the editors, so that the editors could worry more about catching factual mistakes and bigger problems.She said yes, and it became a bit of a side job for her. She edited every single chapter in my book, and I paid her to do it. This had some nice side effects:She got some spending money, which she could use to pursue her own interests. I got to show her some cool data science and software engineering stuff, which she now knows better than probably many people reading this blog.It was win win win. A better book was written, she got spending money, and I got to teach her some things that can help her in the future.Let me be clear: I might never have finished my book if it were not for my daughter’s help. She saved a lot of time, and I was using the very little spare time I had to write the book. She has absolutely made a solid contribution to data science and network science. I want to recognize her for that.About #100daysofnetworksHowever, that book deal didn’t come out of nowhere. I got the book deal because I wrote so much about networks and network science throughout the original #100daysofnetworks series that I did on LinkedIn. That adventure got me a book deal and a company.And she helped with #100daysofnetworks, so she gets credit there, as well.When I was doing #100daysofnetworks, one of the most entertaining ideas was to do a network analysis of the Twitter K-pop ecosystem. At the time, and still now, my daughters really enjoy K-pop. I know a few artists by name because of my girls, and I actually enjoy some of the music. It’s not my favorite genre, but it’s really quite impressive.Fine, I’ll say it. I like BLACKPINK. And Rosé is very, very talented.Just for fun, here’s two songs I like:I have to admit it. K-pop is just magical. It’s a cool genre, and very high quality pop. My girls both love it, and I enjoy it.So, during #100daysofnetworks, we had the idea to analyze K-pop networks, and that was a really fun thing to do. We did this in 2020, and it was fun to talk about on LinkedIn. I also mention it in my book.COOL STUFF!To supplement this post, I did a few things:I created a K-pop network edgelist of Wikipedia pages for you to use in analysis.I created a K-pop analysis notebook, which you can use to start an analysis.I’ll probably use this during #100daysofnetworks, as it’s a fun network to explore. For instance, as a teaser, here’s the Ego Network for BLACKPINK with a radius of 2.There is a lot more to explore. Thank you, to my daughter, for inspiring this idea. We will use this, and expand on what was done in 2020. We can use this to find new information, music, and media.About Before All ThatMy daughter has been writing code since she was six years old, just like me. I have the mindset that if you can learn to read and write, you can learn to write commands that get computers to do what you want them to do. That is all that programming is, and there’s no reason kids can’t play with computers. It is enjoyable if they enjoy it.By the time she was ten years ago, I could give her pseudocode written on paper, and she could convert it to working code.So, at age ten, I was giving a big data talk and needed to show how VoltDB and Kafka could be used together. I jokingly asked her if she’d write the Kafka producer, and she agreed to do it. I plugged in the Kafka parts, but she wrote the overall logic. Because she did this, she was invited to present with me, in 2016, at age ten.All that to say, everything I learn, she hears about and sometimes learns. She’s familiar with everything I do. She knows the three different kinds of machine learning. She understands everything I’ve written about in my book.This is a Daughter Appreciation PostSo, today, I wanted to thank my daughter. She helped make this book, this adventure, and even this blog series possible. She has already agreed to help edit the second edition of my book, and we’ll keep that going forever, if she’d like. She is a very good writer and has a capable mind. She has already made quite a solid and impressive contribution to Data Science and Network Science, even before graduating High School. I am so excited to see what she makes with her life. Right now, she is very interested in the performance arts (singing) than coding. She practices for hours every day, more than I practice Network Science. If coding helps her later in life, great. If not, and she wants to sing, also great! Follow your dreams!But at least she has these skills, if she ever needs them. For instance, as I have been showing, these techniques are useful for knowledge exploration and discovery. That’s useful to everyone, not just programmers and scientists.Thank you so much for always inspiring me and helping me to learn and show what I have learned!Some Royalties Go to Her!As she has helped with all of this, I do give her a percentage of every one of my royalty checks. So, if you’d like to support her, please buy OUR book!","['Artificial Intelligence and Machine Learning', 'Egocentric Network Analysis', 'Graph Visualization']"
/work/blog_data/138057049.day-15-of-100daysofnetworks.html,"I think most of us have heard the phrase CONTEXT IS KING. There is a good reason for that. Any piece of information can be distorted if taken out of context, or if context is intentionally twisted. We heard it often, that we should not take things out of context. However, techniques for identifying context are rarely discussed outside of journalism.For this post, I decided to ask ChatGPT what it felt the saying meant.""Context is king"" means that the meaning or importance of something depends on the surrounding information or circumstances. Just like a king rules a kingdom and makes decisions based on the situation at hand, context guides our understanding of words, actions, or situations. Without the right context, things can be easily misinterpreted or misunderstood. So, in everyday life, it reminds us to consider the bigger picture and all the relevant details before making judgments or decisions.Let’s unpack that:The meaning or importance of something depends on surrounding information or circumstances.DANGER: Without the right context, things can be misinterpreted or misunderstood.This saying reminds us to consider the bigger picture and all relevant details before making judgements or decisions.In data science and data engineering, identifying and adding context to datasets is known as Data Enrichment. If you are building machine learning models, you may hear of it called Feature Engineering. Feature Engineering is Data Enrichment. In this blog post, I show data enrichment and feature engineering in action.Why is Data Enrichment Important?Data enrichment is important for identifying and extracting useful context. In order to identify additional context, domain knowledge is important. For instance, if I want to use graphs to further enrich my data, I need to understand a bit about graphs. Likewise, if I want to use language data to further enrich my data, I should also know a bit about NLP.But what I have seen often is that language and network data is disregarded. There may be a few reasons for this:Uncertainty on how network or language data can be usefulLack of knowledge in how to use network or language dataLack of experience in using network or language dataSo, in Machine Learning, a shortcut can be to disregard all data that is not boolean or numeric. This is lazy and limiting.For today’s blog post, I’m going to use the Titanic dataset to show that there is usefulness beyond the numeric data. Personally, I enjoy being creative with this dataset, because this dataset contains numeric data, language data, and network data. However, blog posts and articles I have read on using this dataset tend to prioritize filling in missing values for age. But the dataset is about the Titanic, and predicting who lives and who dies. Maybe, instead of age, there are other features that are more predictive. Maybe there is more to explore and learn beyond the few numeric and categorical fields!The point is this: you might not be using the full potential of your datasets, and learning how to use language and network data may make you more effective. It has certainly helped me.Graph and Language DataTo me, graph and language data are very important data, when present. Language is rich with information, if you know how to use it, and networks/graphs reveal relationships, communities, and node importance. Keep an eye out for language and relationships in your dataset.For instance, in the Titanic dataset, people have names, and names have titles that can convey age. Also in the dataset, people are assigned rooms, and multiple people can share the same room. We can use this to build a rough social network of the Titanic.Let’s Dig InThe code for today is available here. You should use it to follow along. The Titanic dataset has been downloaded from Kaggle and placed into my repo. You can use it for your own experimentation.Here is what the data looks like:You can read the data dictionary here, but here are a few initial observations:passenger_id is most likely useless unless the order that passengers were added had something to do with their class or sex or age.survived is the label we will use for Machine Learning. 0 means they died, and 1 means they survived.name is the passenger’s name, and it is language data, which means that NLP may be useful.sex is the passenger’s sex, and it is categorical, so it should either be label encoded or one-hot encoded.cabin is the room number, and if cabin and name are used together, we can build a bipartite network and then project it to build a social network.That’s the original dataset.So, what does enrichment look like?When I think of data enrichment, I usually think of the journalistic questions:Who did what?What did they do?When did they do it?Where did they do it?Why did they do it?How did they do it?If you start with journalistic questions, it gives you a baseline for interrogating your datasets and identifying useful context, or holes where we are missing useful context.In this dataset, we have the WHO (names) and the WHEN and the WHAT (lived or died), but there are important questions we can ask:Did age or sex have anything to do with survival? Were women and children saved first? Was this true for all classes or only first class?Did travelers who traveled alone have less of a chance for survival compared to those who traveled with their families, or was it such a chaotic disaster that family members couldn’t help each other very well?When families bought tickets for multiple cabins, did the fact that the family was spread out have any effect on survival?Was saving small children prioritized over older children?So, the first thing I did is enrich the data by adding some new fields. Please see the code for how.I think that for a small child, common sense would have it that survival probably has something to do with a parent being present, so I added a few fields to capture that, as well as a few fields to discern whether a person is a child (<=12), small child (<=6), or tiny child (<=2).Now, to get to the cool stuff, using a Bipartite Graph, I can make very short work of converting our dataset into a rough social network of those who were aboard the Titanic, BUT THIS IS VERY ROUGH. It can be improved with work. This is the quick version. You can read about Bipartite Graphs here.In a bipartite graph, you have two different types of nodes, where in a social network, you only have one type. As we only have one field for people’s names, we can’t simply turn names into a network. However, with a bipartite graph, we can project this into a graph of who knows who, based on some similarity. In this case, people shared rooms. This is simple but has one extra step than usual:First, I import bipartite, then I create a Bipartite Graph using name and cabin. Finally, I project the graph. I am not positive I did the last part correctly, but it seems to work. I’ll dig into that.As a result, I can see the actual groups of people aboard the Titanic.Look how the last names are grouped together, for validation. This turned out better than expected. Note that I am using k_core to remove the isolate nodes. I am only showing the nodes that have one or more edge.Here is one family:What is their story? We can use this graph data for enrichment!We have a graph, now, and we can use any of the techniques that I have written about in my book or over the last fifteen days on this graph. We can explore the different groups that were present on the Titanic.To me, that’s much, much, much cooler than just feeding X to Model and getting a 1 or 0. This is life data. These were people! I don’t want to just build models to make predictions. I want to understand what contributed to survival or death. I WANT TO UNDERSTAND, NOT JUST PREDICT.However, Machine Learning models can tell you WHY they predicted one way or another. This has to do with ML Explainability and Interpretability. Serg Masis wrote a book on this very topic, and this is a must-have, in my opinion.So, the next thing I do is convert the Graph into node embeddings, so that I can merge it back with the dataset, as enriched data.I gave the embedding columns names of g_eb_#, where the g stands for graph, eb stands for embedding, and the number is the embedding number. I used Karate Club for this, and you can learn more about Karate Club in this book. I also describe how to do this in my book.I then merge these embeddings with the previously enriched data, and then used the data to build a Machine Learning Model.What Was Important?Some Machine Learning models will tell you what they found to be important in making predictions. These are the top twenty features my baseline model found to be important.Note that out of all of the fields, only sex, pclass, age, and embarked were the most important from the original data. Note also that possible_father_present shows as the third most important feature. That’s surprising! My hunch was that the mother would matter more for survival. That’s an interesting insight, if correct.And then note that the other fifteen important features were graph features! However, this model is not great. It was built quickly, in minutes, for this example. I am not competing on Kaggle. This is for illustrative purposes, to show that enriching data is useful and often overlooked.This is my WhyThis is the actual reason I wrote my book: I wanted people to understand the added potential that comes with enriching your data, beyond surface level feature engineering. I want people to understand that you don’t just disregard data that you don’t know how to use. It can be useful, for additional context, if you learn to use it. I want people to begin to see networks everywhere, even when it is not obvious (people and cabins in the dataset).Context is king, and more context can make better models and higher quality artificial intelligence. However, context fields can also be useful in analysis to be able to answer more interesting questions.For instance, why did it matter so much that a father was present rather than a mother? Is this an error or an insight? The best part of data science is that a question will lead to more questions and then to greater understanding.That’s All, Folks!That’s all for today! Thanks for reading! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Artificial Intelligence and Machine Learning', 'Graph Algorithms', 'Graph Visualization', 'Subgraphs and Connected Components']"
/work/blog_data/138373463.day-16-of-100daysofnetworks.html,"I have a special gift for you all today! I created a simple crawler for building Music Collaboration Networks using Spotify. In order to use the Spotify crawler, you will need to have a Spotify account and get set up for authentication.To make all of that much easier, I am using the Spotipy Python Library. You can learn about it here.And you can use this to get setup on Spotify’s end.Today, I used this to build a network of B.B. King’s music collaboration (or sampling). I got to see him twice in concert in the late 1990s, and he was just amazing.But First…Before I dive into how to use the Spotify crawler, I want to say some things. A few things have made me really happy and thankful this week.My book got another five star review. It currently has 18 five star reviews, and nothing else. I really appreciate the enthusiasm of my readers! I don’t expect it to stay at five stars forever, but I’m enjoying it. If you’ve read my book, feel free to leave a review (even if not five stars). It helps other readers find the book.A few of my LinkedIn connections have been sending their networks to me and my blog. Thanks a ton. Network Science isn’t as popular as other topics in Data Science, and it takes a lot more energy to popularize this, so I appreciate it. I want people to learn this, because it is useful. Today, this blog has hit a nice milestone for 100th subscriber! I’ve also been having some really great conversations with readers. A few interesting ideas have come up in our conversations:Once you start working with networks, it affects how you think. You no longer think in simple lists, checklists, and spreadsheets. You start to think about communities, ecosystems, flow, and how things influence other things. It gives you a deeper understanding of reality, once you see how things fit together and are able to dig into relationships and influence.Every single time we look at networks, we identify relationships that make us go ‘aha’. Like, they seem obvious after you see it, but not before. Your brain is building the connection, as well.I just love talking to people who have read my book or work with networks, to hear their thoughts, and to learn how this awareness and understanding has affected their lives. Thank you, everyone. It’s been a cool week.Back to the Spotify CrawlerIn 2020, I worked on a Graph Neural Network project for work. /we used a Scientific Paper Collaboration Network as the dataset. I often think about that project, and it made me wonder what a Music Collaboration Network would look like.In 2021, I manually created a Science Fiction Author Collaboration Network, during the original #100daysofnetworks. That was really neat, and I’ll do that again, using crawling, eventually.Today, I created a Spotify crawler, and it’s only current use is building Music Collaboration Networks, so that they can be analyzed.For instance, some musicians collaborate with other artists. B.B. King was one such artist. I went to see him perform twice when I was younger, and I treasure those memories.Other artists—such as Wilco—collaborate with other Musicians, but not as much.And some artists, like Radiohead, don’t seem to collaborate with other Musician’s at all, at least not directly on their albums. But they do side projects.Today’s crawler looks for direct collaboration, not side projects. It is also based on the Spotify API, which doesn’t seem perfect, but seems good enough to make interesting networks. In some of my searches, I noticed that it did not pick up artists that I know collaborated on an album. I can look into this, in future iterations of the crawler.You can get the crawler code here.And you can see the quick analysis here.This crawler is brand new, so be patient with it. There might be bugs. It might not be perfect. The code is a bit gnarly, but I cleaned it up some. I have never crawled Spotify. Everything is new and confusing, but I did my best, and it seems to work.What Do I See?I have a few observations. If you use the crawler and set a seed artist who does not collaborate with anybody, then the network is going to be one node. Nothing worth looking at. If you don’t get anything interesting, it might not be a collaborative artist.But other artists are much more interesting. B.B. King was a good one, and Steve Vai was another good one to use.And you can pass in a list of artists, if you want to see how they overlap.To build a decent network, I searched for B.B. King. Here is the whole network.Without a doubt, this is cooler than many of the prepackaged networks that come with NetworkX. Learning how to build your own networks allows you to analyze the things that actually interest you, not somebody else’s data.If I look at B.B. King’s ego network with a radius of 2, it looks like this:And that led me to the questions:What did Lil Wayne do with B.B. King?What did U2 do with B.B. King?Let’s find out!What did Lil Wayne do with B.B. King’s music?What did U2 do with B.B. King?Networks have shown some collaborations I was never aware of. This is so cool. I love this. Even if the networks are smaller than the Wikipedia crawler produces, they’re very interesting networks, and useful if you love music exploration.I will experiment more and more with the spotipy library, as there is much more that can be done than this. Perhaps there are other networks and content that can be explored, using the spotipy library. That’s All, Folks!That’s all for today! Thanks for reading! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Artificial Intelligence and Machine Learning', 'Egocentric Network Analysis', 'Graph Visualization', 'Whole Network Analysis']"
/work/blog_data/138590036.day-17-of-100daysofnetworks.html,"People often ask how networks can be used for something practical, so in this post, I’m going to show you how I made a lot of use of Network Science and Data Science while working in Data Operations. Anyone who works in Data Operations, DevOps, or Site Reliability Engineering, this will be useful to you. However, I have also used this in Data Engineering, to understand various data pipelines.Absolutely, this is useful in these roles, and I am living proof. But how was this impactful?Before figuring out that this was possible, it used to take me weeks to dissect and map out a production server, sometimes over a month. Doing server uplifts requires understanding what runs on a server and what are the upstream and downstream dependencies. It’s very complicated, and I had to hold all of this in my head, in various Visio diagrams, and in way too many documents.After figuring this out, I was able to map out production systems in 1/10 the time. Yes, people ramble on about 10x engineers, but this literally made me 10x faster for the most time consuming part of uplift/migration work. Testing goes fast after you have a map. This creates the map.I was hired by one company in 2015 to uplift one very old and very complicated server. I call these “ancient” servers, though it was only 15 years old. All of the documentation was so outdated that it was actually harmful to use, because it was so incomplete. The server had close to 200 scripts running on it, automated, and created one very large file. It was a complicated beast, and the uplift project took THIRTEEN MONTHS. Other teams had tried and failed. My team was the first to succeed, and it ran smoothly after that until I left the company.So, these are real techniques, used by me, in Data Operations. In fact, in my work, I mapped out thousands of scripts across hundreds of servers, across data centers.After figuring out these techniques, we were able to do our uplifts much faster, and in one year, we were so successful in so many that we were recognized by the CEO. This is not imaginary. This helps. Others in the company wanted to learn these techniques from me, but I had not written my book, and it’s complicated to teach.In today’s blog, I am only going to scratch the surface. I used these and many other techniques from 2017 to 2020, and we had a perfect record on our uplifts/migrations, with no problems whatsoever. We received praise for having such boring migrations. If it sounds too good to be true, it’s not. It’s the right tool for the right job.Code and Data NetworksYou can follow along with the code here. I’ve tried to keep the code very simple today, as we are discussing something new. I have never met anyone else who does this, and this is my first time showing how to do it, openly.I usually call these “Dataflow Networks”, as they show how data is transformed. These days, in Data Engineering, DAGs do this. DAGs are graphs, and they can be visualized, but they can also be analyzed. What I’m showing today extends beyond DAGs and can be used with any code whatsoever. All it needs are inputs and outputs.Building the GraphToday, I built a simple imaginary dataflow network that is an ultra-simplistic similar system than the one I was hired to uplift. Instead of close to 200 scripts, I’ve only added a handful, to keep things simple, to help with understanding.I also manually created the graph, like so:I’ve added comments so that you can see each script separately. Look near 3.py, read those comments. If the script is put on the left, then the output is placed on the right. If a file is an input, put it on the left and put the script on the right. At the top, I’m using a Directed Graph, because that’s how this would work. Inputs get used by scripts to create one final combined_export.json file.Visualizing the Dataflow NetworkAfter creating this graph, we can visualize it.Even with just that, it’s easy enough to know what this imaginary server does and how it works. At the top left, I can see that two Python scripts create two .json files. Next, 3.py and 4.py write the data to MySQL. 5.py then picks up the data and extracts two things. Then 6.py takes this data and uses it to create a combined export file.Troubleshooting is EasyI know, that’s a generalization. However, on ancient, undocumented servers, teams can spend DAYS trying to figure out what broke, what it impacted, and how a cascading failure occurred.With this, instead of days, it can take MINUTES. After mapping out ancient, undocumented, orphaned servers, it is easy to find the source of the problem. Just start at the known failure and work your way backwards, upstream. Let’s pretend that combined_export.json is still being created, but the data is stale. What’s upstream from the file? 6.py. The log files from 6.py indicate that the script is running well. Ok, let’s go back another step. OH NO! There’s an error in 5.py, and it’s been failing and no new data has been written to the two files it produces. And 6.py doesn’t check for changes in the data before doing it’s work. Hey, that explains the stale data! Turns out MySQL crashed, causing 5.py to malfunction, causing staleness in the combined export.That’s an easy story to tell, with a network visualization. That is something your manager will understand and appreciate. This seems like the way data operations should be done, but it is not. We didn’t get data science help in data operations. Most analysis was very simple. Most troubleshooting was manual and slow.This makes understanding complex production systems very easy. You can do this. Anyone can.Network Science Goodies are UsefulOnce the graph has been constructed, everything else I have written about becomes available. You can use ego networks to understand individual nodes and what works around them. You can use communities to find subsystems in the overall system. You can identify weaknesses and bolster them. And you can easily find what are the most critical pieces.That makes sense. If MySQL goes down, everything that uses MySQL is affected. If 5.py goes down, the final export file is never created. Everything else is important, but less critically important, and it’s measurable.Fine, How Do I Start?It has been an uphill battle getting people in Data Operations to use these techniques, even though they are so obviously useful and informative. I think it’s because there is no tool that just does all of this naturally. There are tools that do SOME of this, and DAGs can be visualized, but there is nothing that covers everything, and there may never be.That doesn’t mean you give up. You can use these tools to do enough to be useful for whatever you are working on.Here’s how you start simple:Find a simple server or workflow that you know about that just has a few things running on them.Start at the orchestration, the automation piece. This is often CRON. CRON will tell you what runs on a server and when. It doesn’t matter if it’s CRON. Find the orchestrator. Note the scripts.For each script, identify the inputs and outputs.Build the graph, like I showed.Read my book and blog and practice whatever looks useful and interesting.Just start small and simple, even with an imaginary system, like I did. Or try something I’ve never tried: map out how your code/system should work before even writing a single line of code. Visualize how you want it to work, then build it. Try the reverse of what I’m doing.Want More Like This?Network Science and network analysis is useful for more than just social scientists and data scientists. There is no reason that people in Data Operations should not be using these techniques. This saved me so much time and made my team effective. It made us 100% successful on our uplifts/migrations, and it got us recognition all the way to the top of the company. But these are different types of networks than I usually show. If this is useful and informative to you, please let me know, and I’ll be encouraged to write more. I might even write a smaller book on just this topic, to help the Data Operations community next.Dive In and Try StuffJust jump in and try stuff. Once you start, there’s no going back to the old ways. This has completely replaced the old methods I used to use. I use this in my own work, and in my company. And if you get stuck or just want to brainstorm an idea, message me on LinkedIn. This might feel tedious at first, but only at first. This is good “listen to music and get stuff done” work. Sometimes, you just have to dig in. Creating the graphs is simple once you identify the inputs and outputs, and then insights are easy to get to immediately after.Fun Piece of TriviaI’m actually writing this blog post using a laptop I won in 2019 at McAfee as an award for mapping out production systems using these techniques. That’s All, Folks!That’s all for today! Thanks for reading! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Community Detection', 'Egocentric Network Analysis', 'Graph Visualization']"
/work/blog_data/138621722.day-18-of-100daysofnetworks.html,"In my previous work, I worked on a Graph Neural Network, and we used an academic paper author collaboration network as our dataset. That work has inspired other interesting network analysis I’ve set up and done, such as:Spotify Artist Collaboration NetworksScience Fiction Author Collaboration Networks (I will redo, soon)Today, I’m adding arXiv Author Collaboration Networks to the list. I’ve built a way to pull datasets from arXiv (using the ‘arxiv’ Python library and some minimal code) on any topic whatsoever. This means that you can programmatically pull data from arXiv to research any topic whatsoever. If someone has written about a topic, you can find it.arXiv is a platform for scientific papers. That’s the quick and dirty. This searches for academic papers on any topic.Use this for yourselves and for your companies. This can help you have a more focused learning path than everything everywhere all at once.'How To Use It?The code is available here, and I’ve created a simple analysis notebook here.How I Used It?I wanted to use this to research Network Science researchers, to understand who is collaborating the most.With the first dataset I pulled, I built a author collaboration network. With that, I can see which authors have the highest Page Rank, indicating overall importance. This is what I see, in the ego network of the most collaborative author.If I drop the center (one of my favorite techniques), I can see the subgroups that are part of Danielle’s ego network.And I can see which articles Danielle and her collaborators worked on together.The data that this crawler pulls is also very recent, with the latest article not even 24 hours old. This will be a useful way to keep up with any topic.Follow Your Curiosity and CreativityThe best advice I have for any learner is to follow your curiosity and creativity. Yes, sometimes you will have to buckle down and knock out tedious work, but you can also have fun learning. That means, use things like this to pull datasets that excite you or stimulate your creativity. Chase what makes your heart sing. That’s what you are meant to research, in my opinion.Manageable Knowledge ManagementThat’s quite a phrase, but hear me out. I often hear data professionals say that they can’t keep up with the deluge of information about Machine Learning. Well, with this, you can. You don’t have to read everything. Everything everywhere all at once is a terrible approach to learning anything. Using this tool, you have articles, authors, dates, and summaries. With a few NLP tricks for similarity, you could easily build a simple search to find what is most relevant to you, and get rid of that overwhelming feeling of drowning in information. You could also filter by date to only read the recent stuff.That’s All, Folks!I’m going to keep today’s post light and stop here. There’s much more we can do with these networks. We will use these throughout this adventure.That’s all for today! Thanks for reading! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Artificial Intelligence and Machine Learning', 'Egocentric Network Analysis', 'Graph Algorithms', 'Graph Visualization', 'Subgraphs and Connected Components', 'Whole Network Analysis']"
/work/blog_data/138791842.day-19-of-100daysofnetworks.html,"Earlier this year, I was reading a book called The Ascent of Information. The entire book is a whirlwind of insights as well as a page turner. I couldn’t put it down. Later in the book, the field of Artificial Life is discussed, and I became intrigued the more I read. If you haven’t read this book, it is one of my favorites.Last week, I created an arXiv data collector, so that I can quickly research any topic of interest, and analyze the collaboration networks that exist in the topic ecosystem. Today, I used that arXiv collector to investigate Artificial Life. I think I have always been interested in this field. Artificial Intelligence gets all of the attention, but there are people working on Artificial Life research as well. It is a very cool field of study.Code is AvailableThere are two Jupyter notebooks this week:Network AnalysisText Analysis (Simple NLP)Bipartite Projection CorrectionIn one of the steps used during this blog series, I did not quite understand what was needed for one parameter of a function, so I did it wrong. It worked, which confused me. It wasn’t a breaking error, but it essentially created a network of authors as well as a network of titles. Today’s code has the correction, for how to properly do Bipartite Projection, to build these collaboration networks.It looks like this. You are supposed to provide the nodes that you want to appear in the projected graph. I was giving it every node, which isn’t the way it should be used. Good to know! I also suspect that this projection approach is a better approach than how I mapped out the Alice in Wonderland network in my book, so I will be playing with that idea in later days, to see if I can improve the network.What’s in the Artificial Life Network?I have a lot to cover and limited space on Substack, so will go fast. Play with the code to learn more, and to do your own research on your own topics of interest!The arXiv search engine was returning too much junk data about Artificial Intelligence, so I used a filter today to only keep articles that explicitly mention “artificial life” in the summaries. This is the first simple NLP trick, using NLP to cleanup a graph.After the filter, a very small network remained.Graph with 307 nodes and 2026 edges
This is easily small enough to visualize.I think this is beautiful. It shows that this is a small network. I think of it as quiet. Large networks are much more complex. This has all of the interesting pieces I would hope to find:Isolate nodes (people who wrote a paper alone and only one paper)Connected components (small groups that worked on a paper)Dense ecosystem (people who have collaborated on multiple papers)This is enough to also be able to identify some of the most active authors in the collaboration network.If someone wanted to enter the field of Artificial Life, these might be people worth getting to know. They have written papers, and their network positioning shows their influence.Some of the ego networks are very interesting and reveal something about papers. There are some papers that have dozens of authors.In this image, it shows that Kenneth likely collaborated on at least three papers. One with Jay, one with the group on the top, and one with the group on the bottom right. But that supercluster is unusual, and now I am noticing them more on arXiv.If I look at the core of the network, I can see that there are three of these clusters.What Articles Are There?Today, I want the focus to be on usability of these networks. I want to show what papers I’ve found, more than I want to talk about networks. I want this to be useful, not just neat.If you look at the code, you can see that this is not a fast-moving field in terms of number of papers written per month. It’s not like Artificial Intelligence, where it’s impossible to keep up with the research.After applying the filter to remove non-ALife papers, the maximum number of papers per month is four. The topic is also written about in different categories. Read about arXiv’s category taxonomy.The top three categories are:Computer Science: (Neural and Evolutionary Computing)Computer Science: (Artificial Intelligence)Nonlinear Sciences: (Adaptation and Self-Organizing Systems)Good to know! Now, to top off this article, I’ll link to as many papers as I have space, the ones that look most interesting to me today.Artificial Life Papers by CategoryCategory: cs.NEAchieving Connectivity Between Wide Areas Through Self-Organising Robot Swarm Using Embodied EvolutionAn Artificial Life Simulation Library Based on Genetic Algorithm, 3-Character Genetic Code and Biological HierarchyAn Artificial Neural Network Functionalized by EvolutionAn Overview of Open-Ended Evolution: Editorial Introduction to the Open-Ended Evolution II Special IssueAn artificial life approach to studying niche differentiation in soundscape ecologyArtificial Life in Game Mods for Intuitive Evolution EducationArtificial Life using a Book and BookmarkerArtificial life: sustainable self-replicating systemsAspects of Evolutionary Design by ComputersBiologically Inspired Design Principles for Scalable, Robust, Adaptive, Decentralized Search and Automated Response (RADAR)Capturing Emerging Complexity in LeniaChallenges and Opportunities of Evolutionary RoboticsCo-generation of game levels and game-playing agentsComplex NetworksDXNN Platform: The Shedding of Biological InefficienciesDesign of a P System based Artificial Graph ChemistryDigital Genesis: Computers, Evolution and Artificial LifeEvolutionary Generation of Visual Motion IllusionsEvolving Differentiable Gene Regulatory NetworksFlow-Lenia: Towards open-ended evolution in cellular automata through mass conservation and parameter localizationIncreased Complexity and Fitness of Artificial Cells that Reproduce Using Spatially Distributed Asynchronous Parallel ProcessesIncreasing Behavioral Complexity for Evolved Virtual Creatures with the ESP MethodJohnnyVon: Self-Replicating Automata in Continuous Two-Dimensional SpaceMatchmaker, Matchmaker, Make Me a Match: Geometric, Variational, and Evolutionary Implications of Criteria for Tag AffinityModelling SARS-CoV-2 coevolution with genetic algorithmsOn the Liveliness of Artificial LifeOpen Questions in Creating Safe Open-ended AI: Tensions Between Control and CreativityPassive and Driven Trends in the Evolution of ComplexityQualities, challenges and future of genetic algorithms: a literature reviewRole of Morphogenetic Competency on EvolutionSafe Mutations for Deep and Recurrent Neural Networks through Output GradientsSelf-Organizing Intelligent Matter: A blueprint for an AI generating algorithmSelf-Replicating Machines in Continuous Space with Virtual PhysicsSignalGP-Lite: Event Driven Genetic Programming Library for Large-Scale Artificial Life ApplicationsSooner than Expected: Hitting the Wall of Complexity in EvolutionThe Importance of Open-Endedness (for the Sake of Open-Endedness)The Self-Organizing Symbiotic AgentThe Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research CommunitiesTowards Large-Scale Simulations of Open-Ended Evolution in Continuous Cellular AutomataTowards a Framework for Observing Artificial Evolutionary SystemsWebAL-1: Workshop on Artificial Life and the Web 2014 ProceedingsCategory: cs.AIAIR5: Five Pillars of Artificial Intelligence ResearchAn Inductive Formalization of Self Reproduction in Dynamical HierarchiesChemlambda, universality and self-multiplicationEmotional Responses in Artificial Agent-Based Systems: Reflexivity and Adaptation in Artificial LifeEvolved Open-Endedness in Cultural Evolution: A New Dimension in Open-Ended Evolution ResearchFrom Alife Agents to a Kingdom of N QueensHybrid Life: Integrating Biological, Artificial, and Cognitive SystemsIntelligence as information processing: brains, swarms, and computersMachine LoveMotility at the origin of life: Its characterization and a modelPast Visions of Artificial Futures: One Hundred and Fifty Years under the Spectre of Evolving MachinesPerspective: Purposeful Failure in Artificial Life and Artificial IntelligencePerspectives for Strong Artificial LifeQuantifying Natural and Artificial Intelligence in Robots and Natural Systems with an Algorithmic Behavioural TestTuring's Imitation Game has been ImprovedCategory: nlin.AODiversity EvolutionEvolution of Complexity: Introduction to the WorkshopIntroduction to Random Boolean NetworksNetwork Complexity of FoodwebsOpen-Ended Artificial EvolutionPossible Laws for Artificial Life EvolutionSelf-Organization and Artificial LifeSelf-Organization and Artificial Life: A ReviewThe differences between natural and artificial life. Towards a definition of lifeCategory: adap-orgEvolutionary Learning in the 2D Artificial Life System ""Avida""Propagation of Information in Populations of Self-Replicating CodeSwarms, Phase Transitions, and Collective IntelligenceCategory: physics.gen-phEmergence in artificial lifeEvolution in the MultiverseThe Evolution and Development of the UniverseCategory: nlin.CGClassification of Complex Systems Based on TransientsEvolving Structures in Complex SystemsLenia - Biology of Artificial LifeLenia and Expanded UniverseCategory: q-bio.PEEvolution of ComplexityCategory: cs.ROSustainably Grown: The Underdog Robots of the FutureCategory: cs.ITJIDT: An information-theoretic toolkit for studying the dynamics of complex systemsMechanical generation of networks with surplus complexityTowards a Synergy-based Approach to Measuring Information ModificationCategory: cs.MASelf-Regulated Artificial Ant Colonies on Digital Image HabitatsCategory: q-bio.OTA number theoretical observation about the degeneracy of the genetic codeCategory: cs.OHOrganized Complexity: is Big History a Big Computation?Category: math.NAIn search of an evolutionary coding styleCategory: physics.bio-phAb Initio Modeling of Ecosystems with Artificial LifeCategory: physics.opticsThe Enlightened Game of LifeCategory: cs.DCArtificial life, complex systems and cloud computing: a short reviewCategory: q-bio.NCOn Artificial Life and Emergent Computation in Physical SubstratesCategory: cs.CYCities as they could be: Artificial Life and Urban SystemsThat’s All, Folks!Sorry for the sporadic linking at the end. I reached max length for a Substack post so had to stop.That’s all for today! Thanks for reading! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Artificial Intelligence and Machine Learning', 'Artificial Life', 'Egocentric Network Analysis', 'Graph Algorithms', 'Graph Visualization', 'Subgraphs and Connected Components', 'Whole Network Analysis']"
/work/blog_data/138977991.day-20-of-100daysofnetworks.html,"Hello everyone. Today’s post for #100daysofnetworks is going to be very simple, as I want to show how to do just one thing: convert a PDF to text. That’s the goal.But first, DAY TWENTY!!! NICE MILESTONE!In my book, I show how to convert raw text into social networks that can be explored. I use the text from Alice in Wonderland to create the coolest social network that I am aware of. However, I learned this technique a few years prior, when I used the Bible’s book of Genesis to create an ancient social network graph. It’s not much to look at right now, but this is what I created back in 2018 or so.It’s my first network visualization. It would look much better if I were to recreate it again, now, with all that I have learned. It would be very cool to explore in Graphistry, too. Graphistry is the coolest network visualization software that I am aware of.The larger circles are nodes with the most edges (connections). So, the largest nodes were God and the most talked about people in the book of Genesis. Even though I am not religious, the fact that I could convert text into networks felt like a superpower. I used to think of it as “Cliffs Notes on Steroids”, because rather than just reading someone’s summary of what a book is about, I could actually interact and explore the social network, getting to know that characters and people in another way. Read this paragraph slowly and understand: This can be done with ANY text whatsoever. If people are mentioned in text, if NER (Named Entity Recognition) can properly extract those entities, then you can build a network off of that text. Any text imaginable. Email. Random PDF files from the internet (be careful of malware). The internet is made of text, and there are files that can be used as well (with care).That means that this has limitless use. Even audio can be transcribed into text and then text into networks. And this isn’t limited to the English language.We can use this to explore and understand our world and existence in ways not previously possible. It would be extremely time consuming to map out the book of Genesis manually, to read each verse individually and draw dots and lines for each person mentioned. With NLP, you can create social networks from raw text very quickly, in seconds.The largest file I have attempted this on was a random 600 page PDF I found on the internet. I tried it the day after my book was published, just to see if the techniques in my book would work with much larger pieces of text. Yup. No problem at all. In today’s code, I show how to convert a PDF file to text, and then use that text with a sentence tokenizer (a first step in NLP). From that point on, text is data and we have a lot of options with how we can use it.The code is very simple, because the PyPDF2 library is doing the complicated work, making this easy. I also include ‘tqdm’ in the function, which gives a nice status bar. A 600 page PDF takes a lot longer than a ten page PDF, so it is nice to have.That’s it. That’s all I’m going to teach today. We’ll be using in coming days to convert a PDF into text and then into a network, and then we’ll explore that network. I’m looking for a PDF I want to use in the demonstration, and I suspect arXiv will be less useful with NER because citations are written differently than how people are named in other documents.Text as DataMost of my work involves “text as data”. I’ve gotten very good at extracting information and context out of text. In my work and personal research, the book Text as Data has been really helpful to me. If you are interested in Natural Language Processing (NLP) or in using text to extract networks, this is my current favorite NLP book. I can’t recommend this enough. This is not a programming book. It is a very useful description of NLP, and it describes how and where techniques can be useful. Recognizing OthersOne of my book readers has been really getting into Network analysis and Natural Language Processing, and I want to showcase some of her work. It makes me happy to see others who “get it”, who understand the value and opportunities that exploring networks can give to us. I love that she shows her work and makes cool presentations for each of her analysis.Snoop Dogg Musical Collaboration Network (Spotify)Britney Spears Musical Collaboration Network (Spotify)I also really enjoy her random Network Analysis study posts, like this. It’s always cool to see people learning to explore networks that are interesting to them, learning more about the things that they care about.Thank you, Tré Rodríguez-Terry, for showing your learning adventure and progress. It’s so motivating for me to see.A few others have shared their work with me, as well, like the winners of previous book giveaways. Thank you to everyone who has done this! It motivates me to share more and learn more!Do you have something you’d like me to showcase? Let me know and I’ll see if it fits!Music: Just for FunFinally, there’s another side of me that I don’t talk about as often. If there is one thing that I am about, it is EXPLORATION. That exploration manifests a few different ways in my life:On teams I’ve been on, I’ve been tasked to figure out how to even do something. Not necessarily how to do something best, or how to do something fastest, but how to do something at all. To take an idea and figure out a realistic path from idea to runnable and then to automation. It takes a lot of experimentation to do that. There’s not much that is helpful when you are figuring out how to do new things, and I love being in that kind of idea wilderness.These network posts are another example. My book is more about network exploration than it is about Graph Machine Learning, and my book is not at all about Graph Databases. I want to show people how to explore networks, because there are insights to find. You should be able to do this without requiring a database, and I show you how.But my music is where I am most experimental.So, to me, these are all the same. They’re equally a part of me. I am as much a musician as I am a data person or programmer, and I’ve been playing music for just as long.A friend of mine mentioned that I should share some of my music on LinkedIn (where we hang out), because it is a part of me, and because it can bring people happiness and maybe even some peace in their life. So, I started doing that. I won’t do a long introduction like this in later posts. This is just the why. But if I post some new music on LI (I’m not a Youtuber), then I’ll tag it here. Take it or leave it. Maybe you’ll enjoy it. It’s another side of me, an important part of who I am, and it is directly related to learning, creativity, innovation, and exploration. These are not performances. These are me learning music and exploring sound. I don’t sing. My brain won’t even let me…Original untitled, heavierOriginal untitled, cleanOriginal cover of Jimi Hendrix’ “Bold as Love”Original cover of Hozier’s “Take Me to Church”Jimi Hendrix “Little Wing” IntroductionI hope you’ll enjoy it. It’s nice seeing that people enjoy my guitar playing so much. I didn’t expect that. Thank you.That’s All, Folks!That’s all for today! Next, I’m going to show a bit more about Named Entity Recognition, using text extracted from PDF. Stay tuned! Thanks for reading! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Artificial Intelligence and Machine Learning', 'Graph Visualization', 'Whole Network Analysis']"
/work/blog_data/139139713.day-21-of-100daysofnetworks.html,"For a while, I have suspected that I have a simpler way of creating entity networks. In my book, I show how Part-of-Speech Tagging (POS-tagging) and Named Entity Recognition (NER) are both useful in creating entity network graphs. To save everyone a lot of time, here are some considerations:POS-tagging: this allows you to identify the part of speech any word belongs to. In building entity networks, I am looking for NNPs, but if I wanted to build a knowledge graph, I’d add complexity. Read more here.NER: this only extracts entities, but there are many types of entities that can be extracted. I primarily extract people, places, and organizations, but there are more types that can be used. This is a good book about spaCy.My book covers this in detail. Please read it to learn more. I show how to create these entity graphs.Yesterday, I decided to revisit Alice in Wonderland and see if my idea could either:More easily create the networkMore accurately create the networkDo bothMy gut feeling is that this new approach is doing both. Graph construction and cleanup both took fewer steps.How Does It Work?In my book, I show my previous approach. Basically, the difference is that I include a simple loop to wire up the entities, and in the new approach, I let bipartite projection do the work for me. It’s easier and faster.You can see today’s new code implementation here. You can modify it to use any text.I used the entity extraction code from my book and made one small cosmetic improvement. I added in a ‘tqdm’ status bar, so that we can see how far along in the process the NER step is, as that takes some time.Notice that it took 47 seconds to read Alice in Wonderland and extract the entities from each sentence.NER is not exact. It falses occasionally, both positively and negatively. If it doesn’t recognize a name as a name, it won’t catch it. Other times, it’ll trip up on capitalization and consider a non-entity as an entity. But the code shows how to quickly remove junk. In the above screenshot, Longitude, Antipathies, and Ma’am shouldn’t have been extracted as entities. As NER models improve, this problem should decrease over time. I am using spaCy for NER.The next thing I do is convert this entity list into a Pandas DataFrame comprised of columns for the entity and the sentence it belongs to. It might be better for me to call this as ‘row’ rather than sentence, but it is what it is.With that, we have everything we need to create a Bipartite Graph of entities and the sentences they belong to.I showed how to do this a few times already, actually. My Eureka moment that this might be better than my old approach was when I used it on the Titanic dataset. I was shocked by how well it was able to network families together.After doing that, I did not push the idea further. I was just happy with the results, but I already had a hunch that this could replace my former method.I then used it with the arXiv networks, as a bipartite graph could be created using authors and research papers.Yesterday, I tried this bipartite projection approach to Alice in Wonderland, to see if it’d create as good or better of a network as the one in my book. Turns out, it did, and much more easily. After some validation, I expect that this will become my primary approach for creating entity networks from text.This is the code for this new approach. Bipartite projection is wiring up the nodes, where I previously wrote a loop to do it for me. I’ll explain the code:I use the entity_df as input data, as the edgelist. I call this B as it is a bipartite graph.I then do what is called bipartite projection, to convert this into a entity network graph rather than a bipartite graph. Read more here.I drop all the isolate nodes, as most isolate nodes were false-positives from NER. There was no connection from them to other characters. It would be better to skip this step and manually add junk nodes to the drop_nodes list, but this saves time and can be helpful.I then (in another cell) investigated the remaining node names and added all found junk nodes to the drop_nodes list, and then removed them, removing the rest of the junk.Two steps for construction, two steps for cleanup. Super simple. How does it look?I think this looks great. The largest cluster is the social network of the book of Alice in Wonderland. Outside of this core cluster, notice that some of the ‘places’ entities are linked together, due to being written about in the story. This is very clean already. It never felt this easy when I first figured this out using POS-tagging, nor when I figured it out for my book using NER. It’s less work for construction, and less work for cleaning, which means more time for exploration, fun, and finding insights.Network Science in WonderlandWe are using a combination of software engineering, data science, network science, and social science to analyze the social networks found in TEXT that was written in 1865. This does not replace reading, nor does it eliminate the value or importance of reading. This gives us new ways of learning, and new ways of interacting with literature. It is a learning tool.As the network has been built, we can use Page Rank to determine important characters. This is a bar chart of the Page Rank values of each character. Alice has a much higher Page Rank value, and that makes sense as she is the main character. She is connected to many of the characters. Dormouse is the second most important character in Alice in Wonderland, based on network characteristics.We can look at the ego networks of the most important characters.Here is Alice’s ego network. Here is Alice’s ego network with the center (her own node) dropped. When I have a complex ego network, I often drop the center to be able to see the groups that exist in the ego network. After dropping Alice, I can see that there are two larger cliques or groups present (Mary Ann’s and Cat’s). Other ego networks can be investigated as well. Run my code and explore!Neat FindingWhile doing bipartite projection, I had an idea. What if I were to project on the sentence number rather than on the character? How would that look? Now this… this is cool. It basically converted the entity network into a “connect the dots” drawing, and shows the steps for how the characters were wired together. I need to think about this more. I bet that this has uses that I haven’t imagined, yet.That’s All, Folks!That’s all for today! Thanks for reading! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Artificial Intelligence and Machine Learning', 'Egocentric Network Analysis', 'Graph Algorithms', 'Graph Visualization', 'Subgraphs and Connected Components', 'Whole Network Analysis']"
/work/blog_data/139160355.day-22-of-100daysofnetworks.html,"Yesterday, I used a new approach to more easily convert text into usable social networks that can be analyzed. I used Alice in Wonderland, so that I could compare to the approach that I used in my book.I like to use fictional social networks to demonstrate network attack, so, today I’m going to do that. Today’s post will be different than usual.I come from cybersecurity, and think in terms of offense and defense. There is a quote I heard while working at a previous job: ""Defenders think in lists, attackers think in graph.”I don’t agree with the quote, because I am showing you how you can think in graphs or networks. This is just heightened awareness. I call this heightened awareness Network Thinking.I believe that network thinking is valuable because it presents opportunities to understand and influence the world around us. I’ve already written several posts about how this is useful for understanding. Today, I’m going to show a type of influence: disruption.Obviously, there are other kinds of influence. We’ll talk about those another day. Today, we’re going on the offense. We’re going to pretend that the red queen got her way.Network awareness can be useful for defense as well. If you understand your environment, you can bolster it so that it is less brittle. But if you think in terms of lists, not graphs, you will do things differently. We can do some defense experiments another day.Disruption ApproachesYou can get see today’s code here.Today, I am going to try two different approaches to cause as much disruption to the Alice network as possible. The goal is to shatter the network into pieces so that communication is disrupted and the social group is broken into small pieces.Approach 1 (Sequential): I will do four iterations, dropping the node with the highest betweenness centrality at each step. In each step, betweenness centrality is recalculated.Approach 2 (Burst): I will simultaneously drop the four nodes that had the highest betweenness centrality scores in the original network.My intuition is that the first approach will cause greater disruption, as the most important node is identified at each step. I expected that this would take more work but have more impact.First, let’s look at the original network.Even visually, you should be able to notice that if Alice’s node is removed, many nodes will become disconnected. Learn to notice weak spots in networks. They often look like stars. Approach 1: SequentialPlease see the code, for more detail. I will show the results on this blog.In the first iteration, Alice was removed.I am not showing isolate nodes. Already, we can see that the network looks significantly different, and it has broken into pieces. What nodes do you think have highest betweenness centrality? Learn to notice them visually.Next, Hatter is removed. The Mad Hatter!Notice that the community that he was a part of has split.Next, Rabbit is removed, the White Rabbit. Finally, Duchess is removed.And that is all that is left of the communication channels in this social network. The largest cluster only has five nodes.Approach 2: BurstIn the second approach, I identified the four nodes with highest betweenness centrality, and then I dropped them all at once, in one burst. This was a simpler approach, so there is only one output image.I can already see that this method appears less effective. Take a look at Hatter’s community on the left. Several important characters are still grouped together. This shattered the network, but in this instance, it looks a bit less effective. Key figures are still working together.Approach Comparison Let’s compare the two approaches.Original Density: 0.075Final Density:Approach 1: 0.032Approach 2: 0.032Hmm, ok, this looks identical. Are they equally effective?Original Connected Component Count: 1Final Count:Approach 1: 20Approach 2: 18The first approach managed to shatter the network into more pieces.Average Connected Component Size: Final Average Connected Component Size: Approach 1: 1.65Approach 2: 1.83The first approach managed to shatter the network into smaller pieces.In conclusion, in this network, the iterative approach caused greater disruption than the burst approach. With the first approach, resulting groups are smaller.Offense and DefenseThis is offense and defense. This is a more serious topic, but a fun game nonetheless. Because I know that nodes with high betweenness centrality hold networks together, I knew that eliminating them would shatter the network into pieces. This is network thinking. In contrast, it would be much less effective to just work off of a list and hope for the best. The more context you have, the more effective you can be, in offense, defense, or anything.With these networks, we can imagine different outcomes. We can simulate different approaches and ideas.How’d We Do?Before ending, let’s take a final look at the resulting disruptiveness of the first approach.Before:After:Yup, that’s busted. That’s All, Folks!That’s all for today! Thanks for reading! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Attack Simulation', 'Graph Algorithms', 'Graph Visualization', 'Subgraphs and Connected Components', 'Whole Network Analysis']"
/work/blog_data/139376846.day-23-of-100daysofnetworks.html,"Hello, everyone. Last week, I wrote about network attack simulation, showing how removing just a few nodes can be very effective at disrupting a small social network. Today, I wanted to try something much more ambitious. Last week, I used the Alice network, which only contained 51 nodes and 67 edges. Today, I decided to do another attack simulation but on a network of 65,260 nodes and 306,509 edges. This is a complex network. I used the arXiv Network Science collaboration network built previously, but the subject of the network doesn’t matter. This has more to do with structure than what the nodes represent.GET TODAY’S CODE HERE.I had four goals for today’s experiment:Cause as much disruption to the network as possibleWithout destroying it completelyDo it quicklyBe able to investigate the surviving network componentsThis guided my methodology of attack:Destroy 500 top key nodes per iterationDo 50 iterationsDrop all isolate and two-node components at each stepBecause of the last part, the network will shrink at each iteration. Nodes that have been disrupted will be dropped. I say that they have been disrupted because isolates are not connected to another node, and a two-node component isn’t much better. So, I will consider isolates and two-node components as having been disrupted. They can no longer communicate with the rest of the network. Information sharing has been disrupted.Degree DistributionBefore starting, let’s look at the original degree distribution of the original network.This is a histogram of node degrees (edge count). Most of the network has between fifty edges (connections). Rarely do nodes have more than fifty edges, but there are some. After the attack is completed, I expect that there will be very few nodes with many edges, as I expect to be able to massively disrupt this network.Attack SimulationI have left some code in place so that you can print the dropped nodes if you want, but on my end, the simulation output looks like this:running loop: 0
Graph with 57619 nodes and 283677 edges
---------------
running loop: 1
Graph with 56438 nodes and 270568 edges
---------------
running loop: 2
Graph with 55415 nodes and 258991 edges
---------------
running loop: 3
Graph with 54379 nodes and 246397 edges
---------------
running loop: 4
Graph with 53542 nodes and 234412 edgesThe code itself is simple:def remove_nodes(G, iterations=50, cut_size=500):
    
    results = {}
    
    G = G.copy()
    
    for i in range(iterations):
        
        results[i] = {}
        
        results[i]['nodes'] = len(G.nodes)
        results[i]['edges'] = len(G.edges)
        
        print('running loop: {}'.format(i))
    
        cent_df = get_key_node_df(G)

        drop_nodes = sorted(cent_df[0:cut_size].index)
        
        #print('removing key nodes: {}'.format(drop_nodes))

        G.remove_nodes_from(drop_nodes)

        G = nx.k_core(G, 2) # no isolates or two-node groups

        print(nx.info(G))
        
        print()
        print('---------------')
        print()
        
    return G, resultsAt each step, I track the original node and edge count so that I can later investigate the impact of each step. Let’s look at that.Because this is a large network, I am using Page Rank instead of Betweenness Centrality to determine key nodes to drop. Page Rank is very fast on large networks, and Betweenness Centrality becomes painfully slow. Use Page Rank to determine importance when Betweenness Centrality becomes unfeasible. At each step, key nodes are dropped. When key nodes are dropped, many nodes become isolates, as they no longer connect to anything. I drop all isolates and two-node components. Above, you can see that at the first step, nearly 8000 nodes were disrupted, either by being orphaned as isolates or being part of a two-node component. After the initial massive disruption, the network was shattered into pieces (connected components), but there were still more important nodes to attack. Later steps continued to disrupt between 500-2000 nodes per step, eventually stabilizing. How do the edges look?When nodes are dropped, all of their edges are dropped as well. This shows the disruption a bit differently than when looking at nodes. The first step removed close to 25,000 edges, but there were two other steps that also caused the removal of nearly as many edges.Visually, What Happened?The largest connected component (structure) of the original network does not visualize well, so I will show the second largest. The second largest conveys the point well.This is the second largest connected component of the original network, before the attack was simulated. Look and see if you can identify weaknesses in the structure. Look for parts that look like could be cut with a pair of scissors, splitting the network into pieces. I can see several. This is the BEFORE image. This image shows the number of nodes in each of the LARGEST connected components in the original network. Above, I visualized component 816, the one with 121 nodes. The largest component would not be anything to look at. It is too complex for simple network visualization software.Here are the LARGEST connected component sizes AFTER the attack simulation.This is devastating. No component larger than 10 nodes haS stayed intact. It’s very interesting that all of the largest components are ten nodes. Nothing more than ten. Why exactly 10? That’s interesting.Before I show what these look like, let me show you what resilient network structures look like from the original network.I have highlighted some of the types of structures that are more resilient to attack. They remind me of crystals. When each part of a groups is connected with each other, it has a high density value and is less susceptible to this attack.So, what the the largest remaining structures of the attacked network look like? Let’s look at a few:And another:And another:What Does This Show?What’s the takeaway from all of this? Well, networks are all around us and there are different types. However, what this shows is that certain network structures are more susceptible to attack than others, and other structures are more resilient. It shows that a mesh structure can withstand attack better than a star, for instance.The point is to begin to think about the networks that are around us, to understand how they work, and to have a true understanding of their resiliency or fragility to attack. If you have a “single point of failure”, that’s a key node. That’s likely either a bridge node or part of a star. Think networks. Understand beyond lists. Use networks to think both offensively and defensively. Oh, and here’s the final results of the simulation.More than 40,000 nodes were fully disrupted, and nearly 265,000 edges lost in the process. Though the network only lost less than 1/3 of the number of nodes, the impact on edge count (information sharing) is drastic. That network is smashed.That’s All, Folks!I will be doing more attack simulation during this iteration of 100daysofnetworks. I suspect that different types of networks are more resilient to attack simulation due to the nature of the network, and I want to explore that idea.Feel free to play with the code to try different attack simulations. This is only one. Try different approaches. What would smash the higher density areas of the network? I have an idea for another day.That’s all for today! Thanks for reading! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Attack Simulation', 'Graph Algorithms', 'Graph Visualization', 'Subgraphs and Connected Components', 'Whole Network Analysis']"
/work/blog_data/139403026.day-24-of-100daysofnetworks.html,"Today’s post is going to be high level. No code will be written, and no networks will be analyzed. I don’t always feel like writing code. But I enjoy thinking about networks during my weekend. So, I wanted to do a post today, but I wanted to avoid writing code. Rest is good.My first thought was that I should do a post about Network EDA. I will eventually do one on this topic, as I believe that we should strive for some standardization. Many blog posts and videos have already been written about Network EDA, and my book is about exploring networks (the E in EDA), but I would like there to be a tool someday that does similar to df.describe() if you are familiar with Pandas. But I will save that for another day. Today, I decided to talk about the different types of network analysis, because there are more than one. In fact, I’ve already shown several. When I create reports about networks, I typically have a few sections in my reports for different types of network analysis. In fact, I even do that on this blog series. I’ll start with a high level overview of the network, then look for key nodes, then look at ego networks, then look at communities, and so on. Each of these is a separate analysis, but together they provide a lot of context.Types of Network AnalysisAs I mentioned above, my reports often have a few key sections:High Level OverviewKey NodesEgo Networks of Key NodesCommunities and Community ContextThat’s a good outline for any network analysis, but different techniques are used in each section. I’ll get away from the above simpler names and give more appropriate network analysis terminology. Here are some of the types of network analysis I do and describe:High Level EDA (Whole Network Analysis - WNA)Thorough Whole Network Analysis (Similar to above but much deeper context)Centrality Analysis / Key Node AnalysisEgocentric Network AnalysisCommunity / Clique / Group / Cluster AnalysisHub and Authority AnalysisBridge AnalysisCore and Corona AnalysisI am leaving off things like edge prediction or attack simulation. Those are things that can be done with network graphs, but they are not analysis. But the outputs can be analyzed.This is only scratching the surface. Use the arXiv data collector and start reading Network Science articles if you want to learn more about what researchers are researching. In this blog post, I am writing about what I personally use in my work and personal life. I write about Practical, Applied, Hands-on Network Analysis.Discussion of Analysis TypesLet’s discuss what each of these are, where I’ve demonstrated them, and what techniques or functionality is useful.High Level EDAIn this analysis, your goal are quick high level findings, to give yourself a map for deeper investigation. High level Network EDA is done using what is called Whole Network Analysis, shortened as WNA. In high level EDA, my goal is to quickly learn as much high level information as I can about the network. EDA gives us a map for deeper exploration and analysis. When I am doing this, I am looking for node and edge counts, key node identification (centralities), connected component counts and sizes, overall density, connected component densities. You can see examples of many of these in these posts.Thorough Whole Network AnalysisIn this analysis, your goal is to learn as much as you can about the structure of the entire network. This can be very time consuming, so set a cutoff date or goal, or this will be aimless and wasteful.With networks, we’re typically looking for something. To me, personally, WNA context is more trivia than useful. Like, what am I going to do with the knowledge that a 500,000 node network has an overall density of 0.00234353? It tells me that it is a sparse network, but not super helpful. What am I going to do with the knowledge that there are 28,698 triangles in the network?But you can spend days on WNA. There is always more to explore, and more insights to find, and good insights sometimes come from where we least expect it. It’s a great place to be, for learning about network science and network science research.Whole Network Analysis is the top down analysis of the entire network at once, completely zoomed out.Centrality Analysis / Key Node AnalysisIn this analysis, your goal is to identify the key nodes in a network. There are many different kinds of centrality algorithms, depending on the context you are after. Explore them all!Other than the moment of visualizing your first network, centrality analysis is probably the first memorable experience you will have in network analysis. Centrality analysis is about identifying the key nodes in any network. The graph is your input data, and you receive a list of important nodes, but you an also see how important.These are the Page Rank scores of the top twenty most important characters in Alice in Wonderland (according to Page Rank). It is clear that Alice is the main character.I work this into every network analysis I do, because there is value in understanding the key players in any network. I recommend that you learn as much as you can about different kinds of centralities.I actually already wrote a bit about centralities.This leads naturally to Egocentric Network Analysis.Egocentric Network AnalysisIn this analysis, your goal is to zoom in on nodes of interest. There are whole books about Egocentric Network Analysis. Egocentric Network Analysis is all about “zooming in” on the nodes in a network, by investigating their Ego Networks. I have written about this in my book and in this series, multiple times, and shown examples.This is Alice’s Ego Network, from Alice in Wonderland. These are the characters she is affiliated with, and you can even see who the alter nodes (others) are affiliated with as well. Keep this simple by thinking of it as zooming in on a node. Network Analysis feels a lot like using a microscope.Community / Clique / Group / Cluster AnalysisIn this analysis, your goal is to identify groups and understand what makes them different. Networks typically have multiple communities, groups, or clusters of some sort. You can find them by looking at connected components, as I’ve shown several times. These communities can act as their own tiny ecosystems, acting very differently than other parts of the network. Understanding their differences can be illuminating. Why is group A so different from group B? As you do network analysis, write down your questions. They will lead you to more discovery.Beyond investigating connected components, you should spend time learning about Community Detection Algorithms. So far, I like the Louvain Method and SCD (Scalable Community Detection) best, as they both supposedly scale to billion-scale networks.Hub and Authority AnalysisIn this analysis, your goal is to identify the key hubs in a network, or to identify the key authorities in a network.I haven’t written about it on my blog, yet, but I think I wrote about it in my book. I need to go back and check. But here is a very cool algorithm for identifying both the hubs and authorities of any network.Hubs: many outbound linksAuthorities: many inbound linksThink about the internet, or social media. News aggregators are hubs. A news aggregator will link to hundreds of websites. A popular website is an authority. Hundreds of websites will link to a popular website.On social media, a person who constantly shares new stories from many sources is a hub. A person with a million followers is an authority.There are different uses for this knowledge, depending on what you are doing. If you are doing internet research, then hubs and authorities is an important thing to analyze. It would also be useful in understanding any kind of flow (dataflow, information flow, influence, etc). Bridge AnalysisIn this analysis, your goal is to identify what is holding the network together.Networks are held together by bridge nodes. Let’s say that there are two communities: gamers, and anime fans. These are two different things, but some gamers love anime. Some gamers will be a bridge between these two communities. If you look at the two communities themselves, often their behavior will be subtly different than where the bridges form. I think about it like when paint mixes. When ideas collide, interesting things happen. Information isn’t a steady thing. Ideas compete to be included. This is influence.Bridge nodes are the glue of a network. They pull separate communities together, forming a larger ecosystem. Node importance algorithms such as betweenness centrality and Page Rank are useful for finding these bridge nodes, but networkx also has a nx.bridges(G) function that makes short work of identifying bridges.Seek to answer the question of who the key bridges are and what makes them special.On days 22 and 23, I show the impact of attacking bridge/key nodes in a network. If you attack bridge nodes, the network shatters into pieces, leaving only the most dense network structures remaining.Core and Corona AnalysisIn this analysis, your goal is to understand what is happening in different layers of the network. Network Analysis is compared to lots of things (using a microscope or telescope, zooming in, exploring the universe), but I think of peeling an onion when I think of core and corona analysis.Networks have layers. When I think of the layers of a network, I am thinking of k_core and k_corona. With k_core, I can investigate what the nucleus/core of a network is made of, to understand what is guiding the network. With k_corona, I can explore the layers, separately. K_corona(0) will give me all nodes with zero edges. K_corona(1) will give me all nodes with one edge, and any connections between them and other nodes with only one edge. K_corona(2) will give me all nodes with two edges and any connections between them and any other nodes with two edges. And so on. It is another way to look at the network, like peeling an onion.I personally don’t have a lot of use for this, but I often use k_core(G, 1) as a shortcut to quickly throw away all isolate nodes. That’s my main use. But I would love to explore this further and learn more.That’s All, Folks!There’s no code today, just reading. Take this time to think through what you have read and brainstorm the kinds of networks you’d like to explore and how you would use these different types of analysis to learn more.That’s all for today! Thanks for reading! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Attack Simulation', 'Community Detection', 'Egocentric Network Analysis', 'Graph Algorithms', 'Graph Visualization', 'Subgraphs and Connected Components', 'Whole Network Analysis']"
/work/blog_data/139652779.day-25-of-100daysofnetworks.html,"Today is going to be a fast post. Hopefully there won’t be too many typos. I have very little time today but wanted to do something that I’ve had on my mind for a while. If you have read my book or other network analysis books, you have probably heard of community detection. If you haven’t, I recommend you go back and read some of my earlier blog posts in this series.Today, I’m using community detection, doing more. I am building the first steps of a detection approach to detect denser, more interesting communities, not just star graphs. There are three steps to this:Identify the communities, using community detection algorithmsCapture community contextInvestigate communities of interestGet the CodeYou can get today’s code here. Follow along with the code to understand how I have done the above three steps. Step two is the new stuff. First Look at ResultsThe first thing I did after collecting community context was to draw a histogram of each community’s network density, the amount of connectivity in the community, how connected the nodes are to each other.This shows something interesting. There are some communities where everybody knows everyone else. That is the bar right above 1.0. The communities have a density of 1.0 because all nodes in the community are connected.But if you ignore than 1.0 line, you see interesting behavior. This shows that most communities had a density between 0 and 0.4, and fewer between 0.4 and 0.6, and fewer between 0.6 and 0.8, and still fewer between 0.8 and 1.0.Each one of these density slices can be investigated separately. If you know how to work with networks, you can have a lot of flexibility in your analysis. Today, I looked at communities with a density between 0.4 and 0.8 Here is how they look. Let’s look at a few.That’s neat. There are three groups in this community.Another cool three group community.Nice. This is closer to what I was looking for. I wanted interesting communities.And another interesting one.And another.Today, I just wanted to take a first attempt at this. This is a combination of data science and network science. The density stuff comes from network science, the Pandas work comes from data science, and everything else is software engineering.Being able to have total flexibility with networks is powerful. That’s all I will say, today. I have to run.That’s All, Folks!That’s all for today! Thanks for reading! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Community Detection', 'Graph Algorithms', 'Graph Visualization']"
/work/blog_data/139683911.day-26-of-100daysofnetworks.html,"Today, I continued the work I started yesterday. For a while, I’ve wanted better tooling to explore the different communities that can exist in one network. Community Detection is a well known thing in Social Network Analysis, but with a little Data Science and Software Engineering, you can do much more than DETECT communities and look at them. You can explore them, and you can explore what they are about.In my book, I often talk about the marriage of Natural Language Processing and Network Science being a powerful combination. Today’s post shows that, and I’ve shows it in previous posts as well. Graphs and DataFrames are useful side-by-side. This isn’t a one or the other thing. Use both.There’s a few things I want to point out at the start.I am using a network of 59,260 nodes and 302,588 edges. This is not a little toy network. This is a real-world network, of scientific collaboration on arXiv papers. This is not a toy. If you visualize a network of 59,260 nodes, you’re not going to see much of anything, especially with Python network visualization libraries. Gradually, there will be more and more powerful network visualization software, but I’m not holding my breath. If you do Whole Network Analysis (WNA) of 59,260 nodes, it’s going too high level to give you much of anything useful except a list of seemingly important nodes (Centralities and Page Rank).To chop a large network into pieces, you can use Community Detection to turn a network of 59,260 nodes into a few thousand tiny networks of a few dozen nodes. And each one can be analyzed and explored, separately, quickly, easily.You can also do more with the community subgraphs. In this post, I use the nodes to easily identify which papers are affiliated with each community.In short, Community Detection chops up networks into bite-sized pieces so you don’t have to eat a whole whale at once. Learn this.But the larger goal of the last two days has been to build a way to identify interesting communities. I didn’t want communities with a density of 1.0, as all nodes will be connected to all other nodes in the community. I wanted something a bit more sparse, but not as sparse as a star network. I wanted a tool to be able to explore different communities based on their characteristics of interest. I wanted to be able to play with density values and node counts to see how networks would visualize. So, I built a tool to do that, and I can take this as far as I want.Get the CodeYou can get today’s code here. Follow along with the code to understand how I have done all of this. Essentially, I:Use bipartite projection to build the author collaboration networkUse the Louvain Method for community detectionCapture context about each communityVisualize each community and show their affiliated papersHow’s It Look?It looks wonderful. But, before I get into that, I want to show you what you will spend the most time playing with.min_density = 0.1
max_density = 0.3
max_nodes = 100

communities = fetch_communities(community_context_df, min_density=min_density, max_density=max_density, max_nodes=max_nodes)I created a function called fetch_communities that takes inputs of min_density, max_density, and max_nodes. If you don’t set max_nodes, it’ll use the total number of nodes in the network as the maximum. Play with the numbers and see what happens! Build intuition! Build intuition! Build intuition!Before showing pretty pictures, I have a few quick insights, which make sense when you think about it:Below density of 0.1, you will see sparse communities of authors that do collaborate, but most of the papers have one or only a few authors. This means that there will be more papers, but less collaboration.Above this, you will see fewer papers, but more authors on them.At 1.0, you will see fully connected graphs, where every author is connected with every other author in the network. Likely, it’s one paper with a hundred authors, for instance.Now, let’s look at pretty pictures. Look at them closely. See what you see. Try to understand. Write down questions to explore. Get inspired. These are mesmerizing, but also a chance to learn and apply this learning in your own life.Today’s images also provide density values and node counts as additional context, to help you build intuition. I’ll post some of my favorite communities from the 0.2-0.4 density range.For each community, their papers can be explored:If you want to explore the papers, check out the code or run the notebook! I’ll show more networks.I think networks are beautiful. They are also a reflection of life. These are actual arXiv collaborations, not synthetic data. These are life networks. They show how humans interact to accomplish goals, such as writing a research paper.Network Science, Social Network Analysis, and Natural Language Processing give you the tools to explore reality. This is a network that I am interested in, as the theme of the dataset is Network Science. A Little Inception HumorI thought it was pretty funny while doing the analysis when I ran into this paper’s title.'Online-updated High-order Collaborative Networks for Single Image Deraining'I found a paper about collaborative networks while analyzing a large collaborative network. I don’t know that I will read that one, but I got a chuckle from it.Mission AccomplishedThe goal was to build a tool to explore the various communities, based on my own domain knowledge about networks. I built just that, and it was very easy. The more you know about Network Science and Data Science, the more flexibility you have. Your limitations are curiosity, imagination, and skill. I can help you with the last one.That’s All, Folks!That’s all for today! Thanks for reading! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Community Detection', 'Graph Algorithms', 'Graph Visualization', 'Subgraphs and Connected Components', 'Whole Network Analysis']"
/work/blog_data/139830889.day-27-of-100daysofnetworks.html,"I want to write this post tonight so that I’ll have it out of my mind and I can work on other things this weekend. For the last two posts, I’ve been working on a technique to explore different network communities based on density and node size. In other words, I built a tool that allows me to explore network communities based on where I think I’ll find interesting content. I made a few observations over the last three posts. I’m using arXiv data to create author collaborative networks. Here’s my impression, so far.Sparser communities are comprised of authors who have collaborated on papers written by a few authors, and large sparse communities thus have more variety in papers than large dense communities. Dense communities are comprised of authors who have collaborated on papers written by many authors, and large sparse communities might just contain one or two papers. There are arXiv papers with dozens of authors.In between, you get a blend.Personally, I felt that sparser communities had more interesting papers, and more of them, but that is my opinion. So, for today’s exercise, I explored network communities with a density between 0.1 and 0.3. I’m going to give a tour of those today.You can read the previous two posts here and here.As those two posts already describe the how, today will be showcasing results and discovered articles. There will be many articles linked to at the end, for your enjoyment.Communities and their PapersI use Natural Language Processing, Social Network Analysis (SNA), and Network Science together, not separately. As such, you’re going to see two things here:The community that wrote the papers (SNA and Network Science)The papers they wrote (Language, NLP)That is how I will show these. Let’s go!Social Network Analysis: this is a network community. It was identified using the Louvain Method for Community Detection.Network Science: several of the key nodes are colored differently, by their Page Rank values. You can see that Xun Liu and Stefan are central figures in this ecosystem as outer parts of the network are unreachable except through them. Other important nodes are also colored light blue.Natural Language Processing: these are the papers that came from this community. This is text data, and could be analyzed using NLP.['A Deep Neural Model Of Emotion Appraisal',
 'A Humanoid Social Agent Embodying Physical Assistance Enhances Motor Training Experience',
 'A Review of Critical Features and General Issues of Freely Available mHealth Apps For Dietary Assessment',
 'A User-Centred Framework for Explainable Artificial Intelligence in Human-Robot Interaction',
 'Assessing the Contribution of Semantic Congruency to Multisensory Integration and Conflict Resolution',
 'Can AI detect pain and express pain empathy? A review from emotion recognition and a human-centered AI perspective',
 'Focusing and directional beaming effects of airborne sound through a planar lens with zigzag slits',
 'Hierarchical principles of embodied reinforcement learning: A review',
 'Highly efficient anomalous refraction of airborne sound through ultrathin metasurfaces',
 'Improving interactive reinforcement learning: What makes a good teacher?',
 'Incorporating Rivalry in Reinforcement Learning for a Competitive Game',
 'Inference of Affordances and Active Motor Control in Simulated Agents',
 'Intelligent behavior depends on the ecological niche: Scaling up AI to human-like intelligence in socio-cultural environments',
 'Intelligent problem-solving as integrated hierarchical reinforcement learning',
 'Lifelong Learning from Event-based Data',
 'Observation of Acoustic Non-Hermitian Bloch Braids and Associated Topological Phase Transitions',
 'Orbital Deflection of Comets by Directed Energy',
 'Rethinking Continual Learning for Autonomous Agents and Robots',
 'Sequential Attention GAN for Interactive Image Editing']I’m only going to point out the three distinctions once. Try to learn how they relate. These are the papers this community wrote, and if these papers are of interest to you, then you might want to get to know these people.While doing this kind of exploration, I was keeping a side text file with articles I wanted to look closer at. At the end of this, I will paste in the articles so you can read them. I’ll show a few more communities, first.This is an excellent network for demonstration purposes. Can you identify the edge (line) that probably has the highest Edge Betweenness Centrality value? It is the edge that would split the network in two if cut. I think it’s obvious. You will learn to identify these things visually. Serge and Thomas are bridge nodes. They connect two communities together, via a single relationship. One relationship unlocks a world of potential to everyone in this network.I find this community visually very interesting, the way it is winding and has areas of differing density. If you were to use the Louvain Method one more time on this subgraph, you would be able to identify communities within the community, but you can also see them with your eyes. What kind of papers do these people write?['A Recipe for Geophysical Exploration of Enceladus',
 'Black hole and neutron star mergers in Galactic Nuclei: the role of triples',
 'Differing Enceladean ocean circulation and ice shell geometries driven by tidal heating in the ice versus the core',
 'Dynamics or Geysers and tracer transport over the south pole of Enceladus',
 'Ocean Worlds Exploration and the Search for Life',
 'Stability of exomoons around the Kepler transiting circumbinary planets',
 'When do star clusters become multiple star systems? II. Toward a half-life formalism with four bodies',
 'When does a star cluster become a multiple star system? I. Lifetimes of equal-mass small-N systems']Nice. These people would be fun to talk to.This one is actually fascinating. I don’t know if I’ve ever noticed one like this before. This is almost a star graph, but containing small dense communities connected to one individual in the middle. That one person is very important in this community.If you want to explore more, you can get the code here. There are thousands of communities to explore in this one dataset.What Did I Find?Today, I used the Artificial Life collaboration dataset to build this network. There are about 20,000 arXiv articles in the dataset. Rather than being overwhelmed with 20,000 articles, browsing by communities allows a few things:First, I’m not overwhelmed, trying to read 20,000 articles at the same time.Second, I’m not so overwhelmed that I just gave up, falling back to raw luck and random internet searches to find things.Third, it felt very approachable going slow, looking at the community, admiring the network, and slowly reading through the list that the groups managed to write.And because I didn’t feel overwhelmed and could explore by ecosystem, I could find useful information. I’ll link to my articles of interest below, for your enjoyment.That’s All Folks!That’s all for today! Thanks for reading! If you would like to learn more about networks and network analysis, please buy a copy of my book!This may get cutoff in email. Please visit on a browser to see the full list.Articles FoundCategory: cs.CL

Title: Find the Conversation Killers: a Predictive Study of Thread-ending Posts
Date Published: 2017-12-22 19:58:01+00:00
URL: http://arxiv.org/pdf/1712.08636v1

Title: A Deep Learning Approach for Multimodal Deception Detection
Date Published: 2018-03-01 12:38:13+00:00
URL: http://arxiv.org/pdf/1803.00344v1

Title: Causal Explanation Analysis on Social Media
Date Published: 2018-09-04 19:06:34+00:00
URL: http://arxiv.org/pdf/1809.01202v2

Title: Fine-Grained Emotion Classification of Chinese Microblogs Based on Graph Convolution Networks
Date Published: 2019-12-05 12:56:28+00:00
URL: http://arxiv.org/pdf/1912.02545v1

Title: Natural Backdoor Attack on Text Data
Date Published: 2020-06-29 16:40:14+00:00
URL: http://arxiv.org/pdf/2006.16176v4

Title: GraphTMT: Unsupervised Graph-based Topic Modeling from Video Transcripts
Date Published: 2021-05-04 12:48:17+00:00
URL: http://arxiv.org/pdf/2105.01466v4

Title: Thinking Fast and Slow in Large Language Models
Date Published: 2022-12-10 05:07:30+00:00
URL: http://arxiv.org/pdf/2212.05206v2

Title: Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods
Date Published: 2023-03-24 13:24:41+00:00
URL: http://arxiv.org/pdf/2303.13988v4

Title: Large Language Models for User Interest Journeys
Date Published: 2023-05-24 18:40:43+00:00
URL: http://arxiv.org/pdf/2305.15498v1

Title: Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language Models -- and Disappeared in GPT-4
Date Published: 2023-06-13 08:43:13+00:00
URL: http://arxiv.org/pdf/2306.07622v2

Category: cs.SI

Title: Cultures in Community Question Answering
Date Published: 2015-08-20 16:55:42+00:00
URL: http://arxiv.org/pdf/1508.05044v1

Title: On the Behaviour of Deviant Communities in Online Social Networks
Date Published: 2016-10-26 15:13:35+00:00
URL: http://arxiv.org/pdf/1610.08372v1

Title: The Remarkable Benefit of User-Level Aggregation for Lexical-based Population-Level Predictions
Date Published: 2018-08-29 01:33:21+00:00
URL: http://arxiv.org/pdf/1808.09600v1

Title: Fast Algorithms for Intimate-Core Group Search in Weighted Graphs
Date Published: 2019-08-30 15:28:54+00:00
URL: http://arxiv.org/pdf/1908.11788v1

Title: Mining Bursting Communities in Temporal Graphs
Date Published: 2019-11-07 07:25:13+00:00
URL: http://arxiv.org/pdf/1911.02780v1

Title: RP-DNN: A Tweet level propagation context based deep neural networks for early rumor detection in Social Media
Date Published: 2020-02-28 12:44:34+00:00
URL: http://arxiv.org/pdf/2002.12683v2

Title: A Large-scale Friend Suggestion Architecture
Date Published: 2022-12-24 16:42:11+00:00
URL: http://arxiv.org/pdf/2212.12773v1

Title: The Systemic Impact of Deplatforming on Social Media
Date Published: 2023-03-20 14:30:52+00:00
URL: http://arxiv.org/pdf/2303.11147v1

Title: Lady and the Tramp Nextdoor: Online Manifestations of Economic Inequalities in the Nextdoor Social Network
Date Published: 2023-04-11 14:05:05+00:00
URL: http://arxiv.org/pdf/2304.05232v2

Category: cs.LG

Title: Real-Time Bidding by Reinforcement Learning in Display Advertising
Date Published: 2017-01-10 09:30:29+00:00
URL: http://arxiv.org/pdf/1701.02490v2

Title: The Hanabi Challenge: A New Frontier for AI Research
Date Published: 2019-02-01 18:59:07+00:00
URL: http://arxiv.org/pdf/1902.00506v2

Title: SINGA-Easy: An Easy-to-Use Framework for MultiModal Analysis
Date Published: 2021-08-03 08:39:54+00:00
URL: http://arxiv.org/pdf/2108.02572v1

Title: AutoHEnsGNN: Winning Solution to AutoGraph Challenge for KDD Cup 2020
Date Published: 2021-11-25 07:23:44+00:00
URL: http://arxiv.org/pdf/2111.12952v1

Title: Large Graph Models: A Perspective
Date Published: 2023-08-28 12:17:51+00:00
URL: http://arxiv.org/pdf/2308.14522v1

Category: cs.CV

Title: Physical world assistive signals for deep neural network classifiers -- neither defense nor attack
Date Published: 2021-05-03 04:02:48+00:00
URL: http://arxiv.org/pdf/2105.00622v1

Title: SCB-dataset: A Dataset for Detecting Student Classroom Behavior
Date Published: 2023-04-05 15:02:30+00:00
URL: http://arxiv.org/pdf/2304.02488v1

Title: PRAT: PRofiling Adversarial aTtacks
Date Published: 2023-09-20 07:42:51+00:00
URL: http://arxiv.org/pdf/2309.11111v1

Title: SCB-Dataset3: A Benchmark for Detecting Student Classroom Behavior
Date Published: 2023-10-04 01:43:46+00:00
URL: http://arxiv.org/pdf/2310.02522v1

Category: cs.AI

Title: Ease-of-Teaching and Language Structure from Emergent Communication
Date Published: 2019-06-06 03:59:37+00:00
URL: http://arxiv.org/pdf/1906.02403v2

Title: Secure Artificial Intelligence of Things for Implicit Group Recommendations
Date Published: 2021-04-23 16:38:26+00:00
URL: http://arxiv.org/pdf/2104.11699v1

Title: Computational Metacognition
Date Published: 2022-01-30 17:34:53+00:00
URL: http://arxiv.org/pdf/2201.12885v1

Title: Towards Cognitive Bots: Architectural Research Challenges
Date Published: 2023-05-26 23:51:49+00:00
URL: http://arxiv.org/pdf/2305.17308v1

Category: cs.DB

Title: Capturing Topology in Graph Pattern Matching
Date Published: 2011-12-31 05:34:57+00:00
URL: http://arxiv.org/pdf/1201.0229v1

Title: Graph Pattern Matching for Dynamic Team Formation
Date Published: 2018-01-03 14:24:08+00:00
URL: http://arxiv.org/pdf/1801.01012v1

Title: Efficient Top-k Ego-Betweenness Search
Date Published: 2021-07-21 12:46:52+00:00
URL: http://arxiv.org/pdf/2107.10052v1

Category: hep-ph

Title: Wandering in Color-Space -- why the life of pentaquark is so long ? --
Date Published: 2004-08-04 13:29:07+00:00
URL: http://arxiv.org/pdf/hep-ph/0408056v4

Title: XENON1T Anomaly and its Implication for Decaying Warm Dark Matter
Date Published: 2020-06-22 15:46:01+00:00
URL: http://arxiv.org/pdf/2006.12348v1

Category: cs.CR

Title: Malware Detection using Artificial Bee Colony Algorithm
Date Published: 2020-12-01 21:32:09+00:00
URL: http://arxiv.org/pdf/2012.00845v1

Title: A Survey of Neural Trojan Attacks and Defenses in Deep Learning
Date Published: 2022-02-15 04:26:44+00:00
URL: http://arxiv.org/pdf/2202.07183v1

Category: physics.soc-ph

Title: The Twitter of Babel: Mapping World Languages through Microblogging Platforms
Date Published: 2012-12-20 20:43:12+00:00
URL: http://arxiv.org/pdf/1212.5238v1

Title: Entropy and the Predictability of Online Life
Date Published: 2013-12-01 01:34:09+00:00
URL: http://arxiv.org/pdf/1312.0169v2

Category: physics.pop-ph

Title: Funding the Search for Extraterrestrial Intelligence with a Lottery Bond
Date Published: 2013-11-11 15:40:35+00:00
URL: http://arxiv.org/pdf/1311.2467v2

Title: Why do we find ourselves around a yellow star instead of a red star?
Date Published: 2017-05-22 15:48:36+00:00
URL: http://arxiv.org/pdf/1705.07813v1

Category: cs.CY

Title: Mining the Social Media Data for a Bottom-Up Evaluation of Walkability
Date Published: 2017-12-12 14:31:13+00:00
URL: http://arxiv.org/pdf/1712.04309v1

Title: Uncovering Bias in Personal Informatics
Date Published: 2023-03-27 20:49:42+00:00
URL: http://arxiv.org/pdf/2303.15592v2

Category: astro-ph.EP

Title: Does the evolution of complex life depend on the stellar spectral energy distribution?
Date Published: 2019-05-17 15:58:07+00:00
URL: http://arxiv.org/pdf/1905.07343v1

Title: Did life originate from low-temperature areas of the Universe?
Date Published: 2020-10-21 11:26:17+00:00
URL: http://arxiv.org/pdf/2010.10905v2

Category: cs.IR

Title: A Survey on Personality-Aware Recommendation Systems
Date Published: 2021-01-28 18:03:23+00:00
URL: http://arxiv.org/pdf/2101.12153v2

Title: SceneRec: Scene-Based Graph Neural Networks for Recommender Systems
Date Published: 2021-02-12 09:06:12+00:00
URL: http://arxiv.org/pdf/2102.06401v1

Category: cs.SE

Title: Can Requirements Engineering Support Explainable Artificial Intelligence? Towards a User-Centric Approach for Explainability Requirements
Date Published: 2022-06-03 11:17:41+00:00
URL: http://arxiv.org/pdf/2206.01507v1

Category: cs.FL

Title: What can we learn from universal Turing machines?
Date Published: 2021-10-16 08:43:29+00:00
URL: http://arxiv.org/pdf/2110.08511v1

Category: cs.SD

Title: MES-P: an Emotional Tonal Speech Dataset in Mandarin Chinese with Distal and Proximal Labels
Date Published: 2018-08-30 03:02:46+00:00
URL: http://arxiv.org/pdf/1808.10095v2

Category: q-bio.QM

Title: Local Causal Structure Learning and its Discovery Between Type 2 Diabetes and Bone Mineral Density
Date Published: 2020-06-27 08:27:00+00:00
URL: http://arxiv.org/pdf/2006.16791v1

Category: math.OC

Title: G-networks and the optimization of supply chains
Date Published: 2019-03-17 22:57:22+00:00
URL: http://arxiv.org/pdf/1903.10691v1

Category: stat.ME

Title: Outlyingness: why do outliers lie out?
Date Published: 2017-08-12 10:48:58+00:00
URL: http://arxiv.org/pdf/1708.03761v1

Category: astro-ph.GA

Title: Four hot DOGs in the microwave
Date Published: 2015-10-14 16:02:10+00:00
URL: http://arxiv.org/pdf/1510.04179v1

Category: cs.HC

Title: BEAMERS: Brain-Engaged, Active Music-based Emotion Regulation System
Date Published: 2022-11-26 16:37:13+00:00
URL: http://arxiv.org/pdf/2211.14609v1","['Artificial Intelligence and Machine Learning', 'Artificial Life', 'Community Detection', 'Egocentric Network Analysis', 'Graph Algorithms', 'Graph Visualization', 'Subgraphs and Connected Components', 'Temporal Network Analysis', 'Whole Network Analysis']"
/work/blog_data/139847472.day-28-of-100daysofnetworks.html,"Hello everyone. The last several posts have been quite technical, and life has been very busy, so today I want to just pause and reflect. I want to do a few things: I want to think about what we have covered and what we will cover in the future. I want to write about how I feel about the progress of this blog and my attempts to popularize Network Science. I want to talk about some of the nice improvements I’ll be doing in coming days. And finally, I want to get your thoughts on Influence Theory swag.I’ll Start with SwagI’m going to start at the end and do those out of order. I don’t spend much time thinking about swag (t-shirts, stickers, etc), but swag is my favorite part about going to conferences. However, there’s a few things to know about me:I have no intention to make this into a business. I already have a company, and that is my top priority. This is a very, very distant priority, and more of a creative outlet for me. This is art, for me.I don’t need money. I wrote the book with no intention of making money. I wanted to fill a knowledge gap, and that’s the same reason for this blog. I want to write about something useful that is not getting the attention it deserves. None of this is for money. I am utterly apathetic about getting money out of this.I don’t have a lot of free time. I use some of my weekend free time on this, just because it is a creative outlet to me. Playing with networks and improving my skill is useful to me. This is really win win, and it is equally, selfishly for my own good. I am an extremely creative person, and this is a necessary creative outlet for me.With that in mind, I do still sometimes think swag would be cool. My own substack is named “Influence Theory” and I really like that. I occasionally also come up with goofy ideas for tech shirts, like Failure Driven Development instead of Test Driven Development. Is there any interest in swag? I know I’d wear Influence Theory stuff everywhere, so maybe I’ll just do it for me unless others are interested.Ok, moving on.Where Have We Been?On day 1, I discussed many of my ideas for the things I would demonstrate. Looking at the list, we’ve covered everything. We’ve at least scratched the surface. In my opinion, we are well past the basics and are doing interesting things.I have created new techniques for both Temporal Network Analysis as well as Community Detection. New techniques lead to new ideas, and I am going to take both of these further, as far as I can go in 100 days. And if this is anything like the first iteration of #100daysofnetworks, it will probably go forever.I have also shown how you can use Wikipedia, arXiv, and Spotify to create your own interesting networks that are useful to you, not just how to download .csv files from Kaggle or other open data repositories. I’ve shown you how to chase curiosity and study what you find.And I’ve shown you how to do storytelling after network analysis. You can use me as an example when you do your own network analysis and have to report your findings to those in your organization.Where Are We Going?We’re going to get network data from more internet sources, such as Youtube, just like was done with Spotify, arXiv, and Wikipedia. The internet is an amazing dataset if you learn how to use it as such, and you can learn from it. I’m sure I wont stop with Youtube. There are other sources we can find. I show this to show you how you can study the things you are interested in. The source matters less.I am also planning to get really experimental with Temporal Network Analysis. I think I will do that next, as the last three posts have been about Community Detection. I have a book that I am really wanting to dive into about Time Series Analysis, and I’m going to fuse Time Series Analysis with Network Science and show you how. Temporal Analysis is Time Series Analysis of Networks.I plan to do more experimentation with both Community Detection and Egocentric Network Analysis, as well, and still have a dream to create an approach for Exploratory Data Analysis (EDA) of graphs. What do you want to learn about? Post in the comments or let me know on LinkedIn. You all are really quiet, but I enjoy the enthusiasm when I talk to readers one-on-one. Some of you are really understanding the opportunities this exploration presents.How Do I Feel?I restarted this challenge for a few reasons. First, I really needed a creative outlet. I wrote a book last year, and I want to keep that going forever. This series helps me have the discipline to practice and build techniques that I want to go into the next edition of my book. Second, I wanted to popularize this a bit, but with the knowledge that this will never be anywhere near as popular as the hype of the day. I don’t mind that at all. The intention has never been popularity. I wanted steady growth, and I wanted to impact a few people’s lives, and give the ability to do a little of what I can do.So, when I look at the stats, and I see that there is steady growth with subscribers, that makes me happy. I wanted to popularize this topic a bit, and I’m doing it. If I compare myself for blogs about LLMs or ChatGPT, I’d feel pathetic about having 200 followers, but my goal isn’t to add more hype to hype. My goal is to teach something incredibly useful that is incredibly overlooked.Likewise, when I look at daily stats and it seems that there is only a little more interest today than a week ago, that doesn’t bother me. I don’t care if this becomes wildly popular. I want to help a few people, and a few people can change the world. Network thinking has already changed the world (Google) and continues to change the world. So, I just smile and nod, even if growth is slow. This is a marathon, not a sprint. Influence takes time.How Is This Useful to Me?As I have said before, these approaches give me the ability to explore my reality. These techniques help me get to the root of problems. These techniques help me understand how and why things are happening. And these techniques help me cut through noise and find value.For instance, the last few days posts have been an exploration of an Artificial Life dataset, that also contains interesting Artificial Intelligence content. Above is one of thousands of communities in the dataset. When I find a community that writes interesting content, I “follow” them, like you would on social media. I look into what other content they produce that can be useful to me.I found these research papers while doing the analysis that went into yesterday’s post, and I have been reading them. These are the kinds of papers I enjoy, not yet another paper on the same thing others have written. Conversation Killers, Deception Detection, Malware Detection using Artificial Life, Natural Backdoors in NLP Models, and so on. Wicked cool!These skills help me find things. These skills help me learn about the content, but also about the ecosystem (the authors and communities) that produce the content.That’s how it has been useful to me in just the last 24 hours. I have already written other posts about how this has been useful to me, in this series.I Feel Very Good and ThankfulHaving said all that, I feel very good about everything. This is the creative outlet I needed, I have enhanced a few people’s lives with my book and these posts, and I am learning new things. I don’t mind the slow growth. I expected it. I knew that my book would be the same. I don’t need or want to be a celebrity scientist. I just want to teach stuff that is useful.Thank you all who have followed so far. This is only going to get better over time. Oh, and hey, I’ve got some books to give away. I’ll try to do another giveaway soon.That’s All, Folks!That’s all for today! Thanks for reading! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Artificial Intelligence and Machine Learning', 'Artificial Life', 'Community Detection', 'Egocentric Network Analysis', 'Temporal Network Analysis']"
/work/blog_data/139872124.day-29-of-100daysofnetworks.html,"Hi everybody. Over the last three days, I’ve written about how to identify dense communities in networks. While investigating some of these communities, I ran into a paper with an interesting title.Mining Bursting Communities in Temporal GraphsToday’s post has two parts. I describe the paper, and I show my own personal implementation I came up with on the spot, to experiment with the idea of the paper.Let me break this down a bit.Mining: identifying and extracting useful contextBursting Communities: communities pop in and out of existenceTemporal Graphs: network graphs of a period of timePutting this together, the authors are attempting to identify and extract useful context about communities that pop into existence at a moment or period in time. Why would that happen?In their paper, they discuss a few reasons why this might happen:Human communication events occur in a short timeA bursty pattern can show multiple events happening at the same timeThese events could represent emerging activity or behaviorThis can help detect emergency events, that something went very badBut emergent behavior can also show something very good happening. Maybe a new idea has been created and suddenly everyone is writing research papers on the subject. Or maybe a free concert is happening at the park.I recommend reading their paper, as they have an interesting solution using a mix of software engineering and multiple different methods of detection. I was going to read the paper and call it a day, but that’s not how my mind works, so I thought I’d use some of my previous work to show some of these bursting communities.But before I move on, the paper did accidentally highlight something: Network Science researchers are still using stale well-known datasets to identify phenomenon, which makes sense in some cases. However, this phenomenon can easily be shown with any of the #100daysofnetworks datasets that contain date fields. With that, you can create a temporal graph. With a temporal graph, you will almost certainly run into some kind of bursting behavior. You should study using interesting data that you are curios about, not just what is on hand, if you can. I show you how.So, for today’s example, I’m using yesterday’s Artificial Life arXiv dataset that I compiled. That’s much more interesting than an Enron dataset from 1999, in my opinion.Show and Tell Time!First, get the code and follow along. You will want to pay attention and go very slow if you are new to networks. If you are new, think more about the possibilities, and revisit earlier days in this series. This is very advanced. I am pushing my own abilities with this one.After loading the Artificial Life dataset, I do things a little differently than before. I use a Python dictionary for G to store each “time slice”, as I call them. I then took the enrichment function from Day 13 and made some useful modifications and additions.With the enrichment, each time slice looks something like this:{'graph': <networkx.classes.graph.Graph at 0x25279d5c5c8>,
 'dropped_edges': [('Junji Suzuki', 'Kunihiko Kaneko'),
  ('S. T. Petcov', 'A. Yu. Smirnov')],
 'added_edges': [('B. B. Levchenko', 'C. Riccardi'),
  ('B. B. Levchenko', 'G. Boca'),
  ... lots more ...
  ('V. Arena', 'V. A. Nechitailo')],
 'dropped_nodes': ['A. Yu. Smirnov',
  'Junji Suzuki',
  'Kunihiko Kaneko',
  'S. T. Petcov'],
 'dropped_node_count': 4,
 'added_nodes': ['B. B. Levchenko',
  ... lots more ...
  'V. Arena'],
 'added_node_count': 13,
 'graph_dropped': <networkx.classes.graph.Graph at 0x25203b0e788>,
 'graph_added': <networkx.classes.graph.Graph at 0x25203b0ecc8>,
 'density_change': 0.6666666666666667,
 'degree_change': 9,
 'edge_change': 76,
 'graph_core': <networkx.classes.graph.Graph at 0x25203b28348>,
 'graph_core_node_count': 13,
 'graph_core_edge_count': 78}That is ONE time slice. There are many. This little window in time shows that there have been various changes in the network. I love this little enrichment function that has come from this series. It is so useful.And with that, each time slice can be mined for bursting communities.I didn’t want to copy the paper. Their techniques are interesting, but so are mine.Approach 1: Added Node CountsThis idea is simple enough. If suddenly, in a moment of time, 1000 people show up in one place, that might be worth looking into. However, that doesn’t always show an emergency. For instance, in this dataset, the Artificial Intelligence ecosystem is always noisy. To find bursts, I have to set a high threshold, to look for unusual bursts of activity. You can see the code to understand how I played with this idea.This identified a big burst of activity at a given time.And since I captured the core during the enrichment step, I can look at that too.Ok, cool. this tells me something. A big group of people wrote a paper and everyone put their name on it. That’s definitely a burst, but is it unusual? That’s a question for another day. That’s a question about understanding the ecosystem itself, which is a higher level question, and kind of intriguing.Approach 2: K_core Node CountsThis one is a bit inspired by the paper. To do this, I captured the core of each network time slice, and then counted the nodes in the core. Check my code for implementation details.With a little code, we can identify when large clusters show up in a network.threshold = 50result_df = result_df[result_df['graph_core_node_count']>threshold]title = 'Graph Core Node Count by YEARMONTH'_= result_df['graph_core_node_count'].plot.bar(figsize=(16, 8), title=title)These are the results. I’ve filtered to time slices that have 50 or more nodes in the core at each given moment of time. Each of these can also be investigated and explored.Approach 3: Density IncreaseThere are so many ways this idea can be approached. After reading this, after getting to known networks a bit better, think about how you would approach this. How would you identify bursts of activity in a complex network, using graph data?I used a very simple approach, just for this exercise. This is not finished code. This is a first attempt.threshold = .1result_df = result_df[result_df['density_change']>threshold]title = 'Density Change by YEARMONTH'_= result_df['density_change'].plot.bar(figsize=(16, 8), title=title)This is actually very interesting. Look all the way to the right. What do you notice? WHAT IN THE WORLD? Why is there nothing after February 2014? Is this a bug? I started getting nervous when I saw that. So, I looked closer. These are all the months where network density increased by more than 0.1. Did I accidentally leave in a filter while debugging? What did I do wrong?202204    -0.00183
202205    0.034756
202206   -0.034125
202207   -0.000088
202208   -0.002865
202209    0.007511
202210   -0.001453
202211   -0.007569
202212     0.00016
202301    0.000163
202302    0.001126
202303    0.005355
202304   -0.006539
202305   -0.001437
202306    0.004352
202307   -0.001395
202308     0.01193
202309   -0.012969
202310   -0.001433Nothing. It turns out that the Artificial Intelligence and Artificial Life ecosystem is so dense that network density increases and decreases by a little bit every month.So, everything above a 0.1 threshold actually was a significant burst. The network became denser during those times than more recent times. That’s a cool insight! What does it mean?! Has anybody studied this or written about this phenomenon? Cool!When investigating one of these bursts, I saw this. Notice the two dense circles. Those have increased the overall density of the graph at that moment in time. Two papers were written with many authors involved, and they know each other. If those two papers hadn’t been written, density would be very low. Hey, that’s a burst!So, this approach will even work when networks are just starting to develop, because initially, there are often large changes to density. Three people who known each other will appear as a subgraph with 1.0 density, for instance.A Few More ThoughtsThis is a really cool paper. Research papers give possible approaches, not a dogmatic set of rules. I was inspired by their paper and used that I had previously built and was able to identify bursting communities in only a few hours. Work builds on work builds on work builds on work. The biggest contribution today came from Day 13, as it took very little to take inspiration from their idea and implement it in my code. The enrichment function now has some new context to explore!  I also really liked the idea of “Average Separability (AS)”, though I would give it a different name. The way I understand it, it is essentially:AS = Community Density / Overall DensityI think that’ll be a fun idea to explore on another day. That seems useful, to me.The authors also mention something called DENSEST that I’d like to take a look at and see if it can be useful, more useful than my own approaches.Alright, that’s enough. I was going to just read a research paper and relax, but I couldn’t resist doing all of this. When I get inspired, there’s no breaks.That’s All, Folks!That’s all for today! Thanks for reading! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Artificial Intelligence and Machine Learning', 'Artificial Life', 'Community Detection', 'Graph Algorithms', 'Graph Visualization', 'Subgraphs and Connected Components', 'Temporal Network Analysis', 'Whole Network Analysis']"
/work/blog_data/142922071.day-30-of-100daysofnetworks.html,"Hello everyone! Thanks for your patience as I took a short break from #100daysofnetworks so that I could focus on other things for a bit. I’m going to attempt to pick this up again, on weekends, because it is really win-win:I get to show you all kinds of cool and useful things you can do with graphsAnd I get to have a nice creative outlet, which keeps me happy and healthyI want to get this series going again, but I’m not ready to jump straight back into the weeds, doing temporal network analysis, analyzing entire arXiv ecosystems, etc. Let’s do something a bit more creative and simple.This morning, I woke up with a song in my head. This is not uncommon, for me. I woke up with “Somewhere Only We Know” on loop in my head. I don’t know why. Maybe I heard it on a Youtube video recently, or something.I’ve really liked this song for a while. It has a really timeless feel, and I think it’s going to be important for a long time. I like this song so much, I picked up my guitar and worked on learning it. And then, I thought, “I should show my readers how to create word networks, and I should try creating a chord network.”I have never created a chord network before, but it is a simplification of a word network. Chords carry information. Chords are almost like words. A ‘C’ chord has a different implementation and sound than a ‘C#m’. One of my favorite ideas is the idea of Computational Humanities, analyzing text and images programmatically (and more). By now, you should realize that I dabble in this domain quite a bit, as this series has talked about music from the beginning. Music is important to me, as is literature.So, today, I’m going to do something creative. Create a word graph using the lyrics of this song.Create a chord graph using the chords of this song.I’m not doing this because it is incredibly valuable, because it is going to change the world, because it’s going to make me a million dollars. I’m doing this, because it gives me an opportunity to understand a song that I like a little more, and possibly I’ll get a cool learning tool out of this, for creative exploration while playing guitar.Get the CodeToday’s code has been written. You can get it here: https://github.com/itsgorain/100daysofnetworks/blob/main/day_30_song_word_network.ipynbCreating Word Graphs (Lyrics and Chords)The process of converting any text into a network graph is pretty simple:Get the text into a variable that you can useTokenize the textCreate an edgelist using: current_token > next_tokenCreate a graph using the edgelistDo whatever you want with the graphJust follow along with the code and this will be simple and logical to understand. Get text, split the text up into words, and then find the relationship between words. With practice, this becomes very easy.Word Graph (Lyrics)The work is shown in the notebook. I will show the results and talk through them. What does the network look like?As you can see, it is a bit complicated and not super easy to read, but if you know the song, you should be able to understand the directionality. For instance, if you find the word ‘why’, you can follow the lines: why→don’t→we→go→somewhere→only→we→knowRead that. What do you notice? See any repetition? The word ‘we’ has a forked path:we→gowe→knowIn this song, these lyrics carry the rules and information. But even in this little pocket universe, we can learn a bit about the English language. But also notice the shape and structure of the song, and then consider simpler songs that you know. When this graph visualized, I was first impressed by the complexity of this song.As this is a graph, everything that I have covered in this series can be useful.Centralities can be useful to show the most important words, in various context.Communities can be useful to show which words go togetherTemporal analysis can be used to show how a song progressesShortest paths would be interesting to look throughSimple algorithms can be used to create other interesting lyrical alternatives, given the same rulesImagination and time are your limitationsI want you to look at this and be creatively inspired. Work problems can also be solved with creativity. Not everything is tabular data, and many problems aren’t approached (or weren’t approached previously) because NLP techniques were not widely known. Hopefully, there is less of that these days, as NLP and network analysis are not that difficult to learn. It’s just practice, practice, practice, like anything else.And also, if you are learning something, we learn better when we study what interests or is helpful to us. So, throughout this series, I’ve shown how to use Spotify, Wikipedia, arXiv, and more, to help you get the data to become creatively inspired and practice. I’m not giving you stale boring astronomy datasets that I found in random repos from the internet. The lyric network is really interesting, and I could spend some time with it, exploring other possibilities. Maybe I will do more with this.Chord Graph (Chords)How does the chord graph look? Follow along with the code to learn how I built it.As a guitarist and musician, this is really cool. This shows how the chords of the song relate to each other. Without even knowing anything about this song, this network visualization could be useful in learning new chords, practicing chord progression, and coming up with countless ideas for new songs. Notice a few things:I used a directed graph, today, to show the directionality of the relationships.There are nodes that do NOT have edges between them. Not all chords are used together. But this is limited to this one song. So, this is already a fun thing that I can use today while I’m practicing guitar.And this is enough to algorithmically create the foundation of songs. You don’t need a fancy AI for that. Songs tend to have a simple enough structure, so that we can relate to them. Lyrics and lead guitar get much more complicated, but chords and bassline hold the foundation of any song. The above visualization is the foundation for this song. The musicians hit other notes in their individual performances, but this is the scaffolding.Have Some AI ArtI asked ChatGPT (GPT 4.0) to draw a few happy AI learning to play music. Enjoy.Go Learn Guitar!Now that you know this, if you play guitar, maybe you’ll be able to use this for your own creative exploration of music. Or maybe you don’t play guitar, and you realize that the chord graph is not all that intimidating. It really is not. You could use the chord graph for practice, this week:Session 1: practice C and GSession 2: C, G, Dm, EmSession 3: C → C/B transition: what is this and what does it sound like?Session 4: Add on moreEtc. When I first started learning guitar, I was showed the C chord and then left alone. This network visualization is much more than I had on day 1, and would be useful for beginners.In fact, as another idea, chord networks of songs would be a neat learning tool. I’d love to have a whole card deck of these. It’d be a really neat idea for a computational humanities book, showing the networks of various songs. Ideas are endless.That’s All for TodayThanks for reading, today. Today is light reading. I just wanted to be creative and have a bit of fun exploring a song that I like. You can use the notebook and swap in a song that you like and do your own analysis. Have fun with it! It’s always fun, and very good learning and practice, which will help you in your data work.Thanks for reading! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Artificial Intelligence and Machine Learning', 'Community Detection', 'Graph Algorithms', 'Graph Visualization', 'Temporal Network Analysis', 'Whole Network Analysis']"
/work/blog_data/143557855.day-31-of-100daysofnetworks.html,"Hi everyone. I am so excited to write today’s post. I’m announcing something I should have done a long time ago on this blog. But first!Evolution of #100daysofnetworksThe very first #100days project I did was actually #100daysofnlp. Back then, ChatGPT did not exist, and people seemed more interested in Computer Vision than Natural Language Processing, so I created the project to learn more about NLP and to popularize it as well.And my #100days project ideas came from one person on LinkedIn having her own #30daysof___ something. I don’t remember what it was anymore, but I felt that the amount of time wouldn’t be enough for me to learn about NLP, so I created #100daysofnlp. While working on that project, I was already obsessed with networks, and networks are prevalent where language is found. The two go together. So, the next project naturally became #100daysofnetworks.The original #100daysofnetworks was done entirely on LinkedIn, which was not a good idea. A hashtag is not enough to organize information, and posts eventually fell off of LinkedIn. But it was a useful adventure for building interest in network analysis, and it led to a book deal and me creating a company.I wanted a way to have “pages”, for organizing other resources, such as datasets, web links, favorite books, etc. So, last year, when I launched this iteration of #100daysofnetworks, I originally did it on Blogspot. I had an ulterior motive for that, but Blogspot just wasn’t going to work out. It’s too old of a platform and there are much better alternatives. I finally settled on Substack.Since moving my blog to Substack, I have been just writing and writing and writing, happy as a clam. But I realized that I’m missing something. I totally forgot to create a page listing my favorite books that I learned from and/or was inspired by.Books Section Added!It’s great that Substack emails our articles to our readers, but it is useful to visit this blog itself, as there are several different pages already created. Today, I added a new page for books. Here is what it looks like if you visit the blog, rather than read the email:There are a few different sections:Day Catalog: easier to read list of all the days (I need to update this)Datasets: places to get network data to play with and learn fromBook Giveaways!: sometimes I give my book awayBooks: THE NEW SECTION! Books I learned from and/or was inspired byI will be improving the Day Catalog at a later day, and breaking down days by the concepts taught in each, to make this a better learning resource.What’s In the Book Section?You can get to the book section here.I’ve broken down the book section into two parts:Books I own that I’ve learned from and/or was inspired byBooks I own that I haven’t read enough to have much of an opinion but are goodBoth sections are full of good books, but I wanted to separate them this way. The first section shows the books that actually helped me in my learning adventure. The second section includes books that I have read and learned a bit from, but got distracted and moved on to other things. I need to spend more time reading them. All books listed have my blessing. I won’t recommend books that confuse more than help. Where Should You Start?It doesn’t matter what you learn, you need to build a learning path or you are just swinging in the dark. Repetition builds skill, knowledge, and eventually wisdom. But if you’ve made it this far, then you have probably already given some thought into how you will learn network analysis and who you will learn from.But when I want to learn any new topic, I do this:Have a specific topic in mind (Natural Language Processing, Network Analysis, etc)Find several authors who have written about the topic. Find several books.Pick a few that look the best.Read and apply, from each one.This will get you familiar with a topic. After completing that plan, come up with the next plan to dig deeper. Where Did I Start?My path into Network Science started with a class on Coursera. I had no idea what Social Network Analysis was when I enrolled in the class. That was a life-changing class, and I learned so much and became obsessed.Next, I read the book Linked, by Albert-László Barabási. This is a non-technical book explaining how Network Science became a thing, and how it is valuable. This is easy reading and very relaxing. It is a fun read! Then I got his actual textbook “Network Science”, and I still pick at that book all the time. There is so much to learn.Then I read Complex Network Analysis by Dmitry Zinoviev, and his book made this approachable by me, as a programmer. This bridged the gap from the social sciences and mathematics back to coding, my happy place.Then I read the books about Social Network Analysis and Egocentric Network Analysis. These books introduced me to useful techniques for actually dissecting networks and investigating the parts, exploration. Graphs become much easier to work with once you lean to analyze them in pieces.And all the while, I was reading and applying my learning to my own work, finding ways that these skills could help me in my work. Learning Platform, Not TeacherMy goal for #100daysofnetworks is for it to become a learning platform, not for me to be your teacher. I enjoy sharing useful knowledge, but I am not doing this to teach or to make money, I am doing this to share and hopefully inspire. This is useful to me, and I think it can be useful to you, if you learn and do the work.But my goal is not to become your teacher, or to become any kind of name in this field. I wrote a book and crossed that off my bucket list. I will write more. But I do it because I enjoy the act of writing, and I enjoy sharing knowledge, and because I love that sudden spark that people have when they suddenly “get it”. I want you to learn, but I don’t care if you learn from me, from my book, from others, or from their books. This is important and useful skill, and I want to get the word out. But I am happy to hear from people who have read my book. I’m glad I managed to write a book.Now I am Unstuck!I’ve actually been a little bit stuck on this blog for a bit. I have been in a reading mood, lately, and when I am in a reading mood, I am busy learning, not focused on teaching. Learning takes time. I’ve been going back to fundamentals, reading Network Science, revisiting concepts. I haven’t been experimenting as much as I have been just reading and thinking. I’ve had more to think about and less to write about.But now, on some days, I might just write about what I’m reading, instead of making a post explaining a concept. I, myself, will use the new books section to remind myself of what books I need to revisit (the second section).Reading is fun, and talking about what I am reading is also fun.That’s All for TodayThanks for reading. People often ask me what books I recommend, so I’m happy to have this added. I will be sure to maintain it. Please do recommend your favorite Network Analysis in the comments sections.Thanks to everyone who has been following along with this series. Happy learning! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Artificial Intelligence and Machine Learning', 'Egocentric Network Analysis']"
/work/blog_data/144082933.day-32-of-100daysofnetworks.html,"Today, I wanted to write about how learning to analyze network graphs will open the door to learning about other topics that may interest you. Specifically, today, I want to discuss the following:Geospatial AnalysisCausal InferenceKnowledge Graphs and Artificial IntelligenceData Flow Mapping and Source Code AnalysisCybersecurityWhen I wrote my book, I was hoping that it would be useful in bridging the gap between the social sciences and software engineering and data science. These techniques that I have described are an overlap of different domains, they are not common knowledge. We are making use of learning from across domains. My goal in writing my book was to help people understand how this skillset can be useful in multiple domains.But not everyone gets excited about Network Science by itself. They may be interested in other things, such as Geospatial Analysis, or Causal Inference, or Artificial Intelligence, or in using Knowledge Graphs, or maybe they would rather pursue a career in Cybersecurity. Or maybe they are just generally interested in Data Science or Software Engineering.I come from cybersecurity and software engineering. I show how to create Knowledge Graphs. I create Artificial Intelligence. I am learning Causal inference and Geospatial Analysis. What do they all have in common? Graphs. If you are interested in any of these topics, you will inevitably learn about networks and graphs. So, if you would like to become stronger in any of these, you should learn about Network Science and Network Analysis. It will only make you better and give you more useful capabilities. In this post, I will talk about some of this overlap, and recommend books that can help.My Path: Data Operations and CybersecurityI come from cybersecurity. If you are in cybersecurity, I wrote my book for you and you will find it relevant. I am one of you. If you are in OSINT, you will also find it very useful, no doubt.I have already written about this in other posts, but this obsession really started when I first learned relational database design in the 1990s, I just didn’t know it. I really got into designing databases, and in mapping existing production databases into Entity Relationship Diagrams (ERD, or ER Diagrams). These diagrams showed how the tables in a relational database linked together by primary and foreign keys.Later, I created my own framework for profiling servers, work that would be needed in order to successfully “uplift” very old, undocumented, orphaned production servers. If there’s no documentation or owner, how do you find out what a server does? I created a process for that. Maybe I will write more about it someday. Are you interested in that? Leave a comment if it sounds useful to you.After coming up with a methodology for mapping out how production servers worked, I took this further, mapping out production dataflows across entire datacenters. This helped the companies (Intel and McAfee) in a big way and we were able to speed up parts of the uplift process by 10x, no exaggeration. This led to many safe and very boring uplifts, the best kind. We had zero failures or complications. Zero.Later, I was pulled onto a Data Science team, and I shifted my thinking away from Data Operations and more towards Malware Classification. At one point, I mapped out the evolution of malware using these techniques.If you are in cybersecurity and you haven’t read my book, you really should. I wrote it especially for you. It is a natural fit. You defend computer networks from malware networks that are created by dark networks of criminals and adversaries. Get it?Geospatial AnalysisSince those days, I left and created my own company. I now map out the entire internet of billions of websites, in every single language that exists. If you want to know more, you can follow my company on LinkedIn. We do really cool and important work.I now do a lot of OSINT, which stands for Open Source Intelligence. If you are doing OSINT, then Geospatial Analysis will inevitably capture your attention.There is overlap between networks, graphs, and Geospatial Analysis. If you want to get from your home to some place thirty minutes, what’s the fastest route? What will you travel on? You will travel on a transportation network (roads, sidewalks, rail, etc), and the fastest route is the shortest path. Roads follow a network.So, if you are interested in Geospatial Analysis, you will very likely inevitably learn about networks and graphs, and things like Shortest Paths and Betweenness Centrality will be useful in your work.I am just getting started in learning about Geospatial Analysis and have been pleased with the overlap between what I already know from network analysis and this domain. Here are some books that I’ve found useful:Python for Geospatial Data AnalysisApplied Geospatial Data Science with PythonI am working through the first one, lately, and will include some of my learning in this series. I enjoyed reading the second book earlier this year and am going to jump back into it after reading the first one. Have books you enjoy and recommend? Please post a comment with the title and author so that others can find it.Causal InferenceCausal Inference is a cool topic, and one that I hope to learn a lot more about and build my skills. I am only beginning, but really enjoying it.In my earlier work in data operations, anytime anything important would break, the team would be sent to do a “root cause analysis” of why it broke. If you don’t know what “root cause analysis” is, the goal is to understand why the thing broke. What caused the thing to break. Often, it is not one thing but a cascading failure. In order to understand why the thing broke, you have to understand the things that impact it. In network thinking, what is upstream. Which of those upstream things caused the thing to break. You begin to think a lot in terms of upstream and downstream.Already, in your mind, you should be thinking of “the thing” as a thing, a dot, and the things that affect it as other dots, with arrows pointing at the thing. One of those dots is the culprit. Which one?Causal Inference will help you figure it out. In causal inference, you will make use of causal graphs. Being able to visualize them and inspect them visually is very useful in understanding and figuring things out.Here are two books I have read that I recommend:Causal Inference and Discovery in PythonCausal Inference in PythonThe top one is my favorite. It is a total page turner, and I sat outside for several hours reading it the first day I got it. Do you have books you recommend? Add them in the comments so that others can learn from you.Knowledge Graphs and AIThese days, there is so much talk about Artificial Intelligence, and for good reason. If you are already learning or knowledgeable about Artificial Intelligence, then you know how pervasive graphs are in both creating and using AI. Think about it: Neural NETWORK. And these days, when people are building LLMs, there is a lot of talk about Knowledge Graphs. In this series, I’ve shown how to create Knowledge Graphs using Wikipedia data, and other sources.If you want to be able to explore those Knowledge Graphs, then you should learn about Network Analysis and Network Visualization. If you want to be able to build your own Knowledge Graphs from interesting data sources, then you should absolutely read this blog and my book.Here’s a couple books you might find useful:Building Knowledge GraphsKnowledge Graphs: Fundamentals, Techniques, and ApplicationsGraphs are EverywhereThere is no getting away from network graphs. They are everywhere, in everything. Your brain is a network. Ideas follow a network. Disease follows a network. Malware follows a network. People interact, creating social networks. Cause and effect follows a network. If you want to understand reality, then you need to acknowledge networks. If you learn to wield networks and network insights as tools, it puts you at a higher level of thinking and understanding. Broaden your horizons by learning more. If you do any kind of data analysis, you will run into graph data, even without realizing it. Learn to recognize it and use it, and reap rich insights.If you don’t want to take my word, here is what one of my book’s readers had to say about graph analysis.I am only starting to appreciate the enormous power of network thinking thanks to reading David Knickerbocker's book on Network Science with Python.I didn't have any appreciation of the (honestly staggering) number of insights that can be drawn from this.What stands out:Enormous power of network thinkingOnly starting to appreciate (it’s easy to overlook)I didn’t have any appreciation (it’s easy to overlook)Staggering number of insights (so many that it is hard to manage)I’m thankful to all of my readers who have let me know how my book has been helpful to them. I am really happy to see this catching on.So, what are you waiting for? Get started today! That’s All for TodayThanks to everyone who has been following along with this series. Happy learning! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Artificial Intelligence and Machine Learning', 'Graph Algorithms', 'Graph Visualization']"
/work/blog_data/144318072.day-33-of-100daysofnetworks.html,"Hi everybody. Happy weekend. One of my favorite things to do with networks is to use them to simulate some phenomenon I have noticed in the real world. The ability to explore and understand the world around me is one of the main attractions of network science and social network analysis to me. Natural Language Processing (NLP) and Network Science together make these possible, and I try to communicate that with this series, encouraging others to explore their own existence.Last week, I used GPT 4.0 to make this image to supplement the article:I like this image for a few reasons:It looks like me when I was younger, only better dressed. AI gave it the right number of fingers, so that’s a plus. The image captures my curiosity and intensity.I made this image to communicate the overlap between Network Science and Geospatial Analysis, but this also communicates that graphs are useful for exploring the real world and our own existence. Geospatial allows us to explore the physical world, NLP gives us the ability to explore language use (how humans communicate), and Network Analysis allows us to explore how things relate to other things. So, today, I made this image the official icon image for this blog series on Substack. I like this much better than the black and white image I previously had.Noticing a PhenomenonI’ve had an idea that has been rolling around in my head for a while, annoying me to the point that I needed to do something about it. This post is the result of this action.Going back to the beginning, what is a node, and what is an edge? A node is any thing. Nodes can be people. Nodes can be animals. Nodes can be concepts. Nodes can be events. What is an edge? An edge is any relationship between two things. So, what is a network? A network is an object comprised of the relationships between things and other things.So, let’s think about two different kinds of networks:Social Networks are made of people and their relationships to other peopleThe internet is made of websites and their relationships to other websitesAre people all the same? Are websites all the same? Are animals all the same? Are dogs all the same? Are concepts all the same? Are words all the same? The answer is a simple NO. So, when we create a social network, we are creating a network of people and how they relate to other people. But are people all the same? Again, the answer is no. We need a way to understand the differences that are at play in the network.Thinking about why people become friends, one reason is that they have something in common. There is a word for this: homogeneity. Homogeneity: the quality or state of being all the same or all of the same kind.Or, another way to put it: like attracts like. Think about who you are friends with? How much does your friend GROUP have in common? Do most people in the group have a lot in common, or is it a diverse group?Like attracts like. People with one worldview will become friends with others who share the same worldview or have a compatible worldview. Those with an incompatible worldview, there is much less chance of a relationship forming.Introducing ModularityI have several books about networks and network analysis. One of my favorites “Networks” by Mark Newman. Recently, while reading this book, I came across the concept of modularity.Modularity is a measure of the extent to which like is connected to like in a network.There is a Network Science measure of the extent that like attracts like in a network! Here is an illustration from the book.When I saw this image, I was immediately inspired. This network shows how groups of different races interact with one another. I circled parts that were interesting to me, but there is some mixing taking place on the left side as well. It is just harder to see.This is network of people. This image clearly demonstrates that people are not all the same. A network might be about one thing (people, websites, animals, concepts, words, etc), but those things have additional context that can be used as well.Experiment: Simulating the InternetIn my work, I spend a lot of time analyzing the internet. The world wide web of today is comprised of billions of websites, in many different languages, with content about everything humans project onto the internet.But with every network analysis I do, homogeneity is obvious in the communities I inspect. Like attracts like. But there is another hidden rule shown in the gaps: dislike repels dislike. And sometimes, dislike still gets a link (an angry hyperlink to complain about).Today, I built a simulation to see what would happen if I applied some rules to how the network would form.In the Jupyter notebook, I describe the experiment:For this simulation, I am attempting to use rules of attraction to dictate how a network can form. I spend a lot of time analyzing the internet, and I want to see if I can simulate what I see in the wild.On the internet, websites link to websites for a number of reasons, but they can be simplified down to two: homogeneity (like attracts like) and opportunity. Websites are created by people, and we network with other people for the same reasons. We form relationships with others who have similar world views as us, and we form relationships with others who may provide opportunities to us (even if we disagree with them).This very rough simulation is just to see what happens when two things interplay: a) when rules determine connectivity, b) when there is very low probability of finding each other to make a connection.First, I will set some worldviews. I'll just call them 0, 1, and 2. You could consider them leftwing, center, and rightwing. You could also consider them Klingon, Human, and Dog. I will specify a rule that both 0 and 2 can connect with 1, but 0 and 2 will not connect with one another. Their disagreement or misunderstanding is too deep.I will also specify ten interests. I will later set a rule that interest 0 can link with all other interests, but no other interest linking can occur. If you think of these interests as internet categories, you could consider interest 0 as news, as lots of kinds of websites link to news (blogs, entertainment, social media, etc).Of course, this is a simplistic experiment. Reality is much more complicated, but let's see what happens. Part of the power of doing network analysis programmatically is being able to set up creative simulations and see what happens.These are the rules of connectivity:worldview 1 can link to worldviews 0 and 2worldviews 0 and 2 cannot link to each otherinterest 0 can link to all other interests including 0other interests can only link within the same interestthere is only a 10% chance of a connection being made, after the above rules are metToday’s experiment is still incomplete, in progress. This is something new, and something I am going to spend some time on, because I have to think of a path forward in order to take this from theory to usefulness. With these rules, a network does form.And what is most interesting to me is that this is a network that was constructed using worldviews and interests, including a rule for incompatibility of world views. This makes sense to me, logically, much more than a random graph. Zooming in on the ego graph, I can see that the nodes that are part of the small community have a range of worldviews and interests. This is the ego graph for the node with the highest Page Rank score.At this level, we can’t see that these nodes are different. We can see that some are more important, by their node coloring, but we can’t see anything about the groups they belong to. I have to look at them differently, and I am still figuring out what I want to do. I could color the nodes by worldview, or I could color the nodes by interest. But I don’t need to decide that today. I can look at a text representation.node: 101; worldview: 1; interests: [7]
node: 188; worldview: 2; interests: [0, 6]
node: 198; worldview: 1; interests: [2]
node: 279; worldview: 1; interests: [2, 6]
node: 307; worldview: 2; interests: [3, 1]
node: 446; worldview: 0; interests: [9]
node: 456; worldview: 2; interests: [0]
node: 499; worldview: 0; interests: [0, 6]
node: 505; worldview: 2; interests: [9]
node: 508; worldview: 2; interests: [7, 2]
node: 516; worldview: 2; interests: [0]
node: 531; worldview: 2; interests: [4]
node: 546; worldview: 1; interests: [9, 7]
node: 583; worldview: 0; interests: [3]
node: 64; worldview: 1; interests: [2, 6]
node: 663; worldview: 0; interests: [6, 0]
node: 677; worldview: 2; interests: [0]
node: 71; worldview: 2; interests: [2, 9]
node: 723; worldview: 1; interests: [0]
node: 765; worldview: 1; interests: [0]
node: 807; worldview: 1; interests: [9, 2]
node: 861; worldview: 2; interests: [5, 0]
node: 862; worldview: 2; interests: [6]
node: 9; worldview: 1; interests: [6]
node: 901; worldview: 2; interests: [2]
node: 930; worldview: 0; interests: [8]
node: 954; worldview: 1; interests: [0]And I can create subgraphs of the different worldviews in the ego network. This is the subgraph of worldview 1 nodes.And since I have identified worldviews and interests, I can calculate modularity. This code is a bit gnarly, as I needed to just figure out an approach. Clean and good is later. But this works.c1 = set(ego_df[ego_df['worldview']==0].index.values)c2 = set(ego_df[ego_df['worldview']==1].index.values)c3 = set(ego_df[ego_df['worldview']==2].index.values)modularity = nx.community.modularity(ego, [c1, c2, c3])In this ego network, there is a modularity score of 0.146. This was a lot of work to get to one number. There was a lot of setup involved. That is normal when trying to operationalize something theoretical. There is no cookbook for this, no one person for me to lean on. But what is this modularity score?You can read more about it in the networkx documentation.  The page also links to several other sources, including the book I mentioned in the beginning. Basically, a positive score indicates assortative mixing (like attracts like) and a negative score would indicate the opposite (dislike attracts like).I’m Still LearningI am still learning. This is how I learn. I like to learn, and then show what I have learned. There is a saying that if you can’t explain something simply, then you do not understand it well enough. Storytelling is very good practice for communicating, but it also reinforces what we have learned. This is a new concept for me, one that I haven’t explored or used in practice. That means I have a lot to think about, if I want to use this. I have to do certain things a little differently to even capture the context that is needed in this calculation. Community detection algorithms are not enough, as they just capture communities based on network placement and connectivity, not based on WHY the things are connected in the first place.I’m not done with this simulation or this concept. Today, I wanted to get the simulation setup, even if not perfect yet. I have more to think about, more to experiment with. Now I can use this to get to know modularity better.Code is AvailableToday’s code is available. You can get it here. Keep in mind, this is very experimental. I encourage you to learn the concept and do your own experiment and simulation. Use mine for inspiration and take it further, or use it as inspiration and do something completely different. Play with simulation. Explore reality.This is really fun programming. This is also a great way to learn to code, but significantly more challenging than the typical “build a calculator” or “write hello world” stuff.That’s All for TodayThanks to everyone who has been following along with this series. Happy learning! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Artificial Intelligence and Machine Learning', 'Community Detection', 'Egocentric Network Analysis', 'Graph Algorithms', 'Graph Visualization', 'Subgraphs and Connected Components']"
/work/blog_data/144341480.day-34-of-100daysofnetworks.html,"Today, I want to reinforce the learning we have done over the course of this series, summarizing our learning. This article will be useful to bookmark and revisit as a resource, as I will be discussing the different scales and views that are possible in network analysis.Why Do This?In my opinion, the hardest part of learning to analyze networks is learning to look at the different parts of a network rather than the whole thing. It’s very easy to make a very large graph about anything. However, once the graph is made, people get stuck. I have seen this repeatedly. People have graph data, and then they go, “Now what?” Teams consider whether they need a graph database, and then cycles are spent researching graph databases. Months go by and nothing gets done. People then sour on network analysis.It’s a very confusing moment, after you realize that you have a ton of graph data and no idea how to use it.One of the reasons I wrote my book was to take away this specific pain. Throughout my book, I do not teach graph databases. Over the course of this blog series, I’ve shown how to create and analyze several different kinds of graphs: collaboration networks, topic graphs, social networks, and more. And I’ve shown how to do this without any requirement of learning a graph database, giving you an easier learning course.A graph database may eventually be useful to you. But it is not a requirement for learning to analyze and pull useful insights from networks. I do not use one for anything.For me, things suddenly became easy when I learned to dissect networks, cut them into pieces, and look at the pieces individually. Million scale becomes thousand scale (or less) this way, and becomes very easy to investigate. Consider this as untangling complexity, or flying around a network.High Level OverviewThis table describes the approaches that can be used to look at the parts of a network.In this article, I’m going to describe each of these and provide links to networkx documentation. I talk about these concepts throughout my book and blog series, so I’m not going to look up each individual where I mention these. You can use this image as a guide, and find articles where I discuss these concepts. Later, I will do an index for these posts, but they are still being written, and I don’t want to manage that right now.Views and ScalesI’m calling these as views and scales. Each one of these provides a different view of the network, and at a different scale.Whole NetworkWhole Network Analysis (WNA) is the analysis of the whole network. At this level, you can capture high level insights:Centralities and Importances will tell you what are the most important nodes based on some context. For speed, I recommend that you start with PageRank. If your network is small, you can use Betweenness Centrality instead. If your network is large, Betweenness Centrality will be very slow and so will Closeness Centrality. Part of this learning is learning which measures are useful with the network size that you have. Big time saver: use Page Rank by default on any network, then add more that you notice will be suitable and usable.If your network is small enough, you can explore the “shortest paths”.Node and edge counts will tell you the size of the networkDensity and other measures can tell you about the characteristics of the whole networkBridges are cool to explore, to learn about the nodes that are holding the network together.Modularity can tell the extend that group mixing is taking place in the network.At the level of Whole Network Analysis (WNA), I think of this as Network EDA (Exploratory Data Analysis). At this level, it is about building a mental picture of the size, complexity, and key movers of a network.Connected ComponentsConnected Components are essentially the connected structures of a network. Networks typically have one supermassive component, several smaller components, and lots of isolate nodes. In my own book, I describe them as continents and islands. Throughout this series, I’ve shown how to identify the connected components in a network, and you can read the networkx documentation here.This part has a lot of overlap with community detection, so I tend to just identify the individual connected components, inspect the largest ones, and move along. Community detection is a bit more useful to me, so I keep this part light.CommunitiesYou can use Community Detection algorithms to identify the communities that are at play in any given network. If the network is a social network of people or animals, you will find the groups that interact. If the network is not life-based, you will find groups of things. You can think of community detection as clustering algorithms. They identify the clustering that is taking place in any network based on nodes connectivity.There are several community detection algorithms, and I’ve written about them in my book and on this series. My favorite is the Louvain Method, and you will see that used on most days.There are so many different approaches that can be used for community detection, that networkx has made a whole section for this topic. Here are a few I recommend looking at:Louvain CommunitiesLabel PropagationIn my book, I discuss these and others. I also have a whole chapter on using Graph Machine Learning to identify communities, and showcase a cool algorithm called Scalable Community Detection (SCD) which rivals the Louvain Method in both speed and accuracy. You should buy my book. You should also buy this book, as it shows how Machine Learning can be used for community detection and other things.K-coreK-core doesn’t get the attention it deserves. It is rarely talked about, and I only accidentally discovered it in one of the Social Network Analysis books that I own. I use K-core for two different reasons:It’s a great shortcut for removing all isolates with one line of codeIt’s useful for zooming in on the core of the networkThe core of a network can tell you a lot about the key movers and shakers. If you zoom in on the core of any network, you will see a small group of densely connected nodes. Once you find these nodes, you should start coming up with questions to learn more about them.You can find examples of me using k-core throughout this series, and you can read the documentation here.K-coronaWhere k-core is about looking at the core of a network, k-corona is about being able to look at the layers of a network. Think of a network as an onion. There will be nodes with:0 edges (no connections)1 edge2 edges3 edgesand so on all the way to the maximum edge count for a given nodeWith k-corona, you can look at each of these levels. Personally, as k-core can be useful for quickly removing all isolates, I mainly use k-corona for quickly finding all isolates so that I can analyze them rather than discarding them. To do this, I would do something like isolates = nx.k_corona(G, k=0)However, you are not limited to using this to inspect isolates. You could use this to analyze any layer. You can read more about k-corona here.So that this concept sticks: think of this as peeling an onion and being able to analyze the peeled layerEgos and AltersEgocentric Network Analysis is one of my favorite parts of any network analysis. I’ve written about this in my book and in this series. Here is another book that I own on the topic. It’s a good read.In Egocentric Network Analysis, you zoom in on one node, the ego node. This one node—the ego node—is where the name Egocentric Network Analysis comes from. In Egocentric Network Analysis, you pull an ego network from the whole network. Usually, I do this after identifying the most important nodes using centralities and importances. That task leads right to this. The first task makes a shortlist of important nodes, and then in Egocentric Network Analysis, I inspect each one.In yesterday’s article, I took Egocentric Network Analysis further than I have ever done before by identifying the modularity of a single ego network. I am still thinking about what I can do with this and use it for, and how to make the process simpler.In an ego network, the ego node is in the center, and all of the related nodes (friends, enemies, acquaintances, coworkers, etc) also appear. However, they are known as alters. Egos and alters. Those are the words to know. Ego is the center, alters are the others. Ego is the subject, alters co-stars. You can find plenty of examples throughout this series, and you can read the networkx documentation here. SubgraphsA subgraph is just a part of a graph. A graph within a graph. Several of the above extract subgraphs. An ego network is a subgraph of the entire network, focused on one ego node and its alters. A connected component is a subgraph of the entire network, focused on one single structure in the network. Communities are also a subgraph of the entire network, focused on identifying and extracting the individual groups.But there is one networkx function that can be used directly to pull a subgraph.I personally use this alongside community detection. I’ll use community detection algorithms to identify the nodes that belong together, and then I’ll use a subgraph to pull the community from the larger network.It’s also just a generally useful tool to have around, for anytime you want to inspect only a part of the network. You can inspect any given nodes using this.ModularityModularity is a concept I have been learning about recently. You should read yesterday’s post to learn more about it, so that I don’t have to type all of that again.Here is an image from Networks by Mark Newman. Modularity has to do with the extent that different groups mix in a network. Think of it this way: communities are made up of people, but people are not all the same. Modularity allows us to peer into networks, to understand how relationships exist between groups. This means that community detection and ego networks aren’t the deepest we can go. We can use modularity to investigate each of these small groups further.I linked to the networkx resources in yesterday’s post. Please read it.Node Centralities and ImportancesLast but absolutely not least, node centralities and importances are probably the first most exciting thing that anyone will face when they begin learning to analyze networks. It is the first OH MY GOODNESS moment in the learning process, when you realize you can take a gnarly spider web and pull out a clean list of most important and influential nodes.Node centralities and importances tell you who the stars are of any network. There are several that you should know about:PageRank: your go to, no matter the network. It is fast and useful for finding key nodes, no matter the size of your network. Importance has to do with the number of inbound and outbound edges. The Google founders created this algorithm and used it in their search engine to find important websites on any given topic.Betweenness Centrality: this is the one people usually learn first. Betweenness centrality has to do with nodes that appear in the most “shortest paths”. Nodes with the highest betweenness centrality scores sit between people and groups. Information flows through them. They become important to decision making. They could also be a blocker or gatekeeper. This algorithm becomes unusable the larger the size of your network. If it becomes unusable, PageRank or Degree Centrality will do.Degree Centrality: nodes are important if they have more degrees/edges. This is always fast and can be useful on very large networks, when Betweenness Centrality is no longer usable.Closeness Centrality: nodes are important if they are close to many nodes. This algorithm becomes unusable the larger the size of your network. If it becomes unusable, Page Rank or Degree Centrality will do.There are many to explore. There are so many of these that networkx has a whole page about them. There are more available than I have found time to learn. This page is a very good resource for learning.That’s All for TodayLearning to fly around networks and inspect different levels is useful for capturing a wealth of useful context and network insights.Thank you for reading today’s article. Today’s post is a bit of review, and I hope it gave you a fresh perspective of things that can be immediately useful in any network analysis. This is a good page to bookmark as a resource for later use.Thanks to everyone who has been following along with this series. Happy learning! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Artificial Intelligence and Machine Learning', 'Community Detection', 'Egocentric Network Analysis', 'Graph Algorithms', 'Subgraphs and Connected Components', 'Whole Network Analysis']"
/work/blog_data/144541381.day-35-of-100daysofnetworks.html,"Hi everyone, happy weekend. I have a fondness for reading literature. Actually, I have a fondness of reading in general, but there are certain authors that I am very fond of. My favorite authors include Ray Bradbury, Robert Silverberg, Arthur C. Clarke, Kurt Vonnegut, Lewis Carroll, and many, many more. Natural Language Processing and Network Science is cool, but using Natural Language Processing and Network Science to analyze literature is cooler!I’ve been using both to analyze all kinds of writing for years. All writing is information, and we can analyze language using useful technologies. Language is not a black box. Language is not something that is impossible to measure, analyze, or use.Today, I am dabbling in the Computational Humanities. Using software engineering, Natural Language Processing, and Network Science to understand literature is a clear example of Computational Humanities.I think that Computational Humanities is a very important subject that does not receive enough attention. We can use computers to better understand ourselves.Language and CreativityI have been reading an incredible book called The Information, and two quotes have really captivated me, this week.“The circle of the English language has a well-defined centre but no discernable circumference.” In the center are the words everyone knows. At the edges, where Murray placed slang and cant and scientific jargon and foreign border crossers, everyone’s sense of the language differs and no one’s can be called “standard.”This is from page 72, the second paragraph.When I read this paragraph, I knew exactly what James Gleick was describing, because I have seen it with my own eyes, several times.He is describing the English language as a network of words, with the rare words on the outskirts, and the common words in the core.Like this, this is the core of the network I will describe in today’s article. Look in the center, and you’ll see these words: in, and, to, that, so, was, for. Node coloring shows the most common words in red.I learned to do this in 2018, when I was working on some deepfake defense work. We needed to be able to classify between AI and human-generated context, and we did quite well in that. During this project, I had a lot of opportunity to look at both human and AI generated text.When I read the paragraph about the circle of the English language, I knew exactly what he was talking about, and you can too. This is the overlap between Natural Language Processing and Network Science, and they make for the absolute best of friends.The second quote that inspired me this week is from Ada Lovelace, on page 112 of the same book.[Imagination] is that which penetrates into the unseen worlds around us, the worlds of Science. It is that which feels and discovers what is, the real which we see not, which exists for our senses. Those who have learned to walk on the threshold of the unknown worlds… may then with their fair white wings of Imagination hope to soar further into the unexplored amidst which we live.I intentionally split this quote in two parts, because there are two halves:Imagination penetrates into the unseen worlds around us, the worlds of science. Imagination is that which leads to discovery of reality.Those who learn to walk on the threshold of these unseen and unknown worlds will soar further into the unexplored amidst which we live.That’s pretty much the whole point of my book, my blog, and my work. I tap into these hidden networks that exist around us and show how to draw out useful insights. This is not just cool. This is how you go further.I hope these quotes captivate you.Today’s ExperimentToday, I will attempt to build a vocabulary list off of Jane Austen texts. She is not my favorite writer, but I have been impressed with her word-use for as long as I have known about her. Today, I’ll use her as my vocabulary coach.This is a fun thing that I have done many times, even shown similarly on a few days of this series. It’s easy to build word networks, and they are fun to analyze. You can do this with any author, with any text, including your own text. Today, I want to do something fun and simple. I will use literature as input, and I will generate a vocabulary list as output.literature → (NLP + Network Science) = vocabulary listThe NLP for this task is simple, mostly text cleaning and splitting the text on spaces, the very basics of NLP, nothing advanced.You can get the code here.Here is the approach:I used NLTK to identify which Jane Austen texts I could accessI downloaded all of them and smashed them into one single ‘text’ variableI did some wrangling of the text, removing non-word characters and suchI split the remaining text into words, splitting on whitespaceI used the words to create a word networkI used the degrees of the words in the word network to find the rarity of words I was looking for.I recommend that you click the code link above and follow along with the notebook, to see the process in action, and the outputs of each step.In the end, I am able to investigate the layers of the word graph, like peeling an onion, like I described in the previous post.Looking at the final output, of the rare words that start with the letter A, I see these words:a-day, a-shooting, abandoned, abatement, abbreviation, abdication, ablest, abolition, abominably, abominate, abominates, abounded, abridge, abridgement, absences, absented, absorbing, abstained, abstracted, abstruse, absurdities, abundant, abuses, acacia, acceded, accelerate, acceptably, accepts, accessions, accidently, accompaniment, accord, accountable, accounting, accumulation, accumulations, accustom, accustomed, aches, achievement, acquaintances, acquire, acquitting, acres, acrostic, actuated, acutely, adherence, adhering, adjusting, administered, administering, admirers, admits, adoption, adored, adoring, advancement, adventuring, adventurous, adversary's, adverse, advertise, advertised, advertising, affects, affirm, affix, afflict, afflicting, afflictions, affluent, affronts, after-days, afterward, againsts, agent, aggrandise, aggrandizement, aggression, aghast, agitating, agriculture, ah, ahead, aids, ailed, aim, aimable, akin, al-fresco, alienable, alienated, alienations, alighted, all-sufficiency, allay, allayed, alleged, allied, allies, alphabetically, alternation, alternatives, ambitious, amended, amiableness, amiably, amicable, amid, amounting, ancestry, anchorage, anecdote, anecdotes, angles, angrily, ankle, ankles, annoyance, annual, annum, antagonist, anticipations, antidote, anxieties, anymore, anyone, apologized, apparatus, appease, appellation, appendages, appetites, applauded, apple-dumpling, apple-dumplings, apple-tart, apple-tarts, apple-trees, applicant, applicants, appointments, appreciating, appreciation, apprehending, apprehensively, approachable, appropriate, approval, approver, apricot, aptitude, archness, argue, argued, arguing, argumentative, aright, arises, aristocratic, armed, army, arranger, arrear, arrives, arrow, articulation, artlessly, ask-, asperity, aspersion, assailed, assemblies, assenting, assertions, assiduously, assistants, assizes, associated, associating, association, assorting, assuage, astonishingly, atoned, atoning, attachment's, attacking, attaining, attestation, attorney, au, audacity, auditors, augment, augmentation, augmenting, aunts, auspices, auspicious, auspiciously, authentic, authorise, authorising, autumnal, availed, availing, avenue, avenues, avoidance, avowed, avowedly, awaited, awaken, awakening, awaking, awe, awes, awhile, awkwardnessesThese are the most rare words from Jane Austen’s literature. These are not all difficult word, but they are the most rare in her vocabulary.These are just the words that start with the letter A. I recommend that you play with the code and explore, to internalize capabilities.This was an easy experiment. If you are just getting started, this is a good one for you to start with, because setup is so simple.Expanding the ExperimentIf we wanted to show what was described in the first quote, about the circle of the English language, how would we do it differently? Scroll up, look through the steps. There is one single step that needs to be changed to show what that quote is describing. Here is how I would do it:Collect as much random text as you can from as many human sources as you can get access to and use. Throughout this series, I’ve shown some useful sources, and data is everywhere. Find what seems useful.Load all of the data and combine the text into one single ‘text’ variableClean the text, removing non-word characters and suchSplit the remaining text into words, splitting on whitespaceUse the words to create a word networkInvestigate the layers, using degrees or k-coronaThis is Fun StuffComputational Humanities is always fun. When I was in college, we relied on Cliff’s Notes books as study guides to help us in our understanding of literature. Some people used them to cheat, but that is not why they were created. I think of computational humanities as Cliff’s Notes on steroids or rocket fuel. It is one thing to read Alice in Wonderland. It is another thing to play in the networks and run simulations, to let the Red Queen have her way, or to turn the White Rabbit into an assassin in an alternate universe. But these approaches will work on any text. Get creative and run the experiments you want to run.Another Thank YouMy book has been doing really well, and I want to thank all of you who have read it. Today, I want to give a special thank you to my friend Koo Ping Shung who had some really nice things to say about my book.You should follow him on Substack and LinkedIn, if you aren’t already. He shares so much useful information and is very active in the Data Science community. You should read and follow his blog as well.There are a few things that I liked about his review:“His dedication and commitment to having good content can be seen in all these posts, unlike other authors, not necessarily on topics in #datascience or #artificialintelligence, there is no track record to follow on.”“It covers a large variety of topics too like #naturallanguageprocessing & #machinelearning together with network analysis which is not an easy topic to write on and thus not a lot of materials on it.”“There are accompanying #python codes that gives you immediate feedback on your learning or put into actions what is written which can increase your learning efficiency!”“Personally I feel that network analysis is under-utilized, as in it is a overlooked goldmine of insights that can be used so there is first-mover advantage if you have a good grasp of it.”“Get the book! :D”“It widen the variety of technical books in my library! :D”“It combines the social sciences with data”I want to address each of these, because they address the why of why I write.I try to keep human innovation moving forward. I’m not writing for myself. I don’t care to read my old writing. I like to keep things progressing, including human knowledge and innovation. So I write, and write, and write. I don’t do this for money. Creativity helps me relax and recover.Yes, this book and today’s article showcases the value of combining software engineering, network science, and natural language processing. This innovation is powerful.The code is written so that you can immediately understand if you are moving in the right direction. I try to write my code for this so that it is easy to follow, not so that it is ready for production. That is a different task with a different focus.This is absolutely under-utilized. So, I don’t mind that my blog only has hundreds of readers while my LinkedIn has tens of thousands of followers. This is a specialized subject, not as broad as Artificial Intelligence or Machine Learning, and certainly not as popular. I find it more exciting, because it is more useful to me than ML and AI both, but that is something I learned, and something you can learn.Get the book! :DIt will broaden your technical horizons. Re-read that Ada Lovelace quote. That’s why I do this.We are combining the social sciences with software engineering. Why are we doing that? To understand ourselves and reality better. Thank you, Koo Ping Shung! Your review made my day! Thank you for everything that you do!That’s All for TodayThanks to everyone who has been following along with this series. Happy learning! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Artificial Intelligence and Machine Learning', 'Graph Algorithms', 'Graph Visualization', 'Subgraphs and Connected Components', 'Whole Network Analysis']"
/work/blog_data/149580878.day-36-of-100daysofnetworks.html,"To all of my readers, thank you for being patient with me. I’ve enjoyed many interesting conversations about graphs, network science, and artificial intelligence over the past several months, but I haven’t written anything since May. As I have mentioned several times, learning is a creative outlet for me, and I enjoy helping others learn as well. So, I’ve been wanting to get back to writing, but have found it difficult.Even though I haven’t been writing, my blog continues to grow! This blog will pass 500 subscribers soon, which is exciting. My book also continues to do well! It’s been out for almost two years and is highly rated (4.9) on Amazon.Begin Where You AreIn my own life, meditation has been useful to me in several ways. Lately, during meditation, a certain phrase has really stood out: begin where you are. I have been giving people that advice for years, but I have been struggling with it in my own life lately. I have been wanting to write, but I have a million things I want to write about, and I was trying to do too much.I want to get back to writing. I enjoy writing. It gives me peace, and is something for me to look forward to doing. My writing is different than what I work on, so it gives me an outlet to explore other things, such as what I am writing about today.So, today, I am taking this advice: begin where you are. Just start. The hardest part is getting started, and if you sit down for too long, it is difficult to get back up. Writing is like that as well.Guitar Learning ToolI have been playing guitar for 31 years now. Even after all of these years, I still struggle to memorize scales. It is so bad that I just don’t even bother trying to memorize them by name. Rather, I practice them over and over and over and try to capture their sound. But I WANT to memorize them. I want to be able to mentally flip between them. However, the way that scales are taught to guitarists is just not helpful to some of us.For reference, here is an example of ONE scale from my favorite book. There are MANY scales, not just one. This is from my FAVORITE scale book…I struggle with memorizing anything. I have struggled with memorization my entire life. So, I find ways to work around things that give me difficulty.And even this image is only going to the 12th fret, because everything repeats after the 12th fret, so it should be twice as long.But each scale starts at around A and ends at around G, so this scale above goes: A-flat, B-flat, C, D-flat, E-flat, E, G-flatWhat do you think is easier to look at? The above image? Or: Ab, Bb, C, Db, Eb, E, GbWhich one makes it easier to notice that F has been skipped? Yesterday’s IdeaI think tools can be made to make learning a little easier for guitarists like me, who are more auditory than visual, by simplifying the visual and providing the audio.I was practicing this scale last night, and I noticed that if I just focused on A-G at only a single part, the whole thing becomes a lot less intimidating. However, I was still struggling with memorization.So, I thought, what if I converted any scale into a complete graph, and used that as a learning tool? I played with that idea a bit today. Work in ProgressThis is a work in progress, something to play with on weekends, something to help me build up my arsenal of memorized scales, helping me become a more versatile musician, using data science and network science. I didn’t know how to do some of this yesterday, so it is rough. I was hoping there was a simple library for playing notes. Nope. Haven’t found one yet. So, I had to get that working, and I’m not completely happy with it. But in building, get it to work, then get it to work well.Show and TellYou can get today’s code here. From here, I’ll show how this works, and the results. This is day one. I will do more with this. These are the results after about an hour of working on it.First, I didn’t want to choose a specific scale. I wanted to start simple, with A, B, C, D, E, F, G. No minors, no sharps. I wanted to:Construct a complete graph using these notesUse the complete graph to automatically generate playable music sequencesPlay the music sequencesThe third part will be good practice for scales. This will automatically generate sequences of notes that can be used for practice, and you’ll have to remember where notes exist on the next. The visual element has been replaced with audio, in other words. First, I choose the notes: A, B, C, D, E, F, GThen I create the complete graph.Remember, in graph theory and network science, a complete graph is a graph where each node is connected to every other node in the network. As each note in a scale is always accessible, the opportunity for using the notes of a scale fit into a complete graph. The scale presents the rules. The scale decides what is and is not included. Beyond what is included, there are no rules to the sequence notes are played in. However, interesting feels can be brought out by doing interesting things with notes, like playing in unexpected order, or by using repetition, etc, all the musical tricks.Today is about graphs and music, fusing the two. I’m not going to go deep into either, today.From this complete graph, I can generate a list of possible sequences, but I wanted the sequences to be of uniform length, to make practice a little more comfortable and less chaotic. The first five sequences look like this:[['A', 'C', 'D', 'E', 'F', 'G', 'B', 'A'],
 ['A', 'C', 'D', 'E', 'G', 'F', 'B', 'A'],
 ['A', 'C', 'D', 'F', 'E', 'G', 'B', 'A'],
 ['A', 'C', 'D', 'F', 'G', 'E', 'B', 'A'],
 ['A', 'C', 'D', 'G', 'E', 'F', 'B', 'A'],
 ['A', 'C', 'D', 'G', 'F', 'E', 'B', 'A']]I start and end each sequence at A. That’s just what I have done, today. Later, I can expand the sequences by starting with each individual note included in a scale.Each sequence can be looked at visually as a “path graph”. Programmatically, I can pull out a random sequence, visualize it, and listen to it.Extract sequence:path = choice(note_paths)

G = nx.path_graph(path, create_using=nx.DiGraph)
print('Path: {}'.format(path))
draw_graph(G, node_size=10, show_names=True, edge_width=1, font_size=14)Visualize graph:Listen to it:[play_note(note, 0.6, repeat_times=2) for note in path]This is one sequence I actually did find interesting. It has a cool sound for grunge, but would also sound good clean. This is what it sounds like on guitar.Chords:A bit more complexity:This guitar riff came directly from the graph. I listened to a few sequences, found one that I liked, picked up my guitar, and played the chords instead of notes. Longer sequences can also be made, like so:s1 = choice(note_paths)
s2 = choice(note_paths)
s3 = choice(note_paths)

song = s1 + s1 + s2 + s1 + s2 + s2 + s3Look closer and you’ll see that I’ve constructed a bit of a song.Three sequences are pulled from the graphThe first sequence is played twiceThe second sequence is played onceThe first sequence is played onceThe second sequence is played twiceThe third sequence is played onceYou can then listen to the audio of the sequence using the same code as shown previously.This is an end-to-end experiment, showing that graph can be useful in creating learning tools that are more suitable for our individual learning styles.Also, it’s a lot of fun to listen to the random sequences. This Is Why I Love GraphsI have been saying that graphs provide ways of learning about the world we live in. Graphs aren’t the only way. They are a tool that can be used, and I find them very useful for learning, for building systems thinking, and for understanding the abstract.One thing I like about graphs: I don’t look for things to use them on. It is the other way around. Usually, I’ll get frustrated with how something is done (like how we learn scales), and that’ll get me thinking (about the relationship of notes, scales, and chords, etc), and that’ll inevitably lead to me realizing that there is another unexplored approach with graph that could make things easier. What’s NextI’m going to continue with this kind of music + graph stuff for a while. I would really like to come up with something useful for learning scales. This blog isn’t just for teaching, it is for experimentation and learning. That’s All for TodayThanks for reading. I’m glad I was able to find something that would be fun to develop and write about. I’m excited to explore graphs musically. This is a cool topic.Thanks to everyone who has been following along with this series. Happy learning! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Artificial Intelligence and Machine Learning', 'Graph Visualization']"
/work/blog_data/149889400.day-37-of-100daysofnetworks.html,"I’m really excited to write today’s article. Last week, I found a way to use data science, network science to build a tool so that I could more easily learn to play scales as a guitarist. This week, I’ve taken that idea much further, and it is opening up new pathways and opportunities.My Musical Learning and StrugglesA little bit about me: I have been playing guitar since I was fifteen years old. My family is musical, so I have always been around it, but at age fifteen, I saw a group of high school students playing Creep by Radiohead, and I was immediately enthralled. That means that I have now been playing guitar for 31 years. Over all of those years, some things were easy for me, and other things seemed out of reach. Chords and rhythm guitar was always easy for me, but I could not memorize scales, so I really struggled to learn lead guitar. Or, I should say, I struggled to hold interest in learning lead guitar. The way that my mind works, if I can’t do something, I lose interest. But with learning lead guitar, I KNEW that it was not out of reach, because with brute force, I’ve always been able to roughly learn my favorite solos. I knew I could get there with scales, but everything I used to help me learn them would overwhelm me.Last week, I found something that actually was helpful, and since then, it’s as if it unlocked a new capability in my brain. Literally, what I am about to show you has unlocked new things for me.And I know why. I cannot memorize names very well. That has always been true. But, if my mind makes a relationship between a name and something I’m familiar with, my mind struggles less. I think my mind is doing that with scales, now that graph and NLP have allowed me to build relationships with concepts I couldn’t before.So, I am very excited to write this, because:This is something that is actually helping me. It is making a difference in my life.I told you that Network Science and Natural Language Processing are useful for learning about things that exist in our livesThis really shows the versatility of both network science and natural language processing. I am using both on scales, which can be used in both text as well as graph format.Enough. Let’s go!Creating the GraphIn last week’s article, I used graph to show ONE scale. This week, I have built a graph of scales from a book I am learning from.You can get today’s code here!I used a Python dictionary for this:scales = {}

# all C scales
scales['Ionian'] = 'C D E F G A B C'
scales['Dorian'] = 'C D Eb F G A Bb C'
scales['Phrygian'] = 'C Db Eb F G Ab Bb C'
scales['Lydian'] = 'C D E F# G A B C'
scales['Mixolydian'] = 'C D E F G A Bb C'
scales['Aeolian'] = 'C D Eb F G Ab Bb C'
scales['Locrian'] = 'C Db Eb F Gb Ab Bb C'
scales['Jazz Minor'] = 'C D Eb F G A B C'
scales['Jazz Minor Mode 3'] = 'C D E F# G# A B C'
scales['Jazz Minor Mode 4'] = 'C D E F# G A Bb C'
scales['Jazz Minor Mode 7'] = 'C Db Eb E Gb Ab Bb C'
scales['Harmonic Minor'] = 'C D Eb F G Ab B C'
scales['Harmonic Minor Mode 3'] = 'C D E F G# A B C'
scales['Harmonic Minor Mode 5'] = 'C Db E F G Ab Bb C'
scales['Harmonic Minor Mode 6'] = 'C D# E F# G A B C'
scales['Harmonic Major'] = 'C D E F G Ab B C'
scales['Harmonic Major Mode 4'] = 'C D Eb F# G A B C'
scales['Harmonic Major Mode 5'] = 'C Db E F G A Bb C'
scales['Harmonic Major Mode 6'] = 'C D# E F# G# A B C'
scales['Major Pentatonic'] = 'C D E G A C'
scales['Minor Pentatonic'] = 'C Eb F G Bb C'
scales['Major Blues'] = 'C D Eb E G A C'
scales['Minor Blues'] = 'C Eb F Gb G Bb C'
scales['Blues Scale'] = 'C Eb E F Gb G Bb C'This dictionary is the foundation for the rest of the analysis.Scale Similarity (NLP)Before I even wrote last week’s article, my mind was already pestering me, wanting to know which scales were most similar to which other scales.It is one thing to memorize everything in a list, top to bottom. My mind does not work like that. Like I said, if I can notice a relationship between something I want to understand and something else I know or want to understand, then things start to make sense to me. I think it’s my mind building out some systems thinking.So, even originally, I wanted “scale similarity”. I even knew how to do this, as I do a lot with Natural Language Processing. In the scale dictionary, a scale is a Python string. Similarity is not out of reach. I knew I could use NLP for this. So, I used the scales to create TFIDF vectors, and I used those vectors to capture and visualize scale similarity.The similarity matrix is easy to understand after you’ve seen one once. I used cosine similarity for this, which returns a matrix of similarities. Look for Phrygian and you will see it TWICE. At the very bottom right, it has a value of 1. This indicates that the Phrygian scale has exact similarity with itself, because of course it does. Values that are close to one show high similarity. Low values are the opposite.But, this is messy and hard to use for learning. So, I took the most similar scales for each scale and visualized it this way instead, and it can be done for each scale:That’s very cool. It is obvious which scales are most similar to Aeolian. But guess what. While writing the code, I already forgot which notes are in these scales. So, let’s see.I used this code (ugly, don’t care):scales['Aeolian'], scales['Harmonic Minor'], scales['Dorian'], scales['Phrygian']Which gave me the ability to easily see and compare. ('C D Eb F G Ab Bb C',
 'C D Eb F G Ab B C',
 'C D Eb F G A Bb C',
 'C Db Eb F G Ab Bb C')Most importantly, this is the point where I picked up my guitar, tried each of the four, and started to actually understand their similarity, how they relate. This understanding is a deeper knowledge than simple memorization. This understanding will make me a better musician.It’s really fun to explore the similarities and use them for learning.Scale GraphI then used the scales dictionary to create a Scale Graph, mapping out how each scale used each note. Check it out! There is some shape to this! Graph visualizations are always immediately illuminating to me. Things just jump right out:Fewer scales are using Gb, Db, Ab, G#, F#, and D#. They jump off the side of the visualization, because they have fewer connections.The C note has highest page rank (highest importance) across all notes, which makes perfect sense as I used C scales. Every scale used the note C.You can kind of see which scales have similarity even by graph positioning.Don’t just listen to me. What stands out to you. Look closer. Develop your systems thinking abilities, and build confidence analyzing graphs visually.But, I know that this can be visualized in a way that only shows the scales, or only shows the notes. I can use Bipartite Projection for this. You only need to do things a little bit differently to use Bipartite Projection, and it can make a world of difference and unlock new perspectives.B = nx.from_pandas_edgelist(edgelist_df, source='scale', target='note') # new approach
G = bipartite.projected_graph(B, edgelist_df['note'])I’ll walk you through this:The edgelist is a bipartite edgelist, containing of scales and the notes belonging to them. The edgelist contains two types of things, scales and notes. It makes sense to create a bipartite graph for this.If I don’t do the second step, B looks like the graph above. Scroll up. The graph has a mix of scales and notes.The second line takes this bipartite graph (B) and projects it into a new graph that only shows notes. This is what G now looks like:Look at that. All notes, no scales. Clean.This is enough to have fun with. It doesn’t matter if you are a guitarist, bassist, pianist, whatever. You can experiment with different progressions by going from one note to any other note that it is connected to. If you are a guitarist, practice power chords, and then try out minor chords. Experiment. That’s the point.The Page Rank of notes is also very interesting. You can see which notes are most “central” in the note graph. C and E have a lot of importance across ALL scales, in the key of C.Note Ego GraphsThe note ego graphs are also interesting to explore. For instance, here is the ego graph for note E:Here are it’s top ten notes by Page Rank:And for direct comparison, here is the ego graph for Eb:And it’s high Page Rank notes:Oh my goodness, what have I done?Oh my goodness, what have I done? I have taken a topic that has always been out of reach for me and made it accessible and fun. Because I was stuck for so long, my mind, without my permission, decided that scales are BORING. Now, scales are interesting to me. I can learn more about them in a way that works for me.I showed this to a friend of mine last week and she kept saying, “I wish I had something like this when I was learning to play Piano as a kid.” I can relate. So many topics are taught via brute force and memorization, and they become so frustrating for those of us who struggle with memorization. We learn how we learn.And this is actually helping me, as well. Playing guitar is a real world thing that I do. I have used a combination of Natural Language Processing and Network Science to learn more about the world around me. That’s why I wrote my book. My book is a combination of Natural Language Processing and Network Science. I describe how that is a powerful combination because:NLP gives you the what. What is being said. What is the sentiment. What note is being played. What scales are similar, textually.Graphs give you the relationships and their importance. NLP does not give you this. Network Science gives you this.Other Cool ExcitementI want to give a call-out to a few LinkedIn connections.If you want to learn about ontology and ontology networks, you need to follow Jérémy Ravenel on LinkedIn. He posts some of the most interesting stuff that I see in my feed, day after day. He is very active and his posts are very interesting.Yuki Kakegawa recently wrote a book called Polars Cookbook, that really explains the Polars library very well. I am beginning to read it, and finding it very useful. You can read my first impression, here.  You can get his book here.I am experimenting with converting some of my Pandas code to Polars, to see what parts of graph analysis I can speed up. I’ll occasionally use Polars with #100daysofnetworks. It’s a good chance for me to learn Polars, by simply translating a working notebook into another working notebook that uses Polars instead of Pandas. This will help me see what works well, and what doesn’t.Also, wow, there’s been a lot of excitement on LinkedIn about my use of graphs to explore music. I have to say, I am really enjoying this, and it’s nice to be playing with a kind of graph (notes, scales) that I have never used before. It’s fun to try new things. This just shows the versatility of Network Science, Natural Language Processing, and Data Science in general. Finally, obligatory AI art.Gotta have that scowl! The audience is cracking me up, too. Nice.What’s NextI’m going to continue with this kind of music + graph stuff for a while. I would really like to come up with something useful for learning scales. This blog isn’t just for teaching, it is for experimentation and learning.That’s All for TodayThanks for reading. I’m glad I was able to find something that would be fun to develop and write about. I’m excited to explore graphs musically. This is a cool topic.Thanks to everyone who has been following along with this series. Happy learning! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Artificial Intelligence and Machine Learning', 'Egocentric Network Analysis', 'Graph Algorithms', 'Graph Visualization', 'Subgraphs and Connected Components', 'Whole Network Analysis']"
/work/blog_data/150185262.day-38-of-100daysofnetworks.html,"I’m really excited to write this entry for #100daysofnetworks. I’ll cut to the chase. In Python Data Science, Pandas is a well known library for working with data. If you are doing Data Science with Python, there is a good chance you work with Pandas or have worked with Pandas. I use it all the time. Today’s post is an exciting one (for me, at least):It turns out, Polars seems to work ok with Networkx (Python Graph Library)It also turns out that Polars is much faster than Pandas for doing string lookups, which is common pre-processing of DataFrames to prepare them for Graph workI need to experiment more with Polars for use in Natural Language Processing and Network Science, because I am seeing impressive improvements to my workflows.But there’s more:I was on a podcast and talked for over an hour about Network Science and Systems thinking about a month ago, and the podcast is now edited and ready!So, let’s go!Introducing PolarsPolars is a an alternative to Pandas. If you are using Pandas, you should compare the performance of what you commonly do in Pandas against Polars, as I am demonstrating today. I don’t like the idea of using Polars and Pandas together. I resisted using Polars before, because Networkx has a function for nx.from_pandas_edgelist() for creating graphs, but I didn’t see any compatibility for Polars. So, I expected that I’d be doing a bunch of back and forth between Polars and Pandas, and I’d rather just use Pandas in that case.Yuki Kakegawa recently wrote a book called Polars Cookbook, which is really helpful for comparing Pandas to Polars (if you are familiar with Pandas). You can read my review of his book here. And you can buy a copy of his book here or on Amazon.I spent some time over the last few weeks familiarizing myself with Polars, and figuring out how to do some of my common workflows using Polars. Today’s code shows that.Yuki’s book is great, and it make today’s work very easy and fast for me. Thank you, so much! I wish your book complete success! It has really opened me up to the idea of using Polars more, or maybe even replacing Pandas with Polars.Cagematch! Pandas vs Polars!Today, I did a very simple comparison of Pandas and Polars, by timing them at doing the same tasks:Loading a file into a DataFrame (Create DataFrame)Loading a DataFrame edgelist into a NetworkX graph (Create Graph)Doing string searches on a DataFrame (Search and Retrieval)All three of these tasks are important in both Natural Language Processing as well as in Network Science and Social Network Analysis. Here’s why:You need to be able to load data to be able to do anything with it.You need to be able to create a graph to be able to analyze itSometimes, you need to do preprocessing on the data before using itThe results are really  impressive. You can get the code here to see the results.Loading DataFor this comparison, I loaded the largest dataset from #100daysofnetworks, the Arxiv Network Science dataset. The file is 34.6 MB, not an itty bitty toy dataset.Result: Polars loaded the data in almost 1/3 of the time Pandas took. Clear winner. 185ms vs 488ms.Creating a GraphFirst, you should see my original confusion when I was able to use a Polars DataFrame directly with Networkx. Check the notebook to see more. It appears that it actually DID work and looks good. However, I am still skeptical, so will be doing more validation, before I can trust it 100%. However, using a Polars DataFrame rather than Pandas, I was able to create a Graph, and you can see that this is not a toy graph. There are 91,659 nodes and 96,394 edges.Was it faster or slower loading Polars?Polars was barely slower. One tenth of a second difference, hardly noticeable. The first edgelist is a Polars DataFrame, and the second one is a Pandas DataFrame.Doing String SearchesOften, there is some preprocessing involved before creating a graph, such as filtering by a category, or removing junk. I often use string searches to do this. How do the two compare?The first edgelist is Polars, and the second is Pandas. Polars took 10.2 ms to do a string search on the dataframe, and Pandas took nearly 20x longer.When I saw this, I was simultaneously excited and filled with dread! I was excited, because I now have a much faster tool for a lot of my actual WORK work. This is going to have a real impact in my work and life.But it has me wondering:Should I rewrite my book using Polars for the second edition?Should I write a second book called “Network Science with Python: Polars Edition”Do I completely stop using Pandas today and force myself to learn Polars?Or do I just go slow and use Polars for the remainder of #100daysofnetworks, and gradually build skill?I am going to do the latter, and I will consider what to do about my book and future books. I absolutely don’t want to write a book that teaches Network Science using both Polars and Pandas.Polars Wins, Today! This was a quick comparison, for me to begin probing where I can make use of Polars. Turns out, it is much more usable than I expected for Network Science. I can create a Graph using a Polars DataFrame, and I can load and search data much quicker using Polars. However, there is a learning curve, so that is going to slow me down, temporarily.Polars currently seems much faster than Pandas for things I commonly do in NLP and Graph analysis, but I am still nervous about it’s compatibility with other things I will use. I will build trust and confidence over time.However, from here on, on #100daysofnetworks, I will be using Polars, not Pandas. I want to test this out, and this is a good place. Now, you’ll get to learn about Polars and Network Science!Polars is a viable option for Network Science with Python, or at least it is looking that way. I will do some more validation, as I didn’t expect this to work.One more time, thank you Yuki Kakegawa for writing your excellent book!Network Thinking PodcastAnother cool thing happened, recently. About a month ago, I did a podcast with my Packt buddy Ali Abidi. He has a podcast called “All About AI with Ali Abidi”. On the podcast, we talked for over an hour about some important topics, such as:Why I wrote my bookWhat the writing process was likeWhat is systems thinking (or network thinking) and why is it importantWhy systems thinking is important to cybersecurity (my domain) and other fieldsAnd much more…Here is the podcast! You can watch it here, directly!You can follow his podcast here, and you can connect with Ali here. He is a good friend of mine, and I really enjoy his posts. Finally, Music and ScalesFor the previous two articles, I wrote about Networks and Music, and build a very useful tool for comparing scales and for improvisation. I am really happy that there was so much excitement about these two posts. This new tool has really unlocked some new learning for me. It’s hard to explain. It feels like something has literally unlocked.I really liked this interaction I had with one of the readers/users of the tool.I love Network Science and Natural Language Processing because they give me tools that help me learn and help me make sense of the world and my own existence. I don’t like just programming. I have been coding since I was six years old. Code is not special to me. I code for outcomes.I wanted to build a tool that could help me with my inability to memorize music scales, and it became something that is both effective (this has really opened up for me) and fun (I can’t put my guitar down, lately).I wrote my book to help people understand and interact with the world. And I write these articles to keep the dream alive. Oh, and the blog just passed 500 subscribers! Nice!It’s been an all-around great week, both personally and professionally. Thank you, to everyone who is a part of my life, and to those who read my words.That’s All for TodayThanks to everyone who has been following along with this series. Happy learning! If you would like to learn more about networks and network analysis, please buy a copy of my book!",['Artificial Intelligence and Machine Learning']
/work/blog_data/152114000.day-39-of-100daysofnetworks.html,"Hi everybody. Thanks for your patience since my previous post. I had some personal life to deal with and needed some quiet. However, in the time that has passed, an explosion of music learning has happened with me, and I want to share about that today, as this blog and network analysis is responsible.Help Me Help You (And Others)Before I start, I have a favor to ask. I have written 38 of these articles already, as well as a book. Each one of these articles takes hours and hours of prep time, and I gave up every summer for over a year in order to write my book. So, if you can, please do help this blog grow, if you find the content interesting. If you can just tell one of your friends about it, even in passing, it’ll grow. If you know someone who is interested in Data Science, Natural Language Processing, Machine Learning, Artificial Intelligence, Graph Theory and Network Science, or even music, send them my way! I will be using social media less. I will not be posting on LinkedIn as frequently about this blog. If you can share my articles there and on other platforms, it also helps me. Please, help me help others. I am not doing this for money or recognition, I am doing this to share useful knowledge and skills. I’m happy with the growth, but I’m not going to push it on social media as much anymore. Scale Similarity GraphHere is the code for today.I have wanted to do what I am showing today for a while, because I already knew it was possible. I have done similar things with text and also using other matrices (correlation), and the visualizations are really useful. On Day 37, I showed how to create a Scale Similarity matrix and heatmap. This is great, but it is not as intuitive for use as a network visualization. Here is the heatmap, as a reminder.That is usable, but it is a lot of information packed into a small amount of space. It is useful, though. For instance, if I want to find a scale that is similar Harmonic Minor Mode 3, I can look for the darkest square on the same line that is less than 1. Then my eyes have to look all the way to the side without losing the line and find the similar scales. That works, but what a pain.Here’s the thing: you can build a graph using an adjacency matrix, and a similarity matrix will work, and so will a correlation matrix. But you have to set a threshold for it to work.That’s fine. Let’s say that I want to see the network of scales that are 80+% similar. What’s that look like?This is IMMEDIATELY useful to me as a musician, much more than a heatmap will ever be. What if I lower the similarity threshold a little, so that the rest of the network is mapped together? Here it is with the threshold set to 75%. I have never seen anything like this as a musician, and it immediately made my learning more enjoyable. I no longer needed to just repeat scales and hope for memorization to happen. I can now SEE that there are what I call families of scales. The closer the nodes (scales) are together, the more similar they sound.I can SEE what scales sound most similar. It’s like being able to see sound!So, I used this new knowledge and split up all the scales I was learning into four groups.After a lot of repetitious practice, I started to acquire systems thinking about scales. I could see them as families. I could practice them in sets of seven instead of more than twenty. Learning became fun and accessible, and memorable. It is easier to remember names when you have seven to learn from instead of dozens.For each scale, I mapped out the notes from the eighth fret, C.And with that, I practiced, practiced, practiced. I spent so many hours just experimenting, building skill and speed, learning names, and fusing together scales. This thing that always intimidated me became fun and memorable. Learning should be like that. Graph can help, but only if you use it.And I can also compare scales programmatically. Yes, this is gnarly code, but I don’t care. It is not going to be used in production. It is quick and dirty to get information.I can see that the difference between Harmonic Minor Mode 3 and Harmonic Major is:Harmonic Minor Mode 3 uses a A-flat and Harmonic Major uses a GHarmonic Minor Mode 3 uses an A and Harmonic Major uses an A-flatBut the rest of the notes are identicalAnd so on…Music Learning ExplosionThis is no exaggeration. Since Day 37, this Graph + Music combination has led to an explosion of music learning and inspiration for me. Other musicians have also told me that they are finding this useful (read day 37).And since this weird, awkward, intimidating thing became fun, easy, and useful, the floodgates of learning really opened.It has led me to:Really loving my time practicing scales, and internalizing the knowledgeGetting familiar with the entire fretboard, not just repeating old stuff I knowLearning to use triads in arpeggiosPulling my completely neglected 25 year old Mandolin out of the closet and learning how to play it a bit. I restored it, strung it, and now play it.Really fun conversations with my fellow musicians, showing how Data Science could be useful in creating tools for learning in new waysIt’s Why I Wrote My BookI didn’t write my book to teach you to be good at your job. I wrote my book so that you could learn to explore your own reality. I write to give you tools to enrich your own life. That is my why.I do this because I know this is useful, and I know that if you learn this, you will be good at your job, and you will have a more enjoyable life, if you use it. Science is about exploring and understanding reality and the human experience. Graph analysis gives higher level understanding than staring at numbers or a heatmap. A graph visualization is sometimes exactly what you need.Bear With MePlease bear with me. I ALWAYS feel like sharing knowledge, but I DON’T ALWAYS feel like writing, and I am not always in the right mindset for writing. I don’t always feel like writing, but I do always feel like playing/practicing music. My music learning does really well during those times, but I am not able to write about it. So, a long time sometimes goes by, between articles.And then when I do feel like writing about it, I don’t feel like promoting my writing on social media, taking screenshots and pasting them on various platforms, etc. I love sharing information. If you enjoy the information I share, please help me help you by helping promote my material. I don’t make money for this. It isn’t my job. It is just something I enjoy, and I love helping people learn useful skills.And I know that not everyone is a musician, so this blog has taken a bit of a turn. I told you in the beginning that there are many kinds of networks. Scale networks are one of them. Later, I’ll show you a cool way of exploring street networks. There is something for everyone in Graph and Network Science.What Are You Going to Do?I’ve now written 39 of these articles, showing many ways that Graph Analysis is useful. Hopefully, it has given you some ideas for using it in your own life and work. What are you up to? What are you using it for, or thinking about using it for? Have an idea and you need someone to hear you out? Do something with this!In order to find usefulness, you have to start using this, and learning from the insights you discover. Let me know if you get stuck. I challenge you to start playing with graphs on your own a bit. Please read my old articles or my book if you want to get back to the basics.That’s All for TodayThanks to everyone who has been following along with this series. Happy learning! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Artificial Intelligence and Machine Learning', 'Graph Visualization']"
/work/blog_data/153651477.day-40-of-100daysofnetworks.html,"Merry Christmas and Happy Holidays, everyone. I had a really nice time with my family and am enjoying some downtime, building up my energy for an exciting 2025. Today, I have an interesting post for you. There is a lot of knowledge contained in graphs (thus the term Knowledge Graph), but you must be able to get to it and interpret it in order for the insights to be discovered and made use of. For this article, I wanted to try to understand the convergence of concepts. In other words, what do Concept A and Concept B have in common? More specifically, I want to understand where the concepts of Network Science converge with Artificial Life (ALife).What is Artificial Life?I ran into the phrase Artificial Life while reading the book The Ascent of Information. I’ve known of researchers who have made simulations of life, but I never really thought of it much as an approachable topic of research until reading this book. Then I realized, I did my own Artificial Life simulation about six years ago when I was attempting to mimic a certain behavior of fireflies that I read about in the book Sync. Neat, I have already dabbled. I would like to learn more. Here is Wikipedia’s current definition:Artificial life (ALife or A-Life) is a field of study wherein researchers examine systems related to natural life, its processes, and its evolution, through the use of simulations with computer models, robotics, and biochemistry. The discipline was named by Christopher Langton, an American computer scientist, in 1986. In 1987, Langton organized the first conference on the field, in Los Alamos, New Mexico. There are three main kinds of alife, named for their approaches: soft, from software; hard, from hardware; and wet, from biochemistry. Artificial life researchers study traditional biology by trying to recreate aspects of biological phenomena.


== Overview ==
Artificial life studies the fundamental processes of living systems in artificial environments in order to gain a deeper understanding of the complex information processing that define such systems.  These topics are broad, but often include evolutionary dynamics, emergent properties of collective systems, biomimicry, as well as related issues about the philosophy of the nature of life and the use of lifelike properties in artistic works.This is very interesting. We hear so much about Artificial Intelligence, but not nearly as much about Artificial Life. In my opinion, it is useful to understand both, and the understanding of both will be used together. If you want to create an artificial life, you will need to give it an artificial intelligence. So, I’m not fond of AI getting all the attention, and I would like to learn more about ALife. Network Science has been helpful in learning more about ALife, and I have written about this on prior days.First, CodeHere is the code for today’s analysis. For this investigation, I used Polars instead of Pandas for any preprocessing, as I wanted to familiarize myself a bit more with the language. I am still a beginner with Polars, so please forgive any gnarly code. I am really impressed with the speed of Polars, and so far, I don’t see any reason it can’t replace Pandas for Graph Analysis work.Polars is very fast, and you can learn more here.Not an Ego GraphIf you’ve read my book or followed this blog, you have seen Ego Graphs before. In an ego graph, you can visually inspect the network that exists around ONE “ego” node. The nodes around it are called “alter” nodes. Ego graphs are very, very useful for cutting through the noise of any graph. We typically have an idea of what we are looking for in a graph, so an ego graph is a good place to start. And if we don’t know, then centralities or page rank is useful for identifying important nodes in any graph, and these central nodes can be inspected with ego graphs. Here is the ego graph for Artificial Life. The center node is the “ego” and all other nodes (dots) are called “alters”. Nice. That’s pretty useful. I see some interesting concepts that can be investigated:History of artificial lifeSynthetic biologyLife simulation gameQuantum artificial lifeArtificial ReproductionArtificial General Intelligence (AGI, yup)But, I can’t see where ALife converges with Network Science. I need something different than an ego graph for this.Here is the Network Science ego graph, for comparison.I can’t see any connectivity to Artificial Life. I need something else.Trying Something NewI don’t even know what to call this. Maybe a convergence graph? Perhaps, someone else has already given this a name. But I do know what I want, and you can see in the code how I got to it. But for simplicity sake, here is my methodology:Pull a wikipedia dataset using the Day 5 wikipedia edgelist builder.Build the graph using the dataset.Calculate the distance of every node to the Artificial Life node.Calculate the distance of every node to the Network Science node.Calculate the average distance between both.Use this average distance of both to attempt to see where topics converge.This above methodology is pseudocode. Without using my code, you can attempt to write your own code using this above methodology. But if you are unfamiliar, please follow along with my code. It is provided in each of these articles.How does this new topic convergence graph look?This is cool and exciting to me. You can think of it as similar to a Venn diagram, but much more complex. It takes work to understand, like any graph. But this is exciting to me, because I have never seen or done anything like this before. I can clearly see how Network Science, Artificial Life, and Artificial Intelligence connect. Here’s a mental tip: look for the regions. If you look at the middle left, most nodes have to do with Artificial Intelligence, but it is connecting to Network Science through the nodes KL-ONE, Efficiency, and Quantum Dot stuff.The Network Science region is on the top right. It is connecting to Artificial Life through the nodes relating to Artificial Chemistry and Artificial Photosynthesis. The Artificial Life region is on the bottom right and parts of the bottom left. It connects to Artificial Intelligence through Artificial Chemistry.This is cool discovery work. Here are all of those nodes from this graph:Artificial Life (journal)
Artificial chemistry
Artificial general intelligence
Artificial intelligence
Artificial life
Artificial life (disambiguation)
Artificial photosynthesis
Efficiency (disambiguation)
Efficiency (network science)
Efficiency Medal (disambiguation)
Energy Sciences Network
Frame (artificial intelligence)
Graph (discrete mathematics)
History of artificial life
Immune network theory
John M. Jumper
Joint Center for Artificial Photosynthesis
KL-ONE
Lawrence Berkeley National Laboratory
List of Nobel laureates
List of unsolved problems in chemistry
List of unsolved problems in mathematics
Network science
Network theory
Niels Kaj Jerne
Outline of artificial intelligence
Percolation (cognitive psychology)
Quantum artificial life
Quantum dot
Quantum dot solar cell
Semantic network
Semantic social network
Solar-cell efficiencyThese are all Wikipedia pages, and you can access them programmatically, as I have shown in the code. You can programmatically pull content and links from any of these pages. The maximum distance threshold can be increased to analyze a larger similarity graph. I have left labels off of this next one, but you can see that there is more complexity. You can play with it using my code.Train your eyes to look for “clumps” or clusters of nodes. These are small ecosystems with a lot going on. Also, I color nodes by their Page Rank score, so you can easily spot important nodes. This is a useful thing to do, but less useful without labels. What’s the TakeawayI get that graphs can be intimidating to look at if you don’t know HOW to analyze them. I’ve tried to simplify this for you, but you have to actually do the work to build the skill and confidence. You will if you do.For me, the takeaway is that there is a way to inspect where ideas converge, and we can learn from this knowledge. For instance, I know that Network Science fascinates me, and I am intrigued by Artificial Life. However, I don’t have ALife researchers banging down my door, begging me to do something with them. And ironically, the Network Science community doesn’t seem all that into networking. This above technique makes ALife accessible to me. It gives me a place to start. You find the first dot that catches your attention, learn about it, learn what concepts it leads to, and then follow them to the next thing.Concept > Concept > Concept > Concept > ConceptYou don’t begin at the end, as some kind of mad scientist capable of fusing Graph-enabled Artificial Intelligence to some Artificial Life organism. So, for me, that means I’d probably enjoy reading about Artificial Chemistry, Artificial Photosynthesis, the two people named in the graph, and learning about other nodes.This is UnlimitedThis is not limited to the two topics of Network Science and Artificial Life. You could do this with Country Music and Heavy Metal. I bet that’d lead to some interesting places. You can do this with any topics that interest you.And you are not limited to TWO. You could extend this idea to THREE or more. But the more you add, the more difficult it will probably be to analyze, maybe. Or maybe it will work really well. I don’t know! Try it!This has to do with understanding what we want to understand. It democratizes information. It highlights the unknown unknowns and makes them known. There were almost 7000 nodes in this graph, and we found the 33 that helped answer the research question of what connects Network Science to Artificial Life. What Are You Going to Do?I’ve written dozens of articles, showing many ways that Graph Analysis is useful. Hopefully, it has given you some ideas for using it in your own life and work. What are you up to? What are you using it for, or thinking about using it for? Have an idea and you need someone to hear you out? Do something with this!In order to find usefulness, you have to start using this, and learning from the insights you discover. Let me know if you get stuck. I challenge you to start playing with graphs on your own a bit. Please read my old articles or my book if you want to get back to the basics.That’s All for TodayToday is day 40, a milestone day! I was hoping to hit day 40 before 2025, and we did it! Thank you to everyone for following along. I hope some of you are learning from this, but learning can even just be awareness. This is possible. People do this. It is useful and relates to Artificial Intelligence, Artificial Life, reality, everything. Everything is connected, so the ability to analyze networks is a very important and powerful skill, and it only seems like magic because it isn’t popular. It is easy to do, with practice. It feels good and is always exciting and educational. Thanks to everyone who has been following along with this series. Happy learning! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Artificial Intelligence and Machine Learning', 'Artificial Life', 'Egocentric Network Analysis', 'Graph Algorithms', 'Graph Visualization', 'Whole Network Analysis']"
/work/blog_data/153741146.day-41-of-100daysofnetworks.html,"Hello, everyone! About a week ago, I was tagged in a LinkedIn post about Cosmograph. So, big thank you to Sergey Mastitsky for that! He mentioned that I should consider writing about Cosmograph for the second edition of my book.This really made my day! I really appreciate the callout!I spent a little time looking at Cosmograph, to get a sense if it would be useful in my workflows. Good news: it can be swapped right in over my previous method, and I’ll show you how!Here’s a few of Cosmograph’s cool demos:Cosmograph WidgetEnglish WordsIt was actually the “English Words” demo that really caught my attention. Cosmograph did quite well rendering a large and dense network in a notebook. That’s impressive and not common with Python libraries. So, I became curious. The demos made it look simple, so I gave it a try.Actually, this wasn’t obvious. It took a couple hours of figuring out, and the solution came to me when I inspected the example DataFrames, not the graph visualization code itself. Often, the problem is in the data itself.The only real problem that I ran into was that:In networkx, you can construct a graph with a DataFrame, and the ‘source’ and ‘target’ fields can be strings or integers.Cosmograph expects integers and doesn’t complain about strings. It just won’t work with strings. It’ll appear to be working but not actually work.So, I spent about an hour trying to figure out why none of the edges were showing, but all of the nodes were. Inspecting their DataFrame led me to the answer. So, there’s the lesson in troubleshooting things. Check the data.First, CodeToday’s code is available in Google Colab, and you can run the notebook directly. I have made it available for public use. Please give it a try and enjoy looking at the interactive graphs! Win, win!In the Jupyter repo for day 41, I link to the Google Colab notebook, so you can always find the link there as well.You can access the Google Colab notebook here. ← THIS IS TODAY’S CODE.To run the notebook, click “Runtime” and then “Run All”, or you can click through the individual cells.What’s the Workflow?The workflow for using Cosmograph isn’t very different than using Scikit-network with NetworkX, and I have created an identically named function “draw_graph” to show how it can be a direct replacement for how I have been doing graph visualizations. The process is simple:Load the dataCreate the ‘points’ dataframeCreate the ‘links’ dataframeVisualize the graphStep three took the most figuring out, as networkx will create a graph when nodes are strings, but Cosmograph will not. So, I needed to create a numeric edgelist, and then this began to work. See the code to understand.Finally, to be practical, I need steps 2-4 to be bundled up in a function that returns a visualization widget. If I do that, this will be usable as a replacement for what I already do, with some parameters tweaked.The draw_graph FunctionThe code to render the graph is about as simple as it is in my other method that I use with Scikit-Network, it’s just different. Here it is:There’s three parts to it:Create the points DataFrameCreate the links DataFrameRender the graphI have done it this way intentionally. In investigations, I work with networkx to do things, and then I typically render the part of the graph I want to look at. I don’t typically render a whole network, as that’s not very useful unless the graph has already been denoised. So, this function takes a networkx graph as input and then renders it, keeping things simple. I can pass in any graph, and this will render it. For example, this code:ego = nx.ego_graph(G, 'Network science')draw_graph(ego)I’m using networkx to pull the Ego Graph for the “Network science"" node from the full graph and then visualizing it.Here is another example:ego = nx.ego_graph(G, 'Artificial intelligence', distance=2)draw_graph(ego)In this example, I am doing something slightly differently in networkx, and then the draw_graph function does what it does.I could pass in any graph. I could pass in any toy graph from networkx, or I could pull out the communities from the whole graph and render those, or I could look at other ego graphs. The point is, this is flexible and suitable as a swap-in for analysis. It is ready and suitable for use for real investigations.Work in ProgressThis is a work in progress. By default, it does not look better than what I do now, but it is fast, and it is flexible. There are an overwhelming number of parameters that you can play with to tweak the visualizations. You can read about them here.I am currently showing my visualizations with a bold orange background. This happened out of frustration, when the edges weren’t appearing and I couldn’t figure out why. I was playing with all kinds of settings, trying to get edge colors to show, playing with opacity, etc, etc. But now, I kind of like it. I like that I can easily play with background colors. This feels like Halloween, and it looks alright too!That’s actually very good. That’s 7000 nodes, and it rendered almost immediately, and it is interactive. I can zoom in, zoom out, and drag things around. This is actually already an improvement, because this is zoomed out. But I need to see how high this can scale up, though it really doesn’t matter all that much. It seems good enough. Because if you apply community detection to a graph:A billion scale network becomes million scaleA million scale network becomes thousand scaleAnd so on, for the most part. All graphs are unique, but in general, I will do preprocessing to remove stuff before I attempt to visualize a graph. A I showed above, I will just visualize the part of the graph I want to look at, which is often just dozens of nodes out of thousands. So, the performance seems fine.And, it does look decent, zoomed in. It just needs work to look nice.As you zoom in, it shows more node names. It becomes a little more difficult to take nice screenshots while zooming in, so just play with the notebook. But here is another example:Challenges and OpportunitiesEvery new capability provides both challenges and opportunities, not just opportunities. These are just some that I immediately see on day one of using this library.Challenges:Everything takes time and effort to learn. Using this means using something I am less familiar with. It will slow me down for a bit, but that’s the way with learning. Sometimes, we slow down temporarily to go much faster.Everything is different. Nodes are called points. Edges are called links. Also, Cosmograph doesn’t work directly with networkx, and I need to familiarize myself with dozens of parameters I have no idea how to use and learn their nuances. Cosmograph isn’t playing nicely with my laptop. I will figure it out. If you have this issue, use Google Colab. Use my notebook as an example and create your own. You can read directly from 100daysofnetworks datasets on my github like I did in my Colab notebook.Opportunities:It really wasn’t hard to get working once I realized that I just needed to modify the edgelist a bit. This is easy to work with and opens up new doors.This is interactive. My other visualization approach is not. With this, you can “fly around” even complex graphs. That will probably impact how I think about investigations, and I’m not sure how, yet. It used to be more work to get to interactive, so I’d just stick to 2D, but this opens up new doors that I am not even able to imagine, yet. Just like I encourage you, I need to play with this to learn more of what it can do.There’s so much flexibility with the settings. You can have curvy edges or straight, tweak opacity, tweak node and edge colors, and so on.But first things first:I want to redo this notebook and see if I can get the graphs to render in an easy to read format, like my current visualizations. The visualizations in my book and previous blog posts are easy enough to read, and Cosmograph sometimes spasms, and the defaults don’t look great. Just needs some fiddling with parameters, I bet. It’s good enough, so long as I can make it more readable and suitable for print. That means, no spasms, and definitely crisp text.I want to also stress test Cosmograph, seeing how well it will renter 10,000 nodes, then 50,000, then 100,000 (if it makes it that far). But, as I said, it’s fast enough, because I’m typically passing in a subgraph or ego graph anyway, not visualizing 100,000 at once. I just want to see how far it’ll go before falling apart.There’s still a lot of unknowns. I’m still just getting started, but I wanted to share this. I wasn’t planning on writing today, but I was too excited after getting this to work and thinking about it. My mind was noisy, so I needed to write.What Are You Going to Do?I’ve written dozens of articles, showing many ways that Graph Analysis is useful. Hopefully, my writing has given you some ideas for using it in your own life and work. What are you up to? What are you using it for, or thinking about using it for? Have an idea and you need someone to hear you out? Do something with this!Begin where you are. In order to find usefulness, you have to start using this, and learning from the insights you discover. Let me know if you get stuck. I challenge you to start playing with graphs on your own a bit. Please read my old articles or my book if you want to get back to the basics.That’s All for TodaySpecial thank you to Sergey Mastitsky for letting me know about Cosmograph! I’m really grateful to my LinkedIn network and have learned a lot from my connections! Likewise, if you learn about or know of anything that can be helpful with #100daysofnetworks or graph analysis, please let me know, so I can play with it and possibly introduce others to it, like I am doing with this short article.Thanks to everyone who has been following along with this series. Happy learning! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Community Detection', 'Egocentric Network Analysis', 'Graph Visualization', 'Subgraphs and Connected Components']"
/work/blog_data/153873865.day-42-of-100daysofnetworks.html,"I am ALWAYS on the lookout for better graph visualization software. Since I began this journey in about 2017, I have gone through several iterations of finding better and better graph visualization software. There are several options, but very few that scale large enough to be useful on real-world networks. Today, I am introducing Cosmograph’s No Code solution for Graph visualization and inspection! I think this is a really important capability that will allow non-coders and other less technical folks to get the same enjoyment at inspecting network graphs.What Is This?This tool enables ANYONE to do interactive graph visualization and inspection. This tool allows you to visualize and interact with very large graphs. It also has some other cool features, such as the ability to extract and inspect a part of the graph (subgraph), separately, and a lot more. Read about it here.Why Does it Matter?This is really useful to me, because I don’t always want to programmatically analyze a graph. That takes a lot of work to do well. This doesn’t replace graph analysis, and it doesn’t really replace every other tool for graph visualization, either. It is a shortcut for whole network visual inspection. It can save you a lot of time. If you have graph data, you can upload it and inspect it in seconds.How Do I use It?This is very easy to use. I have generated a few files to get you started, using networkx’s graph generators. You can download the files here. You can also use any of the 100daysofnetworks datasets from previous days, though any that involve bipartite projection will require some preprocessing to get it into a suitable format for this tool. I recommend downloading one of these files (click the download icon on the right):les_miserables.csv: This is social network from the book Les Miserables. It is a very small graph, allowing you to inspect how this tool does with smaller datasets. It is also a bit more interesting than the other two files, as the nodes are characters, not just numbers.scale_free_100k.csv: Cosmograph says that it can handle million scale, but I wanted to see how well it could do with 100,000 nodes. This is already much more than many tools can handle. This is a scale-free network, which you can read more about here.scale_free_200k.csv: Cosmograph handled 100,000 nodes with ease, so I wanted to see how it could do with 200,000. No problem!To download a file, look for this download icon on the github page:Just save it somewhere that you can find for the next step.Show and TellFor the rest of this article, I will walk you through the first steps and give you some tips. I want to stay out of your way. I want you to get excited to play with this tool, and then I want to leave you alone to be creative and have fun.First, go to https://cosmograph.app/run/Bookmark this page. I have been using it all week on several different datasets and really enjoying it. Put it somewhere easy to access to make it easier for you to find.Next, click on the words “Load Graph”.Next, click “Select data file”. This can be any file that appears to be a network edgelist, but you are currently limited to .csv, .tsv, and .ssv.Next, just click “Launch”. You can check out the other options as you go.Now, wait about three seconds and the graph will appear on your screen.This is an interactive graph, so you can zoom in with your mouse. Also, look at the top right of this image. This is a graph of 100,000 nodes and 193,059, much larger than most toy graphs. This is bigger than many real networks. This tool is powerful. Let’s zoom in a bit.I can already see a key characteristic of scale-free networks. Most nodes have very few connections, and very few nodes have many connections. We can see an example if I zoom in on one of those blue blobs.And we should be able to see things a little better if I reduce the node size a bit. On the left, look for “node scale” and move the slider bar until it is about 0.3. Then it will look something like this:It looks like node 10731 might be the node that the surrounding nodes are connected to. I’ll show how to extract a subgraph a bit later. Please keep reading.This tool can handle large graphs, but let’s use the Les Miserables graph, as it is a bit more interesting and relatable, having to do with people and how they know one another. If I load the file, here is what I see.The structure is there, but it needs a little work. Let’s reduce the node scale to 0.3.Let’s also increase the “link scale” to 2. Push the slider all the way to the right.That’s a lot better, as I an more easily see the noes and edges.Inspecting Subgraphs / EgosOne feature that makes this tool so useful is that you can easily pull out parts of the graph for inspection. Just for review:A subgraph is a part of a graph. Imagine using a slicing tool and just grabbing the top 25% of the image. You would have selected a part of the graph, a subgraph.An ego network is a special kind of graph consisting of an ego (the node of interest) and the alters around it (who/what it is related to).I use ego graphs very, very often, so I will show how to capture one using this tool.First, click on the Valjean node.Notice that it has already faded out the non-related nodes in the background.Next, on the left side tools, click into Analysis and then click “Isolate active selection”.Then click the center icon on the top left. I have added extra red squiggles to make it easier for you to find. It isn’t obvious, and it is useful.Then zoom in and out and apply whatever node/edge formatting you want.That’s it. I don’t want to say more. I want you to play with this and let me know what you think. This opens up graph visualization and investigation to non-coders and less technical folks. This democratizes graph analysis and makes it more accessible. What Are You Going to Do?I’ve written dozens of articles, showing many ways that Graph Analysis is useful. Hopefully, my writing has given you some ideas for using it in your own life and work. What are you up to? What are you using it for, or thinking about using it for? Have an idea and you need someone to hear you out? Do something with this!Begin where you are. In order to find usefulness, you have to start using this, and learning from the insights you discover. Let me know if you get stuck. I challenge you to start playing with graphs on your own a bit. Please read my old articles or my book if you want to get back to the basics.That’s All for TodaySpecial thank you to Sergey Mastitsky for turning me on to Cosmograph! I’m really grateful to my LinkedIn network and have learned a lot from my connections! Likewise, if you learn about or know of anything that can be helpful with #100daysofnetworks or graph analysis, please let me know, so I can play with it and possibly introduce others to it, like I am doing with this short article.Thanks to everyone who has been following along with this series. Happy learning! If you would like to learn more about networks and network analysis, please buy a copy of my book!","['Graph Visualization', 'Subgraphs and Connected Components', 'Egocentric Network Analysis', 'Whole Network Analysis']"
/work/blog_data/172905375.day-43-of-100daysofnetworks.html,"Alright, I have wanted to write a post for #100daysofnetworks for months and months and months, and no matter what, something was just always in the way. This year has been a challenging year, but I don’t want to dwell on that. Let’s get this going.What have I been up to?I have been quiet for way too long, and I have had a very active year. Even though I was not writing, I was busy being impacted by what I have learned during #100daysofnetworks. In particular, I previously wrote two very creative articles, showing how Natural Language Processing, Graph Theory, and Music can be fused together:If you are a musician, you should definitely read those. Even if you are not a musician, pay attention to the notion that Natural Language Processing can be useful even beyond language.These two articles literally ignited something in my musical learning. The outcome of these articles was that I had a clean graph of musical scale similarity, and I could use that to play with scales:Which scales are similar enough with the minor scale that I can swap them in?Which scales go with major chords?Which scales go with minor chords?When you are learning guitar, you do not always have a teacher accessible to you 24/7, so this was a cool experiment that ended up unlocking lead guitar for me, and then after that it led me back to folk roots, where I began to learn about Travis Picking and Clawhammer Technique. I got so into the learning that I made a YouTube Channel and added a lot of videos that helped me learn. I added 195 videos to that channel, showing how into the learning I got.That explosive learning came from graph analysis. It did not start with picking up a guitar or flipping through my scale book. It came from building systems thinking around musical scales.Really, that’s most of what I have been busy with this year, in terms of learning. It was a musical year for me, and I was unable to write, until now. So, I am excited to be back at it.Please Support this BlogI would like to make a special request in this article. This blog has over 600 subscribers. I have written over 40 articles. Each article typically involves about four hours of research and development, so that’s about 200 hours of valuable work and writing that I’ve provided for free, because most important is that I want people to learn this. I am not doing this to make money.However, these days, there are things that I would like to do. For instance, to play with GraphRAG for AI, it is useful to have access to a Graph Database. The cheapest tier Neo4j instance is about $800/year. I would like to work on GraphRAG and write about it so that you all learn, but I cannot do that without support. So, I have opened up a few ways for you to support this blog:If you are a subscriber, please consider converting to a paid subscriber. I provide code, data files, and coding explanations that are absolutely worth more than $8 per month. But I understand that not everyone can afford to pay, and that’s fine. Free is absolutely fine, for those who need free.If you are a paid or unpaid subscriber and you want more flexibility in your contributions, I set up a ko-fi account. CLICK HERE. You can use this this to buy me a coffee ($5 donation) or even to pitch in for a Neo4j Aura instance, which will enable more writing and learning.AI Plus GraphIt’s known that graph is useful in AI stuff, but AI is also useful in graph stuff. :)For instance, my latest research is into:Can a prompt approach create better edgelists from text than previous approaches such as Named-Entity Recognition (NER) or Part-of-Speech Tagging?Can AI agents be useful in providing a chat interface exploring and analyzing graphs?The answer to both, I think, is yes. I have already begun the work. In fact, I already have a few of these AI-based edgelists and we will be analyzing them on an upcoming day, to validate how well the AI did in determining edges.The agentic approach is also very cool and will be useful. Here is a preview. Notice that I am interacting with language.What is Coming?Part of the challenge in the last year was that AI was making moves that I couldn’t keep up with, and it was hard to write about it. Things seem more stable now, and we are in a time of usefulness. The previous image shown is an example. Being able to talk to a graph can make graph analysis much easier. AI can be useful for accessibility.So, we’re going to make some pivots with this blog. I’ve already covered the basics a lot, and they’ll always be a part of any analysis, but moving forward, we are going to dive into:Graph Machine LearningArtificial IntelligenceAgentic Graph Analysis and ExplorationPrompt-based Graph Analysis and ExplorationGeospatial Analysis (Street Networks)We’ve covered the basics well enough and it’s time to have a lot of fun. We will start with Graph Machine Learning. That will be the topic of the next post.If you want to prepare for the next article, pick up a copy of this book and start reading. I am several chapters in and I love it. I also read the first edition of the book and loved it as well. I don’t know exactly what changes they have made in the second edition, but it is great. If you enjoy this blog, you will probably enjoy this book.My BookI am also actively working on kicking off some book projects. I have a few in mind. Please let me know in the comments what sounds most useful to you.Second Edition of Network Science with Python: My book was published in 2023, so I would like to modernize the code a bit and add a few chapters on temporal network analysis and artificial intelligence.Network Science with Python Cookbook: This book would be a more tactical reference book than my original book. My original book gets you into the mindset of how I do what I do and how it can help you. But I would like to put together a tactical desktop reference book to supplement my book.(Graphs: Zero to Hero): I am considering writing a book similar to my first but less about converting text into graphs and more about starting with graph data. Imagine you knew nothing about graph analysis and someone gave you an edgelist. “Oh shoot, what do I do now?” This book would cover that.Source Code Analysis and Data Observability: I am considering writing a book on what originally led me to Network Science in the first place. This would be extremely useful to Data Operations Engineers and Site Reliability Engineers. It led our team at Intel to being able to troubleshoot mystery outages from days to minutes. This is a useful skill that I eventually need to write more about.But please do buy a copy of my book if you haven’t already.Need Any Help?Finally… there’s no fun way to say this. I am out of a job and need to find work. If you work in Data Operations, Data Engineering, Cybersecurity, Data Science, or do anything with Artificial Intelligence or Machine Learning and you think I can be of use to your company, please reach out. I need full-time work with benefits. I need a good problem to help solve. I enjoy collaborations, but I need to pay mortgage and expenses. I am very good at what I do, and I am very good at making companies more effective. I’m a very friendly person and a good teammate, so let me know if you think of anything. Or pitch in and buy a coffee to support this writing.These Will Be More Frequent100daysofnetworks was originally something I did daily, back in 2020. This is the second iteration. Life challenges have gradually slowed me down, to the point that this is the first new article since January. I don’t think I’ll get this back to daily, but these are going to be more frequent from now on. I will try to do at least one a week, and maybe more. Showing support would motivate me. I’m very excited to get back to this, back to learning, and back to being creative.LET’S GOOOOOOOOO!Chappie (ChatGPT) drew us a picture for this post. I let it proofread. I do not use it for writing. My words are my own.Alright, with this. My day is complete. I am heading out. If you would like to buy me a coffee, here you go. And come say hi in the Substack comments!","['Artificial Intelligence and Machine Learning', 'Temporal Network Analysis']"
/work/blog_data/172971472.day-44-of-100daysofnetworks.html,"Hi everybody. I am so glad that this blog is back in action, as I have been working on some exciting things, and now I get to talk about it. Related to graph analysis and AI, I have two things going on, simultaneously:I’ve built a prototype “Text to Edgelist” tool, and in this article I’m going to describe it and how it can be useful.I’ve built a prototype “Agentic Graph” tool, for exploring and investigating graph networks in your own native language.These two things are using two different approaches. The first is using a pretty vanilla prompt-based approach with AI, and the second is relying on AI agents. So, if you want to learn AI engineering, you are in the right place, and you should subscribe to this blog. Because I will show how to use both approaches. Deepnote is Awesome!In the last year, I have made one very significant change to how I do analysis. In the past, I’ve used Jupyter, and then if I needed to collaborate, I’d use Google Colab. However, Google Colab is annoying and simplistic, in my opinion. It bothers me to have to reupload files every time a notebook shuts down, and the collaboration capabilities are inadequate.Last year, I found out about Deepnote, and it is my favorite Data Science “notebook” tool, right now. They are not a sponsor of this blog, but I sure wish they were. :)Their plans are simple and affordable.Even though I am currently unemployed, I pay for the Team Plan. Because with this, I have what I need to do freelance work, or to share notebooks with collaborators, and I can use their more powerful machines. This article isn’t for advertising them, though. So, that’s enough praise. But do check them out. Deepnote is a very useful Data Science tool, and it has replaced Jupyter, for me.So, today’s demo has screenshots from my Deepnote. You can see the kinds of things I work on.What is this? Text to Edgelist?So, what is this text to edgelist thing? It’s simple. Humans use language to communicate things.“Mary had a little lamb, its fleece was white as snow. And everywhere that Mary went, that lamb was sure to go.”What lifeforms and relationships do you read from the text?Mary is in this settingA lamb is in this settingMary possesses the lamb in some wayThe lamb follows Mary aroundYou can learn about relationships from language, and Artificial Intelligence can, as well.Previously, if you wanted to extract entities (people, places, organizations, etc) from text, you would download and use a NER model, such as one of spaCy’s.With this approach, you do not need to download and use a spaCy model. You can use any LLM, and because many LLMs are multilingual, that also simplifies things.Instead of downloading one of these models for every language you need to support:You could just use this one single workflow. So, this is much simpler, and also less bloated. Those spaCy models are large, and if you are using them in expensive workflows, then they affect the scaling of those workflows. So, this simpler workflow could save a company thousands of dollars, if they used it.The idea is simple:The input is text, any text written in any human language. This is multilingual, so the text could be English, Japanese, Chinese, Russian, Slovenian, whatever. The input is text, any text. That is powerful by itself.The output is an edgelist that contains nodes, reasoning for why the edge is included, and example text from the source text.What that edgelist, you can build and analyze a graph. You can learn about the relationships that exist in the text, who knows who, how they know each other, etc.Eventually, I may do one or more of these things:Put it behind an API and make the API available to paying subscribersPut it behind a UI and make the UI available to paying subscribersBuild a company and make this into a simple productBut you can do this too, so learn from me. What I am showing you is much more valuable than the subscription fee for this blog costs, so please subscribe.Let’s See ItAlright, let’s do some show and tell. I’ll keep it simple, and we will look closer on upcoming days. Sorry, this is not in Jupyter. This is in Deepnote. So, I will share screenshots, today.The setup is simple. Use these libraries:Connect to a GPT model of your choice:Create a simple function for making your prompt calls. Call it whatever you want. I didn’t put much thought into this name. My current prompt is simple, and it is not final. This is rough form, non-optimized, non-validated.setup = """"""
Extract an edgelist of relationships from any text.

Rules:

Include only named human, human-like entities, and named animals (exclude objects, places, unnamed groups).

Add an undirected edge if two individuals interact, appear in the same scene/sentence, or one references/thinks of the other.

Resolve vague mentions if possible; otherwise keep as written (e.g. “Her Sister”).

Include all valid entities.

Use one consistent name per entity, capitalize Mixed Casing.

Each row should have a one sentence explanation of why it was included. 

Also include a small snippet from the text supporting the reasoning.

Output:

[(""Name1"", ""Name2"", ""Reasoning"", ""Snippet""),
 (""Name3"", ""Name4"", ""Reasoning"", ""Snippet"")]

Return only the edgelist. You must return valid output in the format specified.

Text follows:

""""""And for now, I use this code to use the above:import re
import string

for chapter in list(chapters.keys()):

    print('Running chapter: {}'.format(chapter))

    # keep letters, digits, punctuation, and whitespace
    text = chapters[chapter]['text']
    text = re.sub(f""[^{re.escape(string.ascii_letters + string.digits + string.punctuation)}]"", "" "", text)
    text = "" "".join(text.split())

    #print(text)

    edgelist = ask_chatgpt(setup, text)
    edgelist = ast.literal_eval(edgelist)

    if len(edgelist) > 0:

        edge_df = pd.DataFrame(edgelist)
        edge_df.columns = ['source', 'target', 'reasoning', 'support_text']

        print(edgelist)

        G = nx.from_pandas_edgelist(edge_df)

        chapters[chapter]['graph'] = G

    chapters[chapter]['edgelist'] = edgelist # empty or loaded is fineThis is the line to pay attention to:edgelist = ask_chatgpt(setup, text)In that line, the input is the chapter text from Alice in Wonderland or Through the Looking Glass, and the output is an edgelist.You can see that I’m doing other useful stuff with it:I’ve created a graph using the text of every chapterI’ve captured the graph and the edgelist in a Python dictionaryMore on this stuff later. Ask questions if you have any!Let’s InspectI’ve made several datasets available as of today. Go to https://github.com/itsgorain/100daysofnetworks/tree/main/data/genai. Let me describe these:Combined: Alice in Wonderland + Through the Looking GlassAlice: Alice in WonderlandLooking Glass: Through the Looking GlassGPT: The model that was used in creating the edgelistSo, that is six edgelist files created by AI using Alice in Wonderland and Through the Looking Glass. These are killer graphs to explore, and you can also learn about AI reasoning, and you can also do a project to learn about the completeness that AI provides in terms of reading comprehension. These are useful datasets for NLP and AI research. PLEASE SUBSCRIBE TO THIS BLOG. This stuff is valuable and I have done the heavy lifting, making it available for you to do actual AI research and have a successful career.Let’s inspect a little bit of the data. Here is an example chapter:[('Alice',
  'Guard',
  'Alice and Guard interact on the train when he demands her ticket and looks at her angrily.',
  'Tickets, please!... Show your ticket, child!... looking angrily at Alice.'),
 ('Alice',
  'Gentleman In White Paper',
  'Alice and the Gentleman In White Paper speak directly when he advises her about return-tickets and she refuses.',
  ""'Never mind what they all say, my dear, but take a return-ticket...' 'Indeed I shan't!' said Alice.""),
 ('Alice',
  'Goat',
  'Alice shares the carriage with the Goat, who comments on her and whose beard she grabs in fright.',
  ""'A Goat... said...' and 'she... caught at... the Goat's beard.'""),
 ('Alice',
  'Beetle',
  'Alice is in the same carriage as the Beetle, who speaks about sending her back as luggage.',
  ""'There was a Beetle... 'She'll have to go back from here as luggage!'""),
 ('Alice',
  'Horse',
  'Alice and the Horse are in the same carriage when it reassures the passengers about jumping the brook.',
  ""The Horse... said, 'It's only a brook we have to jump over.'""),
 ('Goat',
  'Gentleman In White Paper',
  'The Goat is seated next to the Gentleman In White Paper, placing them together in the same scene.',
  'A Goat, that was sitting next to the gentleman in white...'),
 ('Goat',
  'Beetle',
  'The Beetle is sitting next to the Goat, indicating co-presence in the carriage.',
  'There was a Beetle sitting next to the Goat'),
 ('Alice',
  'Gnat',
  'Alice and the Gnat converse at length under the tree about insects and names.',
  ""the Gnat... 'then you don't like all insects?' 'I like them when they can talk,' Alice said.""),
 ('Alice',
  'Rocking-Horse-Fly',
  'Alice observes the Rocking-Horse-Fly that the Gnat points out, placing them in the same scene.',
  ""you'll see a Rocking-horse-fly... Alice looked up at the Rocking-horse-fly with great interest""),
 ('Gnat',
  'Rocking-Horse-Fly',
  'The Gnat identifies and describes the Rocking-Horse-Fly to Alice.',
  ""'half way up that bush, you'll see a Rocking-horse-fly' ... 'Sap and sawdust,' said the Gnat.""),
 ('Alice',
  'Snap-Dragon-Fly',
  'Alice looks at the Snap-Dragon-Fly above her head after the Gnat directs her to it.',
  ""Look on the branch above your head... there you'll find a snap-dragon-fly.""),
 ('Gnat',
  'Snap-Dragon-Fly',
  'The Gnat points out and describes the Snap-Dragon-Fly to Alice.',
  ""there you'll find a snap-dragon-fly... its body is made of plum-pudding...""),
 ('Alice',
  'Bread-And-Butterfly',
  'Alice observes the Bread-And-Butterfly crawling at her feet and asks about its food.',
  'Crawling at your feet... you may observe a Bread-and-Butterfly.'),
 ('Gnat',
  'Bread-And-Butterfly',
  ""The Gnat describes the Bread-And-Butterfly and answers Alice's questions about it."",
  ""Its wings are thin slices of Bread-and-butter... 'Weak tea with cream in it.'""),
 ('Alice',
  'Fawn',
  'Alice and the Fawn speak and walk together through the wood until it recognizes her as a human child.',
  ""'What do you call yourself?' the Fawn said... 'So they walked on together...'""),
 ('Alice',
  'Tweedledum',
  'Alice thinks about visiting Tweedledum and follows finger-posts to his house.',
  ""TO TWEEDLEDUM'S HOUSE... 'I’ll just call and say how d'you do?'""),
 ('Alice',
  'Tweedledee',
  'Alice thinks about visiting Tweedledee and follows finger-posts to his house.',
  ""TO THE HOUSE OF TWEEDLEDEE... 'I’ll just call and say how d'you do?'""),
 ('Alice',
  'Dash',
  'Alice references a named dog Dash while imagining advertisements for lost animals.',
  'answers to the name of Dash: had on a brass collar')]That is plenty to get a preview for what the AI is doing, how it is evaluating, and what is in the example text.I am really excited to play with this on languages other than English. This is going to be very useful.What Are the Uses?There are several uses for this kind of workflow, but it boils down to converting unstructured data into useful data. Depending on what your business does, you will have different uses, and you may want an alternative to this approach, to build different kinds of graphs than flow graphs.For instance, if the prompt were altered, then this could be useful for converting source code into dataflow maps. That’s not much of a stretch, and I know how.And I wish so much that computational humanities and the social sciences would pay attention to this research. This stuff could supercharge your efforts into understanding literature/art, and in understanding society.Do some research. Ask GPT.“What are the business applications of converting unstructured text into an edgelist or graph representation.”And then read this book.And then read mine and this blog, learn, practice, and get good. Use this and provide value to yourself, your organizations, etc.While I am building this out using literature, this has very serious uses in general. This is positively useful in cybersecurity, risk, software engineering, data operations, and definitely the social sciences. Computational humanities should learn from me.Need Any Help?Finally… there’s no fun way to say this. I am out of a job and need to find work. If you work in Data Operations, Data Engineering, Cybersecurity, Data Science, or do anything with Artificial Intelligence or Machine Learning and you think I can be of use to your company, please reach out.I need full-time work with benefits. I need a good problem to help solve. I enjoy collaborations, but I need to pay mortgage and expenses. I am very good at what I do, and I am very good at making companies more effective. I’m a very friendly person and a good teammate, so let me know if you think of anything. Or pitch in and buy a coffee to support this writing.Please Support this BlogI would like to make a special request in this article. This blog has over 600 subscribers. I have written over 40 articles. Each article typically involves about four hours of research and development, so that’s about 200 hours of valuable work and writing that I’ve provided for free, because most important is that I want people to learn this. I am not doing this to make money.However, these days, there are things that I would like to do. For instance, to play with GraphRAG for AI, it is useful to have access to a Graph Database. The cheapest tier Neo4j instance is about $800/year. I would like to work on GraphRAG and write about it so that you all learn, but I cannot do that without support.So, I have opened up a few ways for you to support this blog:If you are a subscriber, please consider converting to a paid subscriber. I provide code, data files, and coding explanations that are absolutely worth more than $8 per month. But I understand that not everyone can afford to pay, and that’s fine. Free is absolutely fine, for those who need free.If you are a paid or unpaid subscriber and you want more flexibility in your contributions, I set up a ko-fi account. CLICK HERE. You can use this this to buy me a coffee ($5 donation) or even to pitch in for a Neo4j Aura instance, which will enable more writing and learning.And no matter what, if you are here, please buy and read my book. I am working on more book projects, as mentioned on Day 43.Oh, and please participate in comments. I am a friendly guy. It is lonely in the comments and always weird to me that people don’t seem to want to talk about this stuff. Why not? What is on your mind? Have any cool ideas you want to brainstorm? Don’t be intimidated, for sure. Be creative instead.Chappie (ChatGPT) drew us a picture for this post. I let it proofread. I do not use it for writing. My words are my own.In the next article, we will be analyzing these edgelists, and getting started with Graph Machine Learning. Read this if you want to read along.Have a great day, everybody! We are getting to the fun stuff, now! Enjoy the data! Play with it! Learn from me! Experiment. And then add your new skills to your resume. Do projects you care about. Try using this, or let me know if you want a custom edgelist.",['Artificial Intelligence and Machine Learning']
/work/blog_data/173121122.day-45-of-100daysofnetworks.html,"Hi everyone! It makes me so happy that #100daysofnetworks is back in action. I’ve written two articles in the last two days, and the momentum is there to continue this pace. There is a ton to write about, and I enjoy writing.Today, I want to write an article for our less technical readers. I’m writing about a technical topic (Prompt Engineering), but I want to talk about the reality of what this does, what it means, what it looks like, and how to do it.Software engineers are able to pick up prompt engineering quickly and easily, but AI conversations still seem to be full of handwaving and mysticism. This is a no-nonsense guide on what you can do with AI, and how. I am a salty engineer, not a hype salesman.What is Prompt EngineeringYou’re going to hear the phrase prompt engineering a lot, until it goes away. It is not the same thing as software engineering, or electrical engineering, or mechanical engineering. It has to do with creating a prompt, which an AI will use to hopefully give you what you want and need.You can think of prompt engineering as being a subset of software engineering. It fits inside software engineering. It is not a replacement for software engineering. For instance, take a look at this image:This image actually shows an interesting phenomenon, if you know to recognize it. This code shows that software engineering currently lives at a weird space where both natural language and code is being used to reach outcomes.In this case, the prompt is: “Please give me Alice’s ego graph edgelist.”The word “please” is optional, and I could have saved a token by leaving that off, but I am a polite and kind person by default. You can write like a dictator, or you can write like a human, and it should still work.Before that moment, before I wrote that line of code:response = agent.run(""Plese give me Alice's ego graph edgelist."")Literally, before that moment happened, this would have to be programmatically done, and my book and all former blog articles show how. After that moment, the universe shifted, and we can now use AI to explore graphs. That is the transformative power of the combination of Data Science and Software Engineering.In short: A prompt is instruction to do something. Sometimes it can be descriptive, and sometimes it can be concise. When you interact with ChatGPT or your favorite, you are constantly giving it prompts, which it responds to.Learn More about Artificial IntelligenceWe will continue in a moment, but before we do, here is a book that I think can help you. It is a high level strategic book on using Artificial Intelligence. Check out The AI Value Playbook.I was reading this right before I decided to write this article. Reading it reminded me that I should include it in this article, as this article is especially for non-technical readers in the AI space. Today, I enjoyed reading Chapter 3, an interview with Sam Liang, CEO of otter.ai.Just check it out. I recommend it. It’s an easy read, and I treat it as a quick injection of inspiration, hearing how others have used AI, what challenges they face(d), etc.Back to Prompt EngineeringOk, so, why does a non-technical reader need to know about Prompt Engineering? You need to understand that the act of coding itself is in a transformation phase. Look at the code above, again. Previously, we didn’t use human language in code. Now we do, and a lot.And it is changing beyond this. A subset of coders are getting into vibe coding, which I do not have much of an opinion of yet other than I’m currently uninterested. In vibe coding, it’s even more hands off. A software engineer tells AI what code to create and how, and then the AI is left alone to do that. Supposedly, people are doing a lot neat stuff, but I am not sold on that idea yet, as the cybersecurity implications freak me out.So, code is changing. Skillsets are changing. Even the kind of coders you need will change.You need:Engineers who understand information itself. How it is manufactured, the information competition that exists, how information can be split into streams, etc. I currently call these informaticians. I have no good word for it, other than that these are more aware and enlightened engineers.Engineers who remember the old ways. They know how to build reliable and secure software. You cannot safely rush into the future with no concern for anything. What happens when your vibe coded app deletes your database and you have no backups, or writes to a public S3 bucket, because some code in its training data told it to?In short, you need both the futurists and the traditionalists. We are in a time of transition, and AI is not “stable” in terms of safety. Use it responsibly, and hire people who know how.Speaking of which, I am available, if anyone needs an AI Engineer. Contact me.No Doom and GloomBe excited about this. I am not a fearmonger. I just tell it like it is. You don’t need to be afraid. You need to understand that things are changing, and it’s not too chaotic to keep up. If you stay outcome driven, it’s really not that noisy, because you care about outcomes, not hype or noise.So, what’s this look like, in action?Quick Tour of Prompt EngineeringFirst, I am able to share my notebooks. I already provide code for #100daysofnetworks. However, if you would like to have a one-on-one training session on anything whatsoever that I write about, message me. We can work something out. But in this article, I will just show screenshots and describe what we are seeing. I will be using OpenAI for this guide, but use whatever you like.First, you do need to get an API key, to do this. So, grab one from your provider. ChatGPT will tell you how to do this, if you use OpenAI. These Python libraries are used.Setup is simple:In the top box, I created a general purpose function that takes instructions and text as input and returns results as output.Input: instructions for processing the textInput: the text to processOutput: the processed text’s outputSimple. Inputs lead to outputs. Input → AI → OutputAnd from there, you use the AI to get what you want, which I will show next.That is as technical as we are going to get in this article, and that is enough for anyone to use as their introduction to prompt engineering.Let’s Do Prompt Engineering!There are two common things I use Prompt Engineering for:Do something specific (summarize, translate, identify key themes)Do data enrichment or data miningIn the first, the instructions will be simple, such as “translate and summarize the given text into a one paragraph summary while retaining completeness and accuracy.”In the latter, I’ll tell the AI to return the answer in a format that I can convert into JSON or a Python list, and then I’ll use that list in a downstream action, programmatically.The first is simplest, and that’s where you should start as a beginner. Do this as long as you need to, to build skill. If you do this, use the ‘gpt-4o-mini’ model as it is really inexpensive. You can build prototypes for dollars, not hundreds of dollars. Get good for cheap. Then use gpt-5 or higher when your prototype is ready for use. More guidance in another article. Ask questions in the comments if you have any.The ‘text’ input for our prompts is a character list from Alice in Wonderland.You can see it says “text = “. That is the hint. There are more. I stopped the screenshot at the letter K.Do Something SpecificHere is the first prompt. Read it and understand.instructions = """"""
I will give you a list of nodes from a Graph. Please tell me which nodes you think are duplicates and why.

Answer in this format: node1, node2, reasoning

Text follows:

""""""This is actual code that I use. The hint is instructions = “““. I have shoved textual instructions into an ‘instructions’ variable, and I’m going to pass that to the AI as input along with the text.answer = prompt_gpt(instructions, text)That is how I do it. In plain English, the answer is the output of AI after I give it instructions and text. That is how you can read my code. Or put another way.output = function(input1, input2)If I run this code, what does AI tell me?King Of Hearts, King of Hearts, capitalization variant of the same title
Queen Of Hearts, Queen of Hearts, capitalization variant of the same title
Knave Of Hearts, Knave of Hearts, capitalization variant of the same title
William The Conqueror, William the Conqueror, capitalization variant of the same name
The Duchess, Duchess, same character with/without the definite article
The Caterpillar, Caterpillar, same character with/without the definite article
Bill, Bill the Lizard, “Bill” is shorthand for Bill the Lizard (same character)
Cook, Duchess's Cook, the Cook is the Duchess’s cook (same character)
King, King of Hearts, in Wonderland “the King” refers to the King of Hearts
Queen, Queen of Hearts, in Wonderland “the Queen” refers to the Queen of Hearts
Alice's Sister, Her Sister, “her” refers to Alice; both denote Alice’s sisterThis is excellent. This tells me that there are some duplicate nodes that I can clean in my graph, giving me a better representation of the thing that I am analyzing. I am using Alice in Wonderland. This could be people, or malware, or businesses. Graphs hold relationships of things. This stuff is versatile. I teach it with non-threatening subjects, such as literature, but this has very serious uses.There are over a hundred nodes in this network. It would have taken me quite a while to do this manually, and AI helped me with this in seconds, and it probably cost me a couple pennies.Do Data EnrichmentGetting answers is great, if that is what you need, but I come from software engineering and cybersecurity. I want AI to give me answers in a format that I can use programmatically. Then I have options:I can persist that data for later use and analysis.I can use that data right now for something.That is important to understand. You can store the outputs of AI and use them for other downstream tasks. This is data enrichment. You start with one data source, you pull valuable stuff out of it, and you use the valuable stuff downstream.This next example is exactly the same as the previous one other than that I am getting the data in the specific format that I want. This of it this way. AI is very creative, but you can get it to be rigid and give tight answers. So, this prompt is slightly different than the previous one. Look for the difference on your own. Get comfortable.instructions = """"""
I will give you a list of nodes from a Graph. Please tell me which nodes you think are duplicates and why.

Return a list of tuples in this format: (node1, node2, reasoning)

Return only the list of tuples.

Text follows:

""""""Do you see it?It’s this stuff:Return a list of tuples in this format: (node1, node2, reasoning)

Return only the list of tuples.The original prompt has been modified and extended for a more specific kind of output. I didn’t have to use lists and tuples. I could have used a Python Dictionary, or some other format. Nobody told me I had to write it exactly this way. You are not going to get arrested if you accidentally say please in a prompt or it doesn’t work exactly as you want it the first time. You have to practice and iterate and experiment. You have to be creative and tenacious, and you have to learn.Let’s see another example and use the data for a downstream task. Here’s the next prompt.instructions = """"""
I will give you a list of nodes from a Graph. Please split them into factions that you know about from the story.
Infer the stories from the character names provided.

Return a list of tuples in this format: (faction, node, reasoning). 

Please sort and dedupe the nodes in each faction.

Return only the list of tuples.

Text follows:

""""""Don’t rely on me to show everything. Try to imagine the outputs. Read it again until you understand.If I use this prompt, I get this output:[('Wonderland - Core/Encounters',
  'Ada',
  'Name Alice fears she has become after changing size'),
 ('Wonderland - Core/Encounters',
  'Alice',
  ""Protagonist of Alice's Adventures in Wonderland""),
 ('Wonderland - Core/Encounters',
  ""Alice's Sister"",
  'Girl on the riverbank in the frame story'),
 ('Wonderland - Core/Encounters',
  'Caterpillar',
  'Hookah-smoking creature who advises Alice'),
 ('Wonderland - Core/Encounters',
  'Cheshire Cat',
  'Grinning cat who appears and vanishes'),
 ('Wonderland - Core/Encounters',
  'Her Sister',
  'Ambiguous; likely refers to Alice’s sister in the frame'),
 ('Wonderland - Core/Encounters',
  'Mabel',
  'Another name Alice worries she has turned into'),
 ('Wonderland - Core/Encounters', 'Mouse', 'Meets Alice in the Pool of Tears'),
 ('Wonderland - Core/Encounters',
  'The Pigeon',
  'Accuses Alice of being a serpent'),
 ('Wonderland - Core/Encounters',
  'White Rabbit',
  'Herald figure whom Alice follows'),
 ('Wonderland - White Rabbit’s Household',
  'Bill the Lizard',
  'Chimney-sweep who’s blasted from the Rabbit’s chimney'),
 ('Wonderland - White Rabbit’s Household',
  'Mary Ann',
  'The White Rabbit’s maid'),
 ('Wonderland - White Rabbit’s Household',
  'Pat',
  'Workman involved in the chimney scene'),
 ('Wonderland - Duchess and Kitchen',
  'Baby',
  'The Duchess’s baby that turns into a pig'),
 ('Wonderland - Duchess and Kitchen',
  'Duchess',
  'Owner of the peppery kitchen'),
 ('Wonderland - Duchess and Kitchen',
  ""Duchess's Cook"",
  'Pepper-throwing cook in the Duchess’s kitchen'),
 ('Wonderland - Duchess and Kitchen',
  'Fish-Footman',
  'Delivers invitation to the Duchess'),
 ('Wonderland - Duchess and Kitchen',
  'Frog-Footman',
  'Receives the invitation for the Duchess'),
 ('Wonderland - Tea-Party', 'Dormouse', 'Tea-guest who keeps falling asleep'),
 ('Wonderland - Tea-Party', 'Hatter', 'Tea-guest stuck at six o’clock'),
 ('Wonderland - Tea-Party',
  'March Hare',
  'Tea host at the perpetual tea-party'),
 ('Wonderland - Tea-Party', 'Time', 'Personified; offended by the Hatter'),
 ('Wonderland - Court of Hearts', 'Five', 'Card-gardener painting the roses'),
 ('Wonderland - Court of Hearts',
  'King of Hearts',
  'Monarch presiding at the trial'),
 ('Wonderland - Court of Hearts',
  'Knave of Hearts',
  'Defendant accused of stealing the tarts'),
 ('Wonderland - Court of Hearts', 'Queen of Hearts', 'Tart-obsessed tyrant'),
 ('Wonderland - Court of Hearts', 'Seven', 'Card-gardener painting the roses'),
 ('Wonderland - Court of Hearts',
  'The Executioner',
  'Headsman at the Queen’s court'),
 ('Wonderland - Court of Hearts', 'Two', 'Card-gardener painting the roses'),
 ('Wonderland - Gryphon, Mock Turtle, and Sea-School',
  'Classics Master',
  'One of the teachers in the Mock Turtle’s school jokes'),
 ('Wonderland - Gryphon, Mock Turtle, and Sea-School',
  'Drawling-Master',
  'Conger-eel teacher who ‘came once a week’'),
 ('Wonderland - Gryphon, Mock Turtle, and Sea-School',
  'Gryphon',
  'Creature who takes Alice to the Mock Turtle'),
 ('Wonderland - Gryphon, Mock Turtle, and Sea-School',
  'Mock Turtle',
  'Melancholy storyteller of sea-school'),
 ('Wonderland - Gryphon, Mock Turtle, and Sea-School',
  'The Lobster',
  'Figure in the Lobster-Quadrille'),
 ('Wonderland - Gryphon, Mock Turtle, and Sea-School',
  'The Owl',
  'From the verse about sharing a pie with the Panther'),
 ('Wonderland - Gryphon, Mock Turtle, and Sea-School',
  'The Panther',
  'From the verse about sharing a pie with the Owl'),
 ('Wonderland - Gryphon, Mock Turtle, and Sea-School',
  'The Porpoise',
  'Mentioned companion in the Quadrille tale'),
 ('Wonderland - Gryphon, Mock Turtle, and Sea-School',
  'The Shark',
  'Sea creature referenced in parodic verses'),
 ('Wonderland - Gryphon, Mock Turtle, and Sea-School',
  'The Snail',
  'Refuses to join the dance in the Whiting’s story'),
 ('Wonderland - Gryphon, Mock Turtle, and Sea-School',
  'The Whiting',
  'Tells the Snail about the Quadrille'),
 ('Wonderland - Gryphon, Mock Turtle, and Sea-School',
  'Tortoise',
  'Teacher (‘because he taught us’)'),
 ('Wonderland - Parodied Poem: You Are Old, Father William',
  'Father William',
  'Subject of Alice’s recitation'),
 ('Wonderland - Parodied Poem: You Are Old, Father William',
  ""Father William's Wife"",
  'Appears in the poem'),
 ('Wonderland - Parodied Poem: You Are Old, Father William',
  'The Young Man',
  'Inquisitive youth in the poem'),
 ('Wonderland - Parodied Poem: The Sluggard',
  'The Sluggard',
  ""Isaac Watts’s original, parodied as '’Tis the Voice of the Lobster'""),
 ('Wonderland - Treacle-Well Sisters',
  'Elsie',
  'One of the three sisters in the Dormouse’s tale'),
 ('Wonderland - Treacle-Well Sisters',
  'Lacie',
  'Another of the three sisters'),
 ('Wonderland - Treacle-Well Sisters', 'Tillie', 'The third sister'),
 ('English History (Norman Conquest lesson)',
  'Edgar Atheling',
  'Anglo-Saxon claimant in 1066'),
 ('English History (Norman Conquest lesson)',
  'Edwin',
  'Earl of Mercia allied with Edgar Atheling'),
 ('English History (Norman Conquest lesson)',
  'Morcar',
  'Earl of Northumbria allied with Edgar Atheling'),
 ('English History (Norman Conquest lesson)',
  'Pope Alexander II',
  'Granted papal banner to William'),
 ('English History (Norman Conquest lesson)',
  'Stigand',
  'Archbishop of Canterbury at the time'),
 ('English History (Norman Conquest lesson)',
  'William the Conqueror',
  'Norman invader who became king of England'),
 ('Other / Unclear',
  'Her Brother',
  'Ambiguous reference not clearly identified in Alice')]That’s data. You can use it. I’ve pasted the actual data. Here is how I convert it for use, programmatically. I have to convert it, because AI just gives you a text string back.import ast

factions = prompt_gpt(instructions, text)

factions = ast.literal_eval(factions)

factionsThat ast library is an unsung hero. It can be used to convert strings of lists or JSON data into actual Python Lists and JSON objects. In the first line, I make ast available for use. In the third line, I convert the text list into a Python list.I can then load the data into a DataFrame and use it.If you don’t know how to do that and you are a software engineer, you should learn how to do that. It is extremely useful and valuable. And for the non-techie readers, it means that AI outputs can be used for other things. It means that you aren’t limited to the input data. It means AI can be an engine for data enrichment.Since I now have the data in a DataFrame, I can analyze it.Or even map it out.Forgive the ugly graph. I had never done this before in my life, and was blown away that it worked. This is going to open some new doors. Think of this as textual community detection for graph, based on incident data, not based only on graph location. Cool stuff shown ugly.Prompt Engineering is Simple EnoughPrompt Engineering is simple enough and not intimidating. Honestly, writing “hello world” scripts in the 90s was more intimidating than this, in my opinion.That means, non-techie folks, feel free to dabble. There’s no gatekeepers. Learn a little Python. Play with it. Chat me up on LinkedIn. I’ll show you how to do some things.Or, if you want 1:1 training for this, reach out to me. I’m not a trainer, but we can figure something out. It won’t be free, though. This blog is the free way for learning.But this stuff is simple, and you can do it in your native language. You don’t have to use English.There’s More to ExplainOk, this post was a gentle introduction to Prompt Engineering, and my goal was to encourage people and to eliminate some of the fear. Don’t be intimidated by Data Science. There is no reason. You don’t need math or stats for Prompt Engineering. You need language and communication skills. This is the least intimidating programming I know about. Be creative and have fun. Learn to use this and reach your desired outcomes. Kick all gatekeepers down a hole.In the next article, we will be talking about validating AI outputs. That is another important topic for both technical and non-technical audiences, and I am going to show you what works. This is not theoretical, this is practical get-stuff-done information.Ok, that’s all for today. I’ve said enough. Please, support this blog.Need Any Help?Finally… there’s no fun way to say this. I am out of a job and need to find work. If you work in Data Operations, Data Engineering, Cybersecurity, Data Science, or do anything with Artificial Intelligence or Machine Learning and you think I can be of use to your company, please reach out.I need full-time work with benefits. I need a good problem to help solve. I enjoy collaborations, but I need to pay mortgage and expenses. I am very good at what I do, and I am very good at making companies more effective. I’m a very friendly person and a good teammate, so let me know if you think of anything. Or pitch in and buy a coffee to support this writing.Please Support this BlogI would like to make a special request in this article. This blog has over 600 subscribers. I have written over 40 articles. Each article typically involves about four hours of research and development, so that’s about 200 hours of valuable work and writing that I’ve provided for free, because most important is that I want people to learn this. I am not doing this to make money.However, these days, there are things that I would like to do. For instance, to play with GraphRAG for AI, it is useful to have access to a Graph Database. The cheapest tier Neo4j instance is about $800/year. I would like to work on GraphRAG and write about it so that you all learn, but I cannot do that without support.So, I have opened up a few ways for you to support this blog:If you are a subscriber, please consider converting to a paid subscriber. I provide code, data files, and coding explanations that are absolutely worth more than $8 per month. But I understand that not everyone can afford to pay, and that’s fine. Free is absolutely fine, for those who need free.If you are a paid or unpaid subscriber and you want more flexibility in your contributions, I set up a ko-fi account. CLICK HERE. You can use this this to buy me a coffee ($5 donation) or even to pitch in for a Neo4j Aura instance, which will enable more writing and learning.And no matter what, if you are here, please buy and read my book. I am working on more book projects, as mentioned on Day 43.Oh, and please participate in comments. I am a friendly guy. It is lonely in the comments and always weird to me that people don’t seem to want to talk about this stuff. Why not? What is on your mind? Have any cool ideas you want to brainstorm? Don’t be intimidated, for sure. Be creative instead.","['Artificial Intelligence and Machine Learning', 'Community Detection', 'Egocentric Network Analysis', 'Graph Visualization']"
